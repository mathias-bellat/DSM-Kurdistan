[["index.html", "Soil information in Kurdistan region, Dohuk governorate (Iraq), supplementary material Preface", " Soil information in Kurdistan region, Dohuk governorate (Iraq), supplementary material Mathias Bellat 2025-01-15 Preface This is a complete description of the workflow related to the paper Soil information in Kurdistan region, Dohuk governorate (Iraq). It contains all the pre-processing steps of the subsequent stages and the data required to understand the entire process. Authors: Mathias Bellat, CRC1070 ResourceCultures, Department of Geosciences, Working group of Soil Science and Geomorphology, University of Tuebingen, Tuebingen, Germany, mathias.bellat@uni-tuebingen.de. Nafiseh Kakhani, CRC1070 ResourceCultures, Department of Geosciences, Working group of Soil Science and Geomorphology, University of Tuebingen, Tuebingen, Germany, nafiseh.kakhani@uni-tuebingen.de. "],["RUSLE.html", "1 RUSLE model protocol 1.1 Introduction 1.2 Equations 1.3 Results 1.4 Annexes", " 1 RUSLE model protocol 1.1 Introduction 1.1.1 Purpose This RUSLE model is generated to produce a soil erosion map for our study area of the B07 project (based on the EHAS survey). This soil erosion map will later be used for a conditioned Latin hypercube sampling (cLHS) to choose sample locations. It was based on the originals equations from Renard et al. (1991). 1.1.2 Procedure We follow Thapa (2020) procedure for the RUSLE model based on 5 factors: rainfall, soil strength, slope, land use and conservation. We also used Cossart, Fressard, and Chaize (2020); Almagro et al. (2019) and Mehri et al. (2024) as reference work. The DEM has been filled with the SAGA GIS function Fill Sinks (Wank &amp; Liu); SAGA GIS 7.8.2 version. Figure 1.1: Methodological framework for potential erosion using RUSLE model (from Thapa, 2020). 1.1.3 Input data All the data listed below are available freely online. (#tab:imput_1)Data used in the production of the RUSLE model. Name/ID Original resolution (m) Type/Unit Source DEM/TE.5 25 Meters ESA and Airbus (2022) Prec. sum/OT.6 1000 mm WorldClim2 Fick and Hijmans (2017) Landuse/OT.4 10 - Zanaga et al. (2021) WRB Soils/OT.11 1000 - HWSD v2.0 FAO and IIASA (2023) 1.2 Equations 1.2.1 R Factor It is the precipitation factor computed from the precipitation raster: \\[ R = 38.5 + 0.35P \\] For P = precipitation per year in mm. 1.2.2 K Factor Is the soil strength factor computed from different parameters from soil map: SAN = Sand in % SIL = Silt in % CLA = Clay in % C = Soil organic carbon in % \\[ Fcsand = 0.2 + 0.3exp(-0.0256 SAN(1 - \\frac{SIL}{100})) \\] \\[\\\\[0.5cm]\\] \\[ Fsi-cl = \\frac{SIL}{CLA+SIL}^{0.3} \\] \\[\\\\[0.5cm]\\] \\[ Forgc = 1- \\frac{0.25C}{C + exp(3.72 -2.95C)} \\] \\[\\\\[0.5cm]\\] \\[ Fhisand = 1- \\frac{0.7(\\frac{SAND - 1}{100})}{(\\frac{SAND - 1}{100}) + exp (-5.51+22.9 (\\frac{SAN - 1}{100}))} \\] \\[\\\\[0.5cm]\\] \\[ K = Fcsand * Fsi-cl *Forgc *Fhisand *0.1317 \\] 1.2.3 LS factor The slope length and steepness factor was computed from the DEM under SAGA GIS 7.8.2. We used the LSFactor single way with the Desmet and Govers 1996 method. Figure 1.2: SAGA Ls factor with the One Step tool. For more detail of the LS parameters: \\(S_j\\) = Slope factor for the j-th segment. \\(\\lambda_j\\) = Distance from the lower boundary of the j-th segment to the upslope (m). \\(m\\) = length exponent of the RUSLE Ls factor. \\[ LSj = \\frac{Sj(\\lambda_j~^{m+1}-\\lambda\\frac{m + 1}{j - 1})}{(\\lambda_j-\\lambda_{j-1})22.13^m} \\] 1.2.4 C factor It was based on the land cover map of Copernicus data set and divided into 6 different classes. Then, based on a literature review, a ratio was established (Morgan 2005; Swarnkar et al. 2018). Table 1.1: Ratio of C factor between different land uses. Land use Value Water 0 Forest 0.001 Shrubland 0.01 Urban Area 0.1 Cropland 0.2 Wasteland 1 1.2.5 P factor P is the conservation factor value. It is difficult to establish in the region as no previous work has been done. As no value was known for this factor, we put a 1 value default as in Mehri et al. (2024). 1.3 Results The erosion map shows a low erosion at Mg/ha-1 year-1 in terms of quantity. Most of the values are under 1.7, and only a few are above it. However, with a quantile classification (not based on continuous classification), we were able to spot the area with more erosion. It is mainly an area on the south of the Be’khair anticline in the Swaka Tika anticline and the upper region of Zakho (under the anticline). The northern and western areas are more preserved from soil erosion as the high anticline is probably due to less land use and forest presence. When the slope factor mainly influences the wadi and river border, the precipitation factor strongly influences the upper Khabur valley. The soil factor seems to have a minor influence on the middle and upper Khabur valley and on the right bank of the Tigris. The most preserved areas are the river and Mosul Lake. Table 1.2: Basic statistics for the erosion map based on RUSLE model. Statistic Value (Mg/ha-1 year-1) Min. value 0 Max. value 117 Mean value 1.17561 SD 3.1574 1.4 Annexes (#fig:erosion map gradient)RUSLE map of erosion with breaks calculated by quantiles (WGS84, UTM38N). References "],["CLHS.html", "2 Conditioned Latin hypercube sampling 2.1 Introduction 2.2 Data treatement 2.3 conditioned Latin hypercube sampling script 2.4 Sampling locations", " 2 Conditioned Latin hypercube sampling 2.1 Introduction 2.1.1 Purpose This document presents the results of the sampling design for the 2022 and 2023 summer field campaigns of the B07 subproject in CRC1070. We used a specific sampling technique called conditioned Latin hypercube sampling (cLHS). This sampling strategy based on combinations of different strata/layers gave a diversity of samplings according to the heterogeneity of the layers. We tried different numbers of sampling locations (100, 150, 200, 250, 300, 350 and 400) to finally agree on a set of 100 samplings (We majored the number of samples by 10% - to reach 110 locations - to avoid any issue on the file in case of non-accessible area). The study area covers over 830 km\\(^2\\) in 2022 and 490 km\\(^2\\) in 2023. The size of area C explored in 2023 is equal to 1450 km\\(^2\\). Still, due to inaccessible places in high mountains or conflict zones between PKK (Kurdistan Workers’ Party) and the Turkish army (Ertan 2022), we reduce the explored area. Sampling areas for 2022 and 2023 2.1.2 Procedure We first produced several maps composed of different factors before using them as layers for the conditioned Latin hypercube sampling. We used different software such as GIS 3.24.1 version and SAGA GIS 7.8.2, GoogleEarthEngine (GEE) and R 4.4 with Rstudio. The procedure is partially based on the Sampling package from Dick Brus and its reference book (Brus 2022). ###Input data All the data listed below are available freely online excepted for the digitized maps (geomorphology). (#tab:imput_2)Data used in the production of the conditioned Latin hypercube sampling. Name/ID Original resolution (m) Type/Unit Source DEM/TE.5 25 Meters ESA and Airbus (2022) RUSLE map/OT.11 25 Mg/ha\\(^{-1}\\) year\\(^{-1}\\) Mathias Bellat Slope/TE.16 25 Radians ESA and Airbus (2022) TWI/TE.22 25 - ESA and Airbus (2022) Soil estimation/OT.12 10 - Nafiseh Kakhani / USGS (2018) Geomorphology/OT.3 1 : 250 000 - Forti et al. (2021) Landsat 8 Green/LA.3 30 0.53 - 0.59 µm USGS (2018) Landsat 8 Red/LA.4 30 0.64 - 0.67 µm USGS (2018) Landsat 8 NIR/LA.5 30 0.85 - 0.88 µm USGS (2018) Landsat 8 SWIR1/LA.6 30 1.57 - 1.65 µm USGS (2018) Landsat 8 SWIR2/LA.7 30 2.11 - 2.29 µm USGS (2018) 2.2 Data treatement 2.2.1 Soil prediction map (Nafiseh Kakhani) The Landsat 8 images were collected via a Google Earth Engine script on a period covering 2020, the median of the composite image from Tier 1 TOA collection was used. To produce the soil estimation map we based our prediction on a Google Earth Engine written script. The javascript codes for scraping these images and computing the soil estimation indexes are available in the supplementary file inside the “2 - CLHS/code” folder. We computed four soil indexes: \\[ Landsat~8~Clay~index = \\frac{SWIR1}{SWIR2} \\] \\[ Landsat~8~Ferrous~minerals~ratio = \\frac{SWIR1}{NIR} \\] \\[ Landsat~8~Carbonate~index = \\frac{Red- Green}{Red +Green} \\] \\[ Landsat~8~Rocks~outcrop~index = \\frac{SWIR1-Green}{SWIR1+Green} \\] 2.2.2 Soil erosion based on RUSLE model The RUSLE map was previously computed in Chapter 1. 2.2.3 DEM, slope and topographic wetness index The DEM is on a 25 x 25 m scale based on the ESA data (ESA and Airbus 2022). The slope under SAGA GIS 7.8.2 with the function Slope, Aspect, Curvature with the method by Zevenbergen, L.W., Thorne, C.R. (1987). For the topographic wetness index the Topographic Wetness Index (One step) command was used with Multiflow direction parameters 2.2.4 Geomorphological map The geomorphological map is computed based on (Forti et al. 2021). We created 17 different layers for each of the geomorphons present. 2.3 conditioned Latin hypercube sampling script 2.3.1 Preparation of the R environement # 0 Environment setup ########################################################## # 0.1 Prepare environment ====================================================== # Folder check getwd() # Select folder setwd(getwd()) # Clean up workspace rm(list = ls(all.names = TRUE)) # 0.2 Install packages ========================================================= # Load packages library(pacman) #Easier way of loading packages pacman::p_load(sf, mapview,raster, clhs) # Specify required packages and download it if needed # 0.3 Show session infos ======================================================= sessionInfo() ## R version 4.4.0 (2024-04-24 ucrt) ## Platform: x86_64-w64-mingw32/x64 ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=French_France.utf8 LC_CTYPE=French_France.utf8 ## [3] LC_MONETARY=French_France.utf8 LC_NUMERIC=C ## [5] LC_TIME=French_France.utf8 ## ## time zone: Europe/Berlin ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] ggplot2_3.5.1 mapview_2.11.2 clhs_0.9.0 sf_1.0-18 raster_3.6-30 ## [6] sp_2.1-4 bookdown_0.41 tufte_0.13 rmarkdown_2.29 knitr_1.49 ## ## loaded via a namespace (and not attached): ## [1] gtable_0.3.6 xfun_0.48 bslib_0.8.0 htmlwidgets_1.6.4 ## [5] lattice_0.22-6 vctrs_0.6.5 tools_4.4.0 crosstalk_1.2.1 ## [9] generics_0.1.3 stats4_4.4.0 tibble_3.2.1 proxy_0.4-27 ## [13] fansi_1.0.6 cluster_2.1.6 pkgconfig_2.0.3 KernSmooth_2.23-22 ## [17] satellite_1.0.5 leaflet_2.2.2 lifecycle_1.0.4 compiler_4.4.0 ## [21] stringr_1.5.1 munsell_0.5.1 terra_1.7-83 codetools_0.2-20 ## [25] htmltools_0.5.8.1 class_7.3-22 sass_0.4.9 yaml_2.3.10 ## [29] pillar_1.9.0 jquerylib_0.1.4 classInt_0.4-10 cachem_1.1.0 ## [33] tidyselect_1.2.1 digest_0.6.37 stringi_1.8.4 dplyr_1.1.4 ## [37] reshape2_1.4.4 fastmap_1.2.0 grid_4.4.0 colorspace_2.1-1 ## [41] cli_3.6.3 magrittr_2.0.3 base64enc_0.1-3 utf8_1.2.4 ## [45] leafem_0.2.3 e1071_1.7-16 withr_3.0.2 scales_1.3.0 ## [49] png_0.1-8 evaluate_1.0.1 rlang_1.1.4 Rcpp_1.0.13 ## [53] glue_1.7.0 DBI_1.2.3 rstudioapi_0.17.1 jsonlite_1.8.9 ## [57] R6_2.5.1 plyr_1.8.9 units_0.8-5 2.3.2 Preparation of the data # 1 Import and prepare the data ----------------------------------------------- # 1.1 Select the the files ##################################################### Area &lt;- st_read(&quot;./data/Sampling_area_2022.gpkg&quot;, layer = &quot;Sampling_area_2022&quot;) # Change the area for 2022 and 2023 campaigns DEM &lt;- raster(&quot;./data/DEM.tif&quot;) Geomorpho &lt;- raster(&quot;./data/Geomorphology.tif&quot;) Soil_erosion &lt;- raster(&quot;./data/RUSLE_map.tif&quot;) Soil_prediction &lt;- raster(&quot;./data/Soil_prediction.tif&quot;) Slope &lt;- raster(&quot;./data/Slope.tif&quot;) Wetness &lt;- raster(&quot;./data/TWI.tif&quot;) # 1.2 Clean and prepare the DEM ################################################ # Crop the DEM to the study area DEM &lt;- crop(DEM, Area, inverse=FALSE, updatevalue=NA, updateNA=TRUE) # Mask the DEM to the study area DEM &lt;- mask(DEM, Area, inverse=FALSE, updatevalue=NA, updateNA=TRUE) # 1.3 Clean and prepare the covariates ######################################### # Create a list of the different variables list &lt;- list(Geomorpho, Soil_erosion, Soil_prediction, Slope, Wetness) names(list) &lt;- c(&quot;Geomorpho&quot;,&quot;Soil_erosion&quot;,&quot;Soil_prediction&quot;,&quot;Slope&quot;, &quot;Wetness&quot;) # Set the CRS WGS84 UTM38N to all raster ZoneUTM &lt;- c(&quot;+init=epsg:32638&quot;) for (i in names(list)) { r &lt;- list[[i]] list[[i]] &lt;- projectRaster(r, crs = ZoneUTM) } # Resample according to the DEM # With NGB only for the categorical raster list[[1]] &lt;- resample(list[[1]], DEM, &quot;ngb&quot;) list[[3]] &lt;- resample(list[[3]], DEM, &quot;ngb&quot;) # With bilinear only for the numerical raster list[[2]] &lt;- resample(list[[2]], DEM, &quot;bilinear&quot;) list[[4]] &lt;- resample(list[[4]], DEM, &quot;bilinear&quot;) list[[5]] &lt;- resample(list[[5]], DEM, &quot;bilinear&quot;) # 1.4 Stack all the predictors ################################################# stack &lt;- stack(DEM, list$Geomorpho, list$Soil_erosion,list$Soil_prediction, list$Slope, list$Wetness) # stack all the data in a stacked raster plot(stack$DEM) # 1.5 Save and export the predictors ########################################### writeRaster(stack ,filename = &quot;./export/predictors.tif&quot;, format=&quot;GTiff&quot;,overwrite=T) rm(list = ls()) ## Reading layer `Sampling_area_2022&#39; from data source ## `G:\\OneDrive\\Ecole\\Articles\\Data_paper\\02 - Cluster Latin Hypercube\\data\\Sampling_area_2022.gpkg&#39; ## using driver `GPKG&#39; ## Simple feature collection with 1 feature and 2 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 264456.8 ymin: 4069145 xmax: 312107.8 ymax: 4110934 ## Projected CRS: WGS 84 / UTM zone 38N ## Reading layer `Sampling_area_2023&#39; from data source ## `G:\\OneDrive\\Ecole\\Articles\\Data_paper\\02 - Cluster Latin Hypercube\\data\\Sampling_area_2023.gpkg&#39; ## using driver `GPKG&#39; ## Simple feature collection with 1 feature and 2 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 289648.8 ymin: 4099749 xmax: 331869.8 ymax: 4121043 ## Projected CRS: WGS 84 / UTM zone 38N 2.3.3 Calculation of the conditioned Latin hypercube sampling # 2 Create the Conditioned Latin Hypercube Sampling --------------------------- # 2.1 Import and prepare the files ############################################# predictors &lt;- raster::stack(&quot;./export/predictors.tif&quot;) names(predictor@layers) &lt;- c(&quot;DEM&quot;,&quot;Geomorpho&quot;,&quot;Soil_erosion&quot;,&quot;Soil_prediction&quot;,&quot;Slope&quot;, &quot;Wetness&quot; ) # Convert the file into Data Frame PredForMap &lt;- as(predictors, &#39;SpatialPixelsDataFrame&#39;) PredForMap &lt;- data.frame(PredForMap) stack_frame &lt;- PredForMap[complete.cases(PredForMap),] # Remove NA values names(stack_frame) &lt;- c(&quot;DEM&quot;,&quot;Geomorpho&quot;,&quot;Soil_erosion&quot;,&quot;Soil_prediction&quot;,&quot;Slope&quot;, &quot;Wetness&quot;, &quot;x&quot;, &quot;y&quot;) stack_frame$Geomorpho &lt;- round(stack_frame$Geomorpho, digits=0) # If ever the categorical classification did not work stack_frame$Soil_prediction &lt;- round(stack_frame$Soil_prediction, digits=0) # If ever the categorical classification did not work stack_frame &lt;- na.omit(stack_frame) save.image(&quot;./export/Pre-process.RData&quot;) # Save the image # 2.2 Conditioned Latin Hypercube sampling parameters ########################## # Select the predictors for the Conditioned Latin Hypercube preds &lt;- c(&quot;DEM&quot;,&quot;Geomorpho&quot;,&quot;Soil_erosion&quot;,&quot;Soil_prediction&quot;,&quot;Slope&quot;,&quot;Wetness&quot;) # Set the size of the sampling c = 110 # Set a seed set.seed(1070) # Run the sampling res &lt;- clhs(stack_frame[, preds], size = c, tdecrease = 0.95, iter = 50000, progress = FALSE, simple = FALSE) # 2.3 Export the results ####################################################### # Fit the results with the line in the table CLHS_sampled_res &lt;- stack_frame[res$index_samples, ] # Convert to spatial data frame point CLHS_sampled_df &lt;- SpatialPoints(coords = CLHS_sampled_res[, c(&quot;x&quot;, &quot;y&quot;)], proj4string=CRS(&quot;+init=epsg:32638&quot;)) # Export the results in a CSV format write.table(CLHI_sampled_df, &quot;./export/CLHS_2022.csv&quot;, col.names = TRUE, sep = &quot;;&quot;, row.names = FALSE, fileEncoding = &quot;UTF-8&quot;) #Replace 2022 with 2023 for the 2023 summer sampling campaign 2.4 Sampling locations Some sampling locations were located in inaccessible areas so not all the spots have been sampled. Only 101 in 2022 and 21 in 2023. References "],["Field.html", "3 Field information and sampling locations 3.1 Sites sampled 3.2 Regional information and collected data", " 3 Field information and sampling locations 3.1 Sites sampled 3.1.1 2017 and 2018 campaigns During the EHAS survey of summer 2017 and 2028 P. Sconzo and T. Rentschler collected and analysed 29 samples on 16 different sites. These sampling sites are matching with archaeologists sites from the EAHS survey. They were selected based on a legacy sampling strategy on different part of the slope of the archaeological sites. 3.1.2 2022 and 2023 campaigns During the two sampling campaigns of 2022-2023, a total of 124 sites were collected via the use of a conditioned Latin hypercube sampling. 3.2 Regional information and collected data The collected information is only for the 2022 and 2023 campaigns, as the EHAS survey in 2017 and 2018 had a different sampling protocol only sampling surface soil (0 - 10 cm). 3.2.1 Landscape observations 3.2.1.1 Zakho region The sampling campaigns took place in three distinct areas. The first one in the north reaches from Zakho in the west toward Mangesh in the east. It has a semi-wet to dry climate with over 400 mm per year precipitations. This region is dominated by pebbly-sandstone (Mukdadiyah formation Al-Mousawi, Fouad, and Sissakian (2007)) and conglomerate formation and colluvial deposits on the surface. Vegetation with dominating oaks (quercus brantii) and olive trees. Wadis and a major affluent of the Tigris, the upper Khabur, flow from east to west. 3.2.1.2 Simele/Selevani Upper plain On the south of the Be’khair mountain, from the foothills towards the main east-west road from Dohuk to Dayrabun in the south, the Upper Simele plain forms another geographical unit. Temperature is higher, and precipitation is below 400 mm per year. There is very little vegetation, and no trees except cultivated species along the rivers and wadis. Surface structures are dominated by glacis, pediments and pebble deposits and relics of conglomerates or sandstone and claystone formations (Injana formation Sissakian, Hagopian, and Hasan (1995)). 3.2.1.3 Simile/Selevani Lower plain The last region is the southern, or Lower, Simile plain, from the Dohuk Dayrabun road to the Mosul dam lake in the south. Temperatures are high, and precipitation is below 400 mm per year, with no vegetation except along rivers and wadis. Surface structures are dominated by badlands, colluvial deposits, sandstone (Injana formation Sissakian, Hagopian, and Hasan (1995)), claystone formations (Fatha formation Sissakian, Hagopian, and Hasan (1995)), and some terraces. 3.2.2 Sampling method The samples were collected with the help of an auger and separated in five-depth increment : 0 - 10 cm with the top ploughed soil 10 - 30 cm the top and sub-surface soil 30 - 50 cm the sub-surface soil 50 - 70 cm the sub-surface and lower soil 70 - 100 cm the lower soil In many cases, the bedrock was reached before 100 cm. Therefore, five samples were not collected in every site location. Samplings were saved in a small plastic bag before being air-dried for 48h, followed by wet sieving at 2 mm. 3.2.3 Collected informations At each location, several pieces of information were registered and can be found in the deposit in the field_observations file. The different entries are: Site_Name the site’s name according to the cLHS sampling Altitude A.S.L. the altitude above is the level in meters, measured with Garmin, GPSMAP 60Cx with max. 3 m accuracy (depending of satellite coverage) X (WGS 84 UTM 38) longitude in WGS84 UTM38N projection (epsg:32638), measured with Garmin, GPSMAP 60Cx with ± 10 - 3 m accuracy (depending of satellite coverage) Y (WGS 84 UTM 38) latitude in WGS84 UTM38N projection (epsg:32638), measured with Garmin, GPSMAP 60Cx with ± 10 - 3 m accuracy (depending of satellite coverage) Texture the texture of the different soil horizons Colors colors of the soils according to the Munsell soil-color charts 2009 edition Number_horizon short description of the different horizons. Description short description of the entire pedon. Interpretation is a short interpretation of the entire pedon. Relief is a short description of the relief surrounding the site. Soil_depth depth of the soil before bedrock in cm. Additionally, photos were taken at each location and they are accessible on the deposit. \\[\\\\[0.5cm]\\] B07_2022_038 sampling location References "],["FTIR.html", "4 Fourier-transform infrared spectroscopy of soil samples 4.1 Protocol and devices 4.2 Raw spectra production 4.3 Prepare the spectra regarding state of the art 4.4 Convert into different spectra variation 4.5 Plot the spectra 4.6 Kennard Stone sampling", " 4 Fourier-transform infrared spectroscopy of soil samples 4.1 Protocol and devices We analysed all of our 29 samples from 2017-2018 campaigns and 532 from 2022-2023 campaigns with mid-infrared spectroscopy to serve as predictors for the prediction model on the soil properties. These techniques are now well knowned and commonly used in soil science as they allow spare time and money on the different laboratory measurements (Wadoux et al. 2021; Viscarra Rossel et al. 2016; Ng et al. 2022; Ge, Wadoux, and Peng 2022; Stenberg et al. 2010; Bahrami, Danesh, and Bahrami 2022). Before being analysed with FTIR the samples were ground under 1 µm with a Pulverisette 5/4, classic line (Fritsh, Idar-Oberstein, Germany) in a 250 ml stainless steel hardness container (ISO: X105CrMo17) with five 20 mm sintered corrodium (99.7 % Al2O3) grinding balls. The settings were set at 350 turns per minute for 12 minutes in total. The last step before FTIR analyse was to realise lenses from the samples with KBr pressling method. 250 mg of potassium brodime (KBr) and 1-1.3 mg of sample substance are mixed and then loaded in a hydraulic press under vacuum with around 10 - 11 tonnes and pressed for 1-2 minutes to form a transparent tablet, e.g. 10 mm in diameter and 1 mm thick. This tablet was analysed under mid-infrared Fourier-transform spectroscopy with a Vertex 80v (Bruker OPTIK GmbH; Germany) under a control environment. The measurement resolution is 4 cm-1 in the interval of 375 - 4500 cm -1, the spectrum is in absorbance and the source is MIR with the optic filter. To calibrate these samples, one control tablet made of 100% of KBr was measured at the beginning of each measurement session. 4.2 Raw spectra production A script was written to produce a raw spectra in absorbance for 375 - 4500 cm -1 interval with a 4 cm-1 resolution and export it under a .txt format. # 0 Environment setup ########################################################## # 0.1 Prepare environment ====================================================== # Folder check getwd() # Select folder setwd(getwd()) # Clean up workspace rm(list = ls(all.names = TRUE)) # 0.2 Install packages ========================================================= # Load packages install.packages(&quot;pacman&quot;) #Install and load the &quot;pacman&quot; package (allow easier download of packages) library(pacman) pacman::p_load(prospectr, remotes,readr) ### Install required packages remotes::install_github(&quot;philipp-baumann/simplerspec&quot;) #Install package from Baumann for spectral analysis library(simplerspec) # 0.3 Show session infos ======================================================= sessionInfo() ## R version 4.4.0 (2024-04-24 ucrt) ## Platform: x86_64-w64-mingw32/x64 ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=French_France.utf8 LC_CTYPE=French_France.utf8 ## [3] LC_MONETARY=French_France.utf8 LC_NUMERIC=C ## [5] LC_TIME=French_France.utf8 ## ## time zone: Europe/Berlin ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] simplerspec_0.2.1 foreach_1.5.2 remotes_2.5.0 prospectr_0.2.7 ## [5] DT_0.33 readr_2.1.5 mapview_2.11.2 sf_1.0-18 ## [9] raster_3.6-30 sp_2.1-4 ggplot2_3.5.1 bookdown_0.41 ## [13] tufte_0.13 rmarkdown_2.29 knitr_1.49 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_1.2.1 dplyr_1.1.4 farver_2.1.2 ## [4] fastmap_1.2.0 leaflet_2.2.2 mathjaxr_1.6-0 ## [7] digest_0.6.37 lifecycle_1.0.4 cluster_2.1.6 ## [10] terra_1.7-83 magrittr_2.0.3 compiler_4.4.0 ## [13] rlang_1.1.4 sass_0.4.9 tools_4.4.0 ## [16] leafpop_0.1.0 utf8_1.2.4 yaml_2.3.10 ## [19] data.table_1.16.2 brew_1.0-10 htmlwidgets_1.6.4 ## [22] curl_5.2.3 bit_4.5.0 classInt_0.4-10 ## [25] plyr_1.8.9 RColorBrewer_1.1-3 KernSmooth_2.23-22 ## [28] purrr_1.0.2 withr_3.0.2 grid_4.4.0 ## [31] stats4_4.4.0 fansi_1.0.6 e1071_1.7-16 ## [34] leafem_0.2.3 colorspace_2.1-1 scales_1.3.0 ## [37] iterators_1.0.14 cli_3.6.3 crayon_1.5.3 ## [40] generics_0.1.3 rstudioapi_0.17.1 reshape2_1.4.4 ## [43] tzdb_0.4.0 DBI_1.2.3 cachem_1.1.0 ## [46] proxy_0.4-27 stringr_1.5.1 parallel_4.4.0 ## [49] base64enc_0.1-3 vctrs_0.6.5 jsonlite_1.8.9 ## [52] hms_1.1.3 bit64_4.5.2 systemfonts_1.1.0 ## [55] crosstalk_1.2.1 jquerylib_0.1.4 units_0.8-5 ## [58] glue_1.7.0 codetools_0.2-20 leaflet.providers_2.0.0 ## [61] stringi_1.8.4 gtable_0.3.6 munsell_0.5.1 ## [64] tibble_3.2.1 pillar_1.9.0 htmltools_0.5.8.1 ## [67] satellite_1.0.5 R6_2.5.1 vroom_1.6.5 ## [70] evaluate_1.0.1 lattice_0.22-6 png_0.1-8 ## [73] bslib_0.8.0 class_7.3-22 Rcpp_1.0.13 ## [76] uuid_1.2-1 svglite_2.1.3 xfun_0.48 ## [79] pkgconfig_2.0.3 # 01 Prepare and import data --------------------------------------------------- # 01.1 Import the raw spectra files ############################################ lfMIR &lt;- read_opus_univ(fnames = dir(&quot;./data/spectra/&quot;, full.names = TRUE), extract = c(&quot;spc&quot;)) # 01.2 Preparing the MIR Data ################################################## MIRspec_tbl &lt;- lfMIR %&gt;% gather_spc() %&gt;% # Gather list of spectra data into tibble data frame resample_spc(wn_lower = 375, wn_upper = 4500, wn_interval = 4) %&gt;% # Resample spectra to new wavenumber interval average_spc(by = &quot;sample_id&quot;) # Average replicate scans per sample_id MIRspec_tbl_rs &lt;- MIRspec_tbl[seq(1, nrow(MIRspec_tbl)), c(&quot;sample_id&quot;, &quot;metadata&quot;, &quot;wavenumbers_rs&quot;, &quot;spc_mean&quot;)] MIRspec_tbl_rs &lt;- MIRspec_tbl_rs[,-c(2,3)] # Create a data frame and change the columns names MIRspec_wn &lt;- data.frame(matrix(unlist(MIRspec_tbl_rs$spc_mean), nrow = nrow(MIRspec_tbl_rs), byrow = TRUE), stringsAsFactors = FALSE) rownames(MIRspec_wn) &lt;- substring(MIRspec_tbl$sample_id, 1)[seq(1, nrow(MIRspec_tbl))] wn &lt;- list(names(MIRspec_tbl_rs[[2]][[1]])) wn &lt;- unlist(wn) names(MIRspec_wn) &lt;- wn MIRspec_wn &lt;- MIRspec_wn[, -c(1)] # Remove NA values from the 4499 cm-1 # 01.3 Export the MIR raw data ################################################# write.table(MIRspec_wn, &quot;./export/spectra/Spectra_raw_Wavenumber.txt&quot;, dec = &quot;.&quot;, sep = &quot;;&quot;, row.names = TRUE, col.names = TRUE, append = FALSE) 4.3 Prepare the spectra regarding state of the art 4.3.1 Interval interferances The MIR spectra present interference in different intervals such as 375 - 499 cm-1 area and 2451 - 2500 cm -1 area (Ng et al. 2018). # 02 Prepare the spectra regarding the state of the art ------------------------ # 2.1 First cleaning of the spectra ============================================ MIRspec_tbl &lt;- lfMIR %&gt;% gather_spc() %&gt;% # Gather list of spectra data into tibble data frame resample_spc(wn_lower = 499, wn_upper = 4500, wn_interval = 4) %&gt;% # Resample spectra to new wavenumber interval average_spc(by = &quot;sample_id&quot;) # Average replicate scans per sample_id MIRspec_tbl_rs &lt;- MIRspec_tbl[seq(1, nrow(MIRspec_tbl)), c(&quot;sample_id&quot;, &quot;metadata&quot;, &quot;wavenumbers_rs&quot;, &quot;spc_mean&quot;)] MIRspec_tbl_rs &lt;- MIRspec_tbl_rs[,-c(2,3)] # Create a data frame and change the columns names MIRspec_wn &lt;- data.frame(matrix(unlist(MIRspec_tbl_rs$spc_mean), nrow = nrow(MIRspec_tbl_rs), byrow = TRUE), stringsAsFactors = FALSE) rownames(MIRspec_wn) &lt;- substring(MIRspec_tbl$sample_id, 1)[seq(1, nrow(MIRspec_tbl))] wn &lt;- list(names(MIRspec_tbl_rs[[2]][[1]])) wn &lt;- unlist(wn) names(MIRspec_wn) &lt;- wn MIRspec_wn &lt;- MIRspec_wn[, -c(1)] # Remove NA values from the 4499 cm-1 # 2.1 Remove interference ===================================================== MIRspec_wn &lt;- MIRspec_wn[, -c(500:511)] 4.3.2 Outlier values Some values of the spectra can also present high interference and therefore should be removed or at least be noticed (Ng et al. 2018). This concerned values over 2 or lower than -2. Here we export the different rows containing values over 2. # Check max value and remove it max(MIRspec_wn) # Remove value higher than + 2 (Ng et al., 2018; Curran et al., 1996) MIRspec_wn_remove_up &lt;- MIRspec_wn[apply(MIRspec_wn, 1, function(row) any(row &gt; 2)), ] # Check lower value and remove it min(MIRspec_wn) # Remove value lower than - 2 (Ng et al., 2018; Curran et al., 1996) MIRspec_wn &lt;- MIRspec_wn[!rowSums(MIRspec_wn &lt; -2),] MIRspec_wn_remove_low &lt;- MIRspec_wn[apply(MIRspec_wn, 1, function(row) any(row &lt; - 2)), ] MIRspec_wn_remove &lt;- cbind(MIRspec_wn_remove_low, MIRspec_wn_remove_up) write.table(MIRspec_wn_remove, &quot;./export/Interference_spectra.txt&quot;, dec = &quot;.&quot;, sep = &quot;;&quot;, row.names = TRUE, col.names = TRUE, append = FALSE) 4.4 Convert into different spectra variation 4.4.1 Convert into wavelength # 03 Convert into different spectra variation ---------------------------------- # 3.1 Convert in Wavelength #################################################### MIRspec_wl &lt;- MIRspec_wn wn &lt;- (1 / as.numeric(names(MIRspec_wn))) * 1e7 wn &lt;- round(wn, digits=0) names(MIRspec_wl) &lt;- wn MIRspec_wl &lt;- na.omit(MIRspec_wl) 4.4.2 Spectra transformations We converted the MIR data into a total of 14 spectra transformations according to literature review (Ludwig et al. 2023; Ng et al. 2018) # 3.2 Prepare the functions #################################################### # Remove near zero variable remove_nzv &lt;- function(x){ y &lt;- nearZeroVar(x, saveMetrics = TRUE) ifelse(sum(y$nzv == TRUE) == 0, x &lt;- x, x &lt;- x[-nearZeroVar(x)]) return(x) } # Remove variable with high correlation remove_hcd &lt;- function(x){ y &lt;- findCorrelation(x, cutoff = .98) ifelse(sum(y) == 0, x &lt;- x, x &lt;- x[,-y]) return(x) } # 3.3 Convert in other spectra ################################################# Spectra &lt;- MIRspec_wn #Different transformation of the Spectra according to Ludwig et al. 2023 IRspectraList_Raw &lt;- list(&quot;SG 1.5&quot; = as.data.frame(prospectr::savitzkyGolay(Spectra, m = 1, p = 1, w = 5)), &quot;SG 1.11&quot; = as.data.frame(prospectr::savitzkyGolay(Spectra, m = 1, p = 1, w = 11)), &quot;SG 1.17&quot; = as.data.frame(prospectr::savitzkyGolay(Spectra, m = 1, p = 1, w = 17)), &quot;SG 1.23&quot; = as.data.frame(prospectr::savitzkyGolay(Spectra, m = 1, p = 1, w = 23)), &quot;SG 2.5&quot; = as.data.frame(prospectr::savitzkyGolay(Spectra, m = 1, p = 2, w = 5)), &quot;SG 2.11&quot; = as.data.frame(prospectr::savitzkyGolay(Spectra, m = 1, p = 2, w = 11)), &quot;SG 2.17&quot; = as.data.frame(prospectr::savitzkyGolay(Spectra, m = 1, p = 2, w = 17)), &quot;SG 2.23&quot; = as.data.frame(prospectr::savitzkyGolay(Spectra, m = 1, p = 2, w = 23)), &quot;moving averages 5&quot; = as.data.frame(prospectr::movav(Spectra, w = 5)), &quot;moving averages 11&quot; = as.data.frame(prospectr::movav(Spectra, w = 11)), &quot;moving averages 17&quot; = as.data.frame(prospectr::movav(Spectra, w = 17)), &quot;moving averages 23&quot; = as.data.frame(prospectr::movav(Spectra, w = 23)), &quot;SNV-SG&quot; = as.data.frame(prospectr::standardNormalVariate(prospectr::savitzkyGolay(Spectra, m = 1, p = 2, w = 11))), #Best option for machine learning treatment (See Ng et al. 2018) &quot;continuum removal&quot; = as.data.frame(prospectr::continuumRemoval(Spectra, as.numeric(colnames(Spectra)), type = &quot;R&quot;))) IRspectraList &lt;- IRspectraList_Raw # Remove the near zero and highly correlated values for (i in 1:length(IRspectraList)) { IRspectraList[[i]] &lt;- remove_hcd(IRspectraList[[i]]) IRspectraList[[i]] &lt;- remove_nzv(IRspectraList[[i]]) } IRspectraList &lt;- c(IRspectraList, list(&quot;raw&quot; = as.data.frame(Spectra))) # 3.4 Export all the spectra ################################################### #Export csv of files for (i in names(IRspectraList)) { write.table(IRspectraList[i], file = paste0(&quot;./export/spectra/Full_spectra_&quot;,names(IRspectraList[i]),&quot;.txt&quot;), dec = &quot;.&quot;, sep = &quot;;&quot;, row.names = TRUE, col.names = TRUE, append = FALSE, fileEncoding = &quot;UTF-8&quot;) } write.table(MIRspec_wn, &quot;./export/spectra/Spectra_Wavenumber.txt&quot;, dec = &quot;.&quot;, sep = &quot;;&quot;, row.names = TRUE, col.names = TRUE, append = FALSE) write.table(MIRspec_wl, &quot;./export/spectra/Spectra_Wavelength.txt&quot;, dec = &quot;.&quot;, sep = &quot;;&quot;, row.names = TRUE, col.names = TRUE, append = FALSE) 4.5 Plot the spectra 4.5.1 The spectra in absorbance # 04 Plot the Spectrum of the spectra ------------------------------------------ # 4.1 Plot the absorbance ###################################################### IRSpectra &lt;- as.data.frame(row.names(MIRspec_wl)) IRSpectra$wn &lt;- MIRspec_wn IRSpectra$wnA&lt;- log(1/MIRspec_wn) # Convert to wave number absorbance IRSpectra$wl &lt;- MIRspec_wl IRSpectra$wlA &lt;- log(1/MIRspec_wl) # Convert to wavelength absorbance rownames(IRSpectra) &lt;- IRSpectra$`row.names(MIRspec_wl)` png(&quot;Soil spectra in wavenumbers absorbance.png&quot;, width = 297, height = 210, units = &quot;mm&quot;, res = 300) pdf(&quot;Soil spectra in wavenumbers absorbance.pdf&quot;, width = 10*2, height = 6*2) matplot(x = colnames(IRSpectra$wn), y = t(IRSpectra$wnA), xlab = expression(paste(&quot;Wavenumber &quot;, &quot;(cm&quot;^&quot;-1&quot;, &quot;)&quot;)), ylab = &quot;Absorbance&quot;, type = &quot;l&quot;, #&quot;l&quot; = ligne lty = 1, col = 1:nrow(IRSpectra$wn)) dev.off() png(&quot;Soil spectra in wavelength absorbance.png&quot;, width = 297, height = 210, units = &quot;mm&quot;, res = 300) pdf(&quot;Soil spectra in wavelength absorbance.pdf&quot;, width = 10*2, height = 6*2) matplot(x = colnames(IRSpectra$wl), y = t(IRSpectra$wlA), xlab = &quot;Wavelength /nm &quot;, ylab = &quot;Absorbance&quot;, type = &quot;l&quot;, #&quot;l&quot; = ligne lty = 1, col = 1:nrow(IRSpectra$wl)) dev.off() 4.5.2 The spectra in reflectance # 4.2 Plot the reflectance ##################################################### png(&quot;Soil spectra in wavenumbers reflectance.png&quot;, width = 297, height = 210, units = &quot;mm&quot;, res = 300) pdf(&quot;Soil spectra in wavenumbers reflectance.pdf&quot;, width = 10*2, height = 6*2) matplot(x = colnames(IRSpectra$wn), y = t(IRSpectra$wn), xlab = expression(paste(&quot;Wavenumber &quot;, &quot;(cm&quot;^&quot;-1&quot;, &quot;)&quot;)), ylab = &quot;Reflectance&quot;, type = &quot;l&quot;, #&quot;l&quot; = ligne lty = 1, col = 1:nrow(IRSpectra$wn)) dev.off() png(&quot;Soil spectra in wavelength reflectance.png&quot;, width = 297, height = 210, units = &quot;mm&quot;, res = 300) pdf(&quot;Soil spectra in wavelength reflectance.pdf&quot;, width = 10*2, height = 6*2) matplot(x = colnames(IRSpectra$wl), y = t(IRSpectra$wl), xlab = &quot;Wavelength /nm &quot;, ylab = &quot;Reflectance&quot;, type = &quot;l&quot;, #&quot;l&quot; = ligne lty = 1, col = 1:nrow(IRSpectra$wl)) dev.off() 4.6 Kennard Stone sampling The Kennard stone sampling is used to select a wide range of a population that will represent the diversity of the individuals (Kennard and Stone 1969). This sampling strategy has proven to be efficient in the soils spectrometry context (Ramirez-Lopez et al. 2014). We selected different numbers of samples for each depth increment with an over-representation of the topsoil (0 - 10 cm). All the samples from 2017 - 2018 were analysed, so no sampling strategy has been applied to them. (#tab:sampling table)Ratio of C factor between different land uses. Depth (cm) Year Number 0 - 10 2022 30 10 - 30 2022 5 30 - 50 2022 5 50 - 70 2022 5 70 - 100 2022 5 0 - 10 2023 10 10 - 30 2023 4 30 - 50 2023 6 50 - 70 2023 6 70 - 100 2023 3 In the selection of samples, you can choose the year and depth increment. Here, we show a selection for the 2022 campaign at 0 - 10 cm depth. The table Samples_info use the information collected during the field campaign and the Field_observations file. The different entries are: Lab_ID Laboratory number gave at the samples when enter into TÃ¼bingen Soil Science and Geomrophology laboratory inventory. Site_name the site’s name according to the CLHS sampling. Depth_cm depth increment of the sampling (0 - 10 - 30 - 50 - 70 - 100 cm). X_WGS84 longitude in WGS84 (epsg:4326), measured from Garmin, GPSMAP 60Cx with ± 10 - 3 m accuracy (depending of satellite coverage). Y_WGS84 latitude in WGS84 (epsg:4326), measured from Garmin, GPSMAP 60Cx with ± 10 - 3 m accuracy (depending of satellite coverage). # 05 Kennard Stone sampling ---------------------------------------------------- # 5.1 Select the samples ####################################################### Samples_info &lt;- read_delim(&quot;./data/Samples_info.csv&quot;, delim = &quot;;&quot;) selection &lt;- Samples_info[Samples_info$Depth_cm == &quot;0_10&quot; &amp; grepl(&quot;B07_2022&quot;, Samples_info$Site_name), ] # You can select the year and the depth # Select the samples to analyse MIRsample &lt;- MIRspec_wl[rownames(MIRspec_wl) %in% selection$Lab_ID,] # 5.2 Run the sampling ######################################################### sample &lt;- kenStone(MIRsample, k = 30, metric = &quot;euclid&quot;) # Make samples with Kennard Stone in euclidian distance MIRsample &lt;- MIRsample[sample$model,] MIRsample &lt;- row.names(MIRsample) write.table(MIRsample, &quot;./export/samples/B07_2022 Samples 0 - 10 cm.txt&quot;, dec = &quot;.&quot;, sep = &quot;;&quot;, row.names = FALSE, col.names = FALSE, append = FALSE) References "],["laboratory-measurement-of-the-selected-soil-samples.html", "5 Laboratory measurement of the selected soil samples 5.1 Protocol 5.2 Properties 5.3 Non mesured samples 5.4 Texture mesurment 5.5 Table of the mesured samples 5.6 Properties of the measured samples", " 5 Laboratory measurement of the selected soil samples 5.1 Protocol Following the selection of samples with the Kennard Stone sampling strategy we had a total of 108 samples (29 for 2017 - 2018, 50 for 2022 and 29 for 2023) to measure in the laboratory. First, the samples were air dried (35 - 45?C) for 24 h, root fragments were removed, and sieved (&lt; 2 mm). Additionally, the samples measured with the elementary particles analyser, were ground under 1 µm with a Pulverisette 5/4, classic line (Fritsh, Idar-Oberstein, Germany) in a 250 ml hardness stainless steel container (ISO: X105CrMo17) with five 20 mm sintered corrodium (99.7% Al2O3) grinding balls. 5.2 Properties All the results from the laboratory measurement are accessible at: Bellat, Mathias; Glissmann, Benjamin; Rentschler, Tobias; Sconzo, Paola; Pfälzner, Peter; Brifkany, Bekas; Scholten, Thomas (2024): “Soil properties in the North-Western Kurdistan region, Iraq, derived from laboratory measurements [dataset]”. PANGAEA, https://doi.org/10.1594/PANGAEA.973701 We measured a total of 8 properties containing chemical, physical and biological attributes: pH measured with Potassium chloride (KCl) solution with ProfiLine pH 3310 and a WTW SenTix 81 pH electrode (Fisher Scientific, Strabourg, France). CaCO\\(_3\\) carbonate calcium calculate in purcent with a calcimeter 08.53 (Royal Eijkelkamp, Giesbeek, Netherlands). Nt total nitrate calculated in percent with a Vario EL III (Elementar, Hanau, Germany). Ct total carbon calculated in percent with a Vario EL III (Elementar, Hanau, Germany). St total sulfur calculated in percent with a Vario EL III (Elementar, Hanau, Germany). Corg total organic calculated in percent with a Vario EL III (Elementar, Hanau, Germany). EC electro-conductivity measured in micro-siemens per centimeters with a Cond 330i/340i (WTW, Weilheim in Oberbayern, Germany). Texture measured in percent either with wet sieving for the sand or with a SediGraph III 5120 combined with an autosampler MasterTech MT 052 for lower fractions (Micromeritics, Norcross, USA). 5.3 Non mesured samples Due to the low amount of sample material collected, some measurements were not possible to realise. All the 2017 - 2018 had not enough material ( &lt; 5 gr.), after texture analysis, to measure their electro-conductivity and samples 41614 and 53881 had less than 10 gr. collected, which was not enough for texture measurement. 5.4 Texture mesurment The texture was divided texture into diverse classes and we also measured the mean weight diameter (MWD) in mm with the following formula. \\[ MWD = Xi * Wi/100 \\] Where: Xi the average diameter in mm. Wi the percentage of aggregate. It can occurs that the total of the measured texture is not perfectly equal to 100%. (#tab:texture table)Texture classes. Texture class Diameter in mm Coarse sand 2 - 0.63 Medium sand 0.63 - 0.2 Fine sand 0.2 - 0.125 Fine fine sand 0.125 - 0.063 Total sand 2 - 0.063 Coarse silt 0.063 - 0.02 Medium silt 0.02 - 0.0063 Fine silt 0.0063 - 0.002 Total silt 0.063 - 0.002 Coarse clay 0.002 - 0.00063 Medium and fine clay 0.00063 - 0.000063 Total clay 0.002 - 0.000063 Total silt and clay 0.063 - 0.000063 Total sand, silt and clay 2 - 0.000063 5.5 Table of the mesured samples 5.6 Properties of the measured samples Vario EL III analyser is not able to measure Nt content lower than 0.03% and St content lower than 0.05%. Therefore, the Sulfur was not predicted as its values were to low. # Remove the non measured samples Lab$Nt[Lab$Nt== &quot;&lt; 0.03&quot;] &lt;- NA Lab$Nt &lt;- as.numeric(Lab$Nt) Lab$St[Lab$St == &quot;&lt; 0.05&quot;] &lt;- NA Lab$St &lt;- as.numeric(Lab$St) # Component values ====================================== par(mfrow = c(2, 6), mar = c(5.1, 4.1, 4.1, .1), oma = c(1, 2.5, 2, 5.9), mgp = c(3, 1, 0), las = 0, cex.lab = 1, cex.axis = 1, cex.lab = 1.5, xpd = FALSE) # c(min(Soil_Data_Observed$pH)-0.1, max(Soil_Data_Observed$pH)+0.1)) boxplot(Lab$pH, xlab = &quot;pH&quot;, ylab = &quot;&quot;, ylim = c(min(Lab$pH)-0.1, max(Lab$pH)+0.1)) boxplot(Lab$CaCO3, xlab = expression(&quot;CaCO&quot;[3]), ylab = &quot;&quot;, ylim = c(0,100)); title(ylab = &quot;%&quot;, line = 2.25) boxplot(Lab$Nt, xlab = &quot;Nt&quot;, ylab = &quot;&quot;, ylim = c(0, 0.3)); title(ylab = &quot;%&quot;, line = 2.25) boxplot(Lab$St, xlab = &quot;St&quot;, ylab = &quot;&quot;, ylim = c(0,0.3)); title(ylab = &quot;%&quot;, line = 2.25) boxplot(Lab$Ct, xlab = &quot;Ct&quot;, ylab = &quot;&quot;, ylim = c(0,15)); title(ylab = &quot;%&quot;, line = 2.25) boxplot(Lab$Corg, xlab = &quot;Corg&quot;, ylab = &quot;&quot;, ylim = c(min(Lab$Corg)-0.01, 6)); title(ylab = &quot;%&quot;, line = 2.25) boxplot(Lab$EC, xlab = &quot;EC&quot;, ylab = &quot;&quot;, ylim = c(50, 950)); title(ylab = &quot;ÂµS/cm&quot;, line = 2.25) boxplot(Lab$Σsand, xlab = &quot;Sand&quot;, ylab = &quot;&quot;, ylim = c(0, 100)); title(ylab = &quot;%&quot;, line = 2.25) boxplot(Lab$`Σsilt `, xlab = &quot;Silt&quot;, ylab = &quot;&quot;, ylim = c(0, 100)); title(ylab = &quot;%&quot;, line = 2.25) boxplot(Lab$Σclay, xlab = &quot;Clay&quot;, ylab = &quot;&quot;, ylim = c(0, 100)); title(ylab = &quot;%&quot;, line = 2.25) boxplot(Lab$MWD, xlab = &quot;MWD&quot;, ylab = &quot;&quot;, ylim = c(0, 0.5)); title(ylab = &quot;mm&quot;, line = 2.25) mtext(&quot;Boxplots of the measured values&quot;, outer = TRUE, line = 0.5, cex = 1) We ploted the soil texture according to (WRB 2006) classification for each soil depth increment. library(soiltexture) #Soil texture texture &lt;- data.frame(&quot;SAND&quot; = Lab$Σsand, &quot;SILT&quot; = Lab$`Σsilt `, &quot;CLAY&quot; = Lab$Σclay) row.names(texture) &lt;- Lab$`Lab label` texture &lt;- na.omit(texture) texture &lt;- TT.normalise.sum(texture, css.names = c(&quot;SAND&quot;,&quot;SILT&quot;, &quot;CLAY&quot;)) texture &lt;- TT.text.transf( tri.data = texture, dat.css.ps.lim = c(0, 0.002, 0.063, 2), # German system base.css.ps.lim = c(0, 0.002, 0.05, 2) # USDA system ) TT.plot(class.sys = &quot;USDA.TT&quot;, tri.data = texture, main = &quot;&quot;, frame.bg.col = &quot;#f2f2f2&quot;, grid.show = FALSE, arrows.show = FALSE, col = &quot;#c96dc4&quot;, pch = 19, cex = 1, css.lab = c(&quot;Clay&quot;, &quot;Silt&quot;, &quot;Clay&quot;)) #Soil texture texture &lt;- data.frame(&quot;SAND&quot; = Lab$Σsand, &quot;SILT&quot; = Lab$`Σsilt `, &quot;CLAY&quot; = Lab$Σclay, &quot;depth&quot; = Lab$`Depth, bot`) row.names(texture) &lt;- Lab$`Lab label` texture &lt;- na.omit(texture) texture_resize &lt;- texture[,c(1:3)] texture_resize &lt;- TT.normalise.sum(texture_resize, css.names = c(&quot;SAND&quot;,&quot;SILT&quot;, &quot;CLAY&quot;)) texture_resize &lt;- TT.text.transf( tri.data = texture_resize, dat.css.ps.lim = c(0, 0.002, 0.063, 2), # German system base.css.ps.lim = c(0, 0.002, 0.05, 2) # USDA system ) texture &lt;- cbind(texture_resize, texture[,4]) text.zero &lt;- texture[texture[,4] == 0.1,] text.ten &lt;- texture[texture[,4] == 0.3,] text.thirty &lt;- texture[texture[,4] == 0.5,] text.fifty &lt;- texture[texture[,4] == 0.7,] text.seventy &lt;- texture[texture[,4] == 1,] par(mfrow = c(2, 3)) TT.plot(class.sys = &quot;USDA.TT&quot;, tri.data = text.zero, main = &quot;0 - 10 cm&quot;, frame.bg.col = &quot;#f2f2f2&quot;, grid.show = FALSE, arrows.show = FALSE, col = &quot;#c96dc4&quot;, pch = 19, cex = 1, css.lab = c(&quot;Clay&quot;, &quot;Silt&quot;, &quot;Clay&quot;)) TT.plot(class.sys = &quot;USDA.TT&quot;, tri.data = text.ten, main = &quot;10 - 30 cm&quot;, frame.bg.col = &quot;#f2f2f2&quot;, grid.show = FALSE, arrows.show = FALSE, col = &quot;purple&quot;, pch = 19, cex = 1, css.lab = c(&quot;Clay&quot;, &quot;Silt&quot;, &quot;Clay&quot;)) TT.plot(class.sys = &quot;USDA.TT&quot;, tri.data = text.thirty, main = &quot;30 - 50 cm&quot;, frame.bg.col = &quot;#f2f2f2&quot;, grid.show = FALSE, arrows.show = FALSE, col = &quot;purple&quot;, pch = 19, cex = 1, css.lab = c(&quot;Clay&quot;, &quot;Silt&quot;, &quot;Clay&quot;)) TT.plot(class.sys = &quot;USDA.TT&quot;, tri.data = text.fifty, main = &quot;50 - 70 cm&quot;, frame.bg.col = &quot;#f2f2f2&quot;, grid.show = FALSE, arrows.show = FALSE, col = &quot;purple&quot;, pch = 19, cex = 1, css.lab = c(&quot;Clay&quot;, &quot;Silt&quot;, &quot;Clay&quot;)) TT.plot(class.sys = &quot;USDA.TT&quot;, tri.data = text.seventy, main = &quot;70 - 100 cm&quot;, frame.bg.col = &quot;#f2f2f2&quot;, grid.show = FALSE, arrows.show = FALSE, col = &quot;purple&quot;, pch = 19, cex = 1, css.lab = c(&quot;Clay&quot;, &quot;Silt&quot;, &quot;Clay&quot;)) References "],["Spectrapred.html", "6 Spectra model prediction 6.1 Model implementation 6.2 Model evaluation 6.3 Predicted values 6.4 Spatial ploting", " 6 Spectra model prediction We used only the raw spectra and three spectra transformations from the 15 spectra produced in the FTIR part. This transformation are improving the quality of the prediction model (Ludwig et al. 2023). In total we had four spectra: Raw spectra. SG-2-11 the Savitzky-Golay with a polynomial order of 2 and a window size of 11. Moving average of 11. -SNV-SG standard normal variate transformation on the Savitzky-Golay with a polynomial order of 2 and a window size of 11. 6.1 Model implementation We implemented the prediction model in Python language. A sanity check was performed to assess the conformity of the data set Sanity_Check_code. Density plot of pH We split the data into an 85% training set and a 15% testing set. As EC values had outliers we removed all samples with values higher than 400 µS/cm: 53802 (827 µS/cm), 53871 (857 µS/cm), 53872 (932 µS/cm), 53902 (718 µS/cm), 53938 (447 µS/cm), 53954 (426 µS/cm) and 55852 (636 µS/cm). All the properties were predicted using the Cubist model under Python environment Cubist_Kfold_code. A 5 time stratified k-fold was used with a total of 9 combinations for the hyperparameter. The stratified k-fold helps have a higher diversity of sampling in the training and testing set. Table 6.1: Hyperparamters of the Cubist model. n_rules n_committees 20 5 30 5 40 5 20 10 30 10 40 10 20 15 30 15 40 15 6.2 Model evaluation 6.2.1 All spectra tranformations metrics (#tab:df raw show)Cubist model evaluation metrics on raw spectra. Metric MSE MAE R2 RMSE CCC RPIQ pH 0.0219010 0.1171735 0.4401298 0.1479898 0.6082283 1.2423942 CaCO3 16.9688594 2.8067966 0.8882458 4.1193275 0.9485314 2.3192395 Nt 0.0029586 0.0325372 0.6229612 0.0543926 0.7375194 1.4595850 Ct 0.3130813 0.3976554 0.8913525 0.5595367 0.9495172 2.3885632 Corg 0.4488793 0.4225908 0.5264148 0.6699846 0.6461363 0.9218613 EC 4402.7957520 52.3580817 0.1141583 66.3535662 0.3278390 0.9523699 Sand 54.1693978 5.1840530 0.7674734 7.3599863 0.8707758 2.3624346 Silt 86.6205805 6.5901216 0.2136148 9.3070178 0.4181861 0.8782774 Clay 63.5131858 5.2104216 0.6930271 7.9695160 0.8342033 2.3912120 MWD 0.0029143 0.0373586 0.4745772 0.0539841 0.6278402 1.2472350 \\[\\\\[1.5cm]\\] (#tab:df moving show)Cubist model evaluation metrics on mooving average 11 transformed spectra. Metric MSE MAE R2 RMSE CCC RPIQ pH 0.0210390 0.1141724 0.4621653 0.1450483 0.6475573 1.221859 CaCO3 17.9060238 2.8147659 0.8820737 4.2315510 0.9460955 2.389729 Nt 0.0028760 0.0288061 0.6334873 0.0536280 0.7473087 1.410630 Ct 0.2587904 0.3585550 0.9101929 0.5087145 0.9600705 2.228431 Corg 0.4632358 0.4024382 0.5112682 0.6806143 0.6651811 1.087405 EC 4018.7082500 48.6017677 0.1914366 63.3932824 0.4001273 1.114983 Sand 62.0563611 5.4404186 0.7336180 7.8775860 0.8500844 2.300951 Silt 98.1877111 7.2681208 0.1086026 9.9089712 0.3801260 0.875066 Clay 49.4520568 4.9724694 0.7609876 7.0322156 0.8670415 2.343124 MWD 0.0029262 0.0377074 0.4724332 0.0540942 0.6365206 1.331620 \\[\\\\[1.5cm]\\] (#tab:df SG show)Cubist model evaluation metrics on SG transformed spectra. Metric MSE MAE R2 RMSE CCC RPIQ pH 0.0231528 0.1177269 0.4081296 0.1521604 0.5814332 0.9712110 CaCO3 14.9984789 2.6152336 0.9012224 3.8727870 0.9545313 2.7690478 Nt 0.0050750 0.0467659 0.3532399 0.0712392 0.4579116 0.4787131 Ct 0.4564530 0.4259481 0.8415988 0.6756130 0.9176226 1.8099295 Corg 0.7257209 0.5595446 0.2343362 0.8518925 0.3608645 0.3591207 EC 4740.0612760 55.0806415 0.0463005 68.8481029 0.3026389 0.9539687 Sand 55.5155791 5.6998147 0.7616948 7.4508777 0.8668046 2.4868337 Silt 74.3838628 6.0993695 0.3247059 8.6246080 0.5037383 0.9129198 Clay 58.4387460 5.9952896 0.7175530 7.6445239 0.8360514 2.4276119 MWD 0.0031091 0.0395931 0.4394541 0.0557593 0.6241677 1.1867736 \\[\\\\[1.5cm]\\] (#tab:df SNV show)Cubist model evaluation metrics on SNV-SG transformed spectra. Metric MSE MAE R2 RMSE CCC RPIQ pH 0.0235093 0.1187346 0.3990151 0.1533275 0.6013606 0.8891613 CaCO3 16.8746809 2.8344541 0.8888660 4.1078803 0.9489888 2.7618729 Nt 0.0013671 0.0217745 0.8257789 0.0369741 0.8966433 1.7119021 Ct 0.2524407 0.3139760 0.9123964 0.5024348 0.9616470 2.7212452 Corg 0.3671160 0.3711759 0.6126784 0.6059009 0.7584114 1.1525632 EC 3507.4818920 45.0053330 0.2942953 59.2239976 0.4909250 0.9349021 Sand 44.1217193 4.7525580 0.8106039 6.6424182 0.8969430 2.6565763 Silt 68.4702337 6.2101832 0.3783928 8.2746742 0.5599477 0.9772474 Clay 63.5502746 5.6798514 0.6928479 7.9718426 0.8251282 2.2699483 MWD 0.0029995 0.0373378 0.4592173 0.0547675 0.6325722 1.1640412 6.2.2 Selected transformed spectra Regarding the results of the predictions, we selected for each variables the following spectra: pH Raw spectra. CaCO\\(_3\\) SG.2.11 spectra. Nt SNV-SG spectra. Ct SNV-SG spectra. SOC SNV-SG spectra. EC SNV-SG spectra. Sand SNV-SG spectra. Silt SNV-SG spectra. Clay SG.2.11 spectra. MWD Raw spectra. ## Warning: Removed 39 rows containing missing values or values outside the scale range ## (`geom_point()`). 6.3 Predicted values 6.3.1 Full predicted values table The full table of 2022 and 2023 samples predicted is visible below or can be found on the deposit as Mir_spectra_prediction file or at : Bellat, Mathias; Glissmann, Benjamin; Rentschler, Tobias; Sconzo, Paola; PfÃ¤lzner, Peter; Brifkany, Bekas; Scholten, Thomas (2024): “Soil properties predicted on mid-infrared (MIR) spectroscopy measurements in North-Western Kurdistan region, Iraq [dataset]”. PANGAEA, https://doi.org/10.1594/PANGAEA.973700 We removed the sample with ID 55864 which is an outlier. 6.3.2 Boxplot of the predicted values \\[\\\\[1.5cm]\\] \\[\\\\[1.5cm]\\] \\[\\\\[1.5cm]\\] \\[\\\\[1.5cm]\\] \\[\\\\[1.5cm]\\] 6.3.3 Texture of the predicted samples We ploted the soil texture according to (WRB 2006) classification. \\[\\\\[1.5cm]\\] \\[\\\\[1.5cm]\\] \\[\\\\[1.5cm]\\] \\[\\\\[1.5cm]\\] \\[\\\\[1.5cm]\\] 6.4 Spatial ploting 6.4.1 0 - 10 cm depth 6.4.2 10 - 30 cm depth 6.4.3 30 - 50 cm depth 6.4.4 50 - 70 cm depth 6.4.5 70 - 100 cm depth References "],["digital-soil-mapping.html", "7 Digital soil mapping 7.1 Introduction 7.2 Soil depth mapping preparation 7.3 Models developpment 7.4 Model evaluation 7.5 Prediction of the area 7.6 Visualisation and comparison with SoilGrid product", " 7 Digital soil mapping 7.1 Introduction 7.1.1 Purpose We present here the methodology for the digital soil mapping based on the soil values from the spectra prediction. 7.1.2 Covariates We used a set of 80 covariates mainly based on Zolfaghari Nia et al. (2022). All the data listed below are available freely online excepted for the digitized maps (geology, geohydrology and geomorphology). (#tab:input_4)Data used in the production of the conditioned soil depth mapping. Name/ID Original resolution (m) Type/Unit Source Landsat 8 Blue/LA.1 30 0.45 - 0.51 µm EROS (2020) Landsat 8 Green/LA.2 30 0.53 - 0.59 µm EROS (2020) Landsat 8 NDVI/LA.3 30 - EROS (2020) Landsat 8 NDWI/LA.4 30 - EROS (2020) Landsat 8 NIR/LA.5 30 0.85 - 0.88 µm EROS (2020) Landsat 8 Panchromatic/LA.6 15 0.52 - 0.90 µm EROS (2020) Landsat 8 Red/LA.7 30 0.64 - 0.67 µm EROS (2020) Landsat 8 SWIR1/LA.8 30 1.57 - 1.65 µm EROS (2020) Landsat 8 SWIR2/LA.9 30 2.11 - 2.29 µm EROS (2020) Landsat 8 EVI/LA.10 30 - EROS (2020) Landsat 8 SAVI/LA.11 30 - EROS (2020) Landsat 8 NDMI/LA.12 30 - EROS (2020) Landsat 8 CORSI/LA.13 30 - EROS (2020) Landsat 8 Brightness index/LA.14 30 - EROS (2020) Landsat 8 Clay index/LA.15 30 - EROS (2020) Landsat 8 Salinity index/LA.16 30 - EROS (2020) Landsat 8 Carbonate index/LA.17 30 - EROS (2020) Landsat 8 Gypsum index/LA.18 30 - EROS (2020) MODIS EVI/MO.1 250 - Didan (2021) MODIS LST day/MO.2 1000 °C Wan, Hook, and Hulley (2021) MODIS LST night/MO.2 1000 °C Wan, Hook, and Hulley (2021) MODIS NDVI/MO.4 250 - Didan (2021) MODIS NIR/MO.5 250 0.841 - 0.876 µm Vermote (2021) MODIS Red/MO.6 250 0.62 - 0.67 µm Vermote (2021) MODIS SAVI/MO.7 250 Meters Didan (2021) MODIS Brightness index/MO.8 250 35 classes Vermote (2021) Distance rivers/OT.1 25 17 classes ESA and Airbus (2022) Geology/OT.2 1 : 250 000 11 classes Sissakian, Hagopian, and Hasan (1995), al-mousawi_geological_2007 Geomorphology/OT.3 1 : 250 000 mm Forti et al. (2021) Landuses/OT.4 10 mm Zanaga et al. (2021) PET sum/OT.5 750 Kj m\\(^{-2}\\) Zomer and Trabucco (2022) Prec. sum/OT.6 1000 °C Fick and Hijmans (2017) SRAD sum/OT.7 1000 m s\\(^{-1}\\) Fick and Hijmans (2017) Diff Max. Min. Temp./OT.8 1000 0.492 - 0.496 µm Fick and Hijmans (2017) Wind sum/OT.9 1000 0.559 - 0.560 µm Fick and Hijmans (2017) Sentinel 2 Blue/SE.1 10 - European Space Agency (2022) Sentinel 2 Green/SE.2 10 - European Space Agency (2022) Sentinel 2 NDVI/SE.3 20 0.833 - 0.835 µm European Space Agency (2022) Sentinel 2 NDWI/SE.4 20 0.665 - 0.664 µm European Space Agency (2022) Sentinel 2 NIR/SE.5 10 0.738 - 0.739 µm European Space Agency (2022) Sentinel 2 Red/SE.6 10 0.739 - 0.740 µm European Space Agency (2022) Sentinel 2 RedEdge1/SE.7 20 0.779 - 0.782 µm European Space Agency (2022) Sentinel 2 RedEdge2/SE.8 20 1.610 - 1.613 µm European Space Agency (2022) Sentinel 2 RedEdge3/SE.9 20 2.185 - 2.202 µm European Space Agency (2022) Sentinel 2 SWIR1/SE.10 20 0.943 - 0.945 µm European Space Agency (2022) Sentinel 2 SWIR2/SE.11 20 - European Space Agency (2022) Sentinel 2 water vapor/SE.12 90 - European Space Agency (2022) Sentinel 2 EVI/SE.13 20 - European Space Agency (2022) Sentinel 2 TVI/SE.14 20 - European Space Agency (2022) Sentinel 2 SAVI/SE.15 20 - European Space Agency (2022) Sentinel 2 LSWI/SE.16 20 - European Space Agency (2022) Sentinel 2 Clay index/SE.17 20 - European Space Agency (2022) Sentinel 2 Brightness index/SE.18 20 - European Space Agency (2022) Sentinel 2 Salinity index/SE.19 20 - European Space Agency (2022) Sentinel 2 Carbonate index/SE.20 20 Radians European Space Agency (2022) Sentinel 2 Gypsum index/SE.21 20 - European Space Agency (2022) Aspect/TE.1 25 - SAGA GIS / ESA and Airbus (2022) Channel network base level/TE.2 25 - SAGA GIS / ESA and Airbus (2022) Channel network distance/TE.3 25 Meters SAGA GIS / ESA and Airbus (2022) Connexity/TE.4 25 - SAGA GIS / ESA and Airbus (2022) DEM/TE.5 25 - SAGA GIS / ESA and Airbus (2022) Flow accumaltion/TE.6 25 - SAGA GIS / ESA and Airbus (2022) General curvature/TE.7 25 - SAGA GIS / ESA and Airbus (2022) MrRTF/TE.8 25 Radians SAGA GIS / ESA and Airbus (2022) MrVBF/TE.9 25 - SAGA GIS / ESA and Airbus (2022) Negative openness/TE.10 25 - SAGA GIS / ESA and Airbus (2022) Normalized height/TE.11 25 Radians SAGA GIS / ESA and Airbus (2022) Plan curvature/TE.12 25 - SAGA GIS / ESA and Airbus (2022) Positive openness/TE.13 25 - SAGA GIS / ESA and Airbus (2022) Profile curvature/TE.14 25 Radians SAGA GIS / ESA and Airbus (2022) Slope height/TE.15 25 - SAGA GIS / ESA and Airbus (2022) Slope/TE.16 25 - SAGA GIS / ESA and Airbus (2022) Standardized height/TE.17 25 - SAGA GIS / ESA and Airbus (2022) Surface landform/TE.18 25 - SAGA GIS / ESA and Airbus (2022) Terrain ruggedness index/TE.19 25 - SAGA GIS / ESA and Airbus (2022) Terrain texture/TE.20 25 - SAGA GIS / ESA and Airbus (2022) TPI/TE.21 25 - SAGA GIS / ESA and Airbus (2022) TWI/TE.22 25 - SAGA GIS / ESA and Airbus (2022) Total catchment area/TE.23 25 0.45 - 0.51 µm SAGA GIS / ESA and Airbus (2022) Valley depth/TE.24 25 0.53 - 0.59 µm SAGA GIS / ESA and Airbus (2022) 7.1.2.1 Terrain For the DEM and all the derivatives, we used SAGA GIS 9.3.1 software and all the specificties of the batch process are detailed below. The last LS factor corresponds to the Total catchment area covariate. [2024-12-24/15:17:21] [Fill Sinks (Wang &amp; Liu)] Execution started... __________ [Fill Sinks (Wang &amp; Liu)] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y DEM: DEM_GLO_25 Filled DEM: Filled DEM Flow Directions: Flow Directions Watershed Basins: Watershed Basins Minimum Slope [Degree]: 0.1 __________ total execution time: 9000 milliseconds (09s) [2024-12-24/15:17:30] [Fill Sinks (Wang &amp; Liu)] Execution succeeded (09s) [2024-12-24/15:18:59] [Simple Filter] Execution started... __________ [Simple Filter] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Grid: DEM_GLO_25 [no sinks] Filtered Grid: &lt;not set&gt; Filter: Smooth Kernel Type: Square Radius: 3 __________ total execution time: 4000 milliseconds (04s) [2024-12-24/15:19:03] [Simple Filter] Execution succeeded (04s) [2024-12-24/15:20:52] [Simple Filter] Execution started... __________ [Simple Filter] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Grid: Filtered Grid Filtered Grid: Filtered Grid Filter: Smooth Kernel Type: Square Radius: 3 __________ total execution time: 3000 milliseconds (03s) [2024-12-24/15:20:55] [Simple Filter] Execution succeeded (03s) [2024-12-24/15:22:20] [Slope, Aspect, Curvature] Execution started... __________ [Slope, Aspect, Curvature] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Elevation: Filtered Grid Slope: Slope Aspect: Aspect General Curvature: General Curvature Profile Curvature: Profile Curvature Plan Curvature: Plan Curvature Tangential Curvature: &lt;not set&gt; Longitudinal Curvature: &lt;not set&gt; Cross-Sectional Curvature: &lt;not set&gt; Minimal Curvature: &lt;not set&gt; Maximal Curvature: &lt;not set&gt; Total Curvature: &lt;not set&gt; Flow Line Curvature: &lt;not set&gt; Method: 9 parameter 2nd order polynom (Zevenbergen &amp; Thorne 1987) Unit: radians Unit: radians __________ total execution time: 1000 milliseconds (01s) [2024-12-24/15:22:21] [Slope, Aspect, Curvature] Execution succeeded (01s) [2024-12-24/15:24:02] [Channel Network and Drainage Basins] Execution started... __________ [Channel Network and Drainage Basins] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Elevation: Filtered Grid Flow Direction: Flow Direction Flow Connectivity: &lt;not set&gt; Strahler Order: &lt;not set&gt; Drainage Basins: Drainage Basins Channels: Channels Drainage Basins: Drainage Basins Junctions: &lt;not set&gt; Threshold: 5 Subbasins: true __________ [Vectorizing Grid Classes] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Grid: Drainage Basins Polygons: Polygons Class Selection: all classes Vectorised class as...: one single (multi-)polygon object Keep Vertices on Straight Lines: false [Vectorizing Grid Classes] execution time: 06s __________ total execution time: 12000 milliseconds (12s) [2024-12-24/15:24:15] [Channel Network and Drainage Basins] Execution succeeded (12s) [2024-12-24/15:30:53] [Channel Network] Execution started... __________ [Channel Network] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Elevation: Filtered Grid Flow Direction: Flow Direction Channel Network: Channel Network Channel Direction: Channel Direction Channel Network: Channel Network Initiation Grid: Filtered Grid Initiation Type: Greater than Initiation Threshold: 0 Divergence: &lt;not set&gt; Tracing: Max. Divergence: 5 Tracing: Weight: &lt;not set&gt; Min. Segment Length: 10 __________ total execution time: 171000 milliseconds (02m 51s) [2024-12-24/15:33:44] [Channel Network] Execution succeeded (02m 51s) [2024-12-24/15:35:02] [Multiresolution Index of Valley Bottom Flatness (MRVBF)] Execution started... __________ [Multiresolution Index of Valley Bottom Flatness (MRVBF)] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Elevation: Filtered Grid MRVBF: MRVBF MRRTF: MRRTF Initial Threshold for Slope: 16 Threshold for Elevation Percentile (Lowness): 0.4 Threshold for Elevation Percentile (Upness): 0.35 Shape Parameter for Slope: 4 Shape Parameter for Elevation Percentile: 3 Update Views: true Classify: false Maximum Resolution (Percentage): 100 step: 1, resolution: 25.00, threshold slope 16.00 step: 2, resolution: 25.00, threshold slope 8.00 step: 3, resolution: 75.00, threshold slope 4.00 step: 4, resolution: 224.99, threshold slope 2.00 step: 5, resolution: 674.96, threshold slope 1.00 step: 6, resolution: 2024.88, threshold slope 0.50 step: 7, resolution: 6074.63, threshold slope 0.25 step: 8, resolution: 18223.88, threshold slope 0.12 step: 9, resolution: 54671.63, threshold slope 0.06 step: 10, resolution: 164014.89, threshold slope 0.03 __________ total execution time: 117000 milliseconds (01m 57s) [2024-12-24/15:36:59] [Multiresolution Index of Valley Bottom Flatness (MRVBF)] Execution succeeded (01m 57s) [2024-12-24/15:42:42] [Topographic Openness] Execution started... __________ [Topographic Openness] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Elevation: Filtered Grid Positive Openness: Positive Openness Negative Openness: Negative Openness Radial Limit: 10000 Directions: all Number of Sectors: 8 Method: line tracing Unit: Radians Difference from Nadir: true __________ total execution time: 134000 milliseconds (02m 14s) [2024-12-24/15:44:56] [Topographic Openness] Execution succeeded (02m 14s) [2024-12-24/15:45:57] [Vertical Distance to Channel Network] Execution started... __________ [Vertical Distance to Channel Network] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Elevation: Filtered Grid Channel Network: Channel Network Vertical Distance to Channel Network: Vertical Distance to Channel Network Channel Network Base Level: Channel Network Base Level Tension Threshold: 1 Maximum Iterations: 0 Keep Base Level below Surface: true Level: 12; Iterations: 1; Maximum change: 0.000000 Level: 11; Iterations: 1; Maximum change: 0.000000 Level: 10; Iterations: 1; Maximum change: 0.000000 Level: 9; Iterations: 1; Maximum change: 0.000000 Level: 8; Iterations: 1; Maximum change: 0.000000 Level: 7; Iterations: 1; Maximum change: 0.000000 Level: 6; Iterations: 1; Maximum change: 0.000000 Level: 5; Iterations: 2; Maximum change: 0.000000 Level: 4; Iterations: 5; Maximum change: 0.646118 Level: 3; Iterations: 7; Maximum change: 0.756226 Level: 2; Iterations: 12; Maximum change: 0.957214 Level: 1; Iterations: 18; Maximum change: 0.973633 __________ total execution time: 14000 milliseconds (14s) [2024-12-24/15:46:11] [Vertical Distance to Channel Network] Execution succeeded (14s) [2024-12-24/15:47:23] [Terrain Surface Convexity] Execution started... __________ [Terrain Surface Convexity] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Elevation: Filtered Grid Convexity: Convexity Laplacian Filter Kernel: conventional four-neighbourhood Type: convexity Flat Area Threshold: 0 Scale (Cells): 10 Method: resampling Weighting Function: gaussian Bandwidth: 0.7 __________ total execution time: 2000 milliseconds (02s) [2024-12-24/15:47:25] [Terrain Surface Convexity] Execution succeeded (02s) [2024-12-24/15:48:23] [Flow Accumulation (One Step)] Execution started... __________ [Flow Accumulation (One Step)] Parameters: Elevation: Filtered Grid Flow Accumulation: Flow Accumulation Specific Catchment Area: &lt;not set&gt; Preprocessing: Sink Removal Flow Routing: Multiple Flow Direction __________ [Sink Removal] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y DEM: Filtered Grid Sink Route: &lt;not set&gt; Preprocessed DEM: Preprocessed DEM Method: Fill Sinks Threshold: false number of processed sinks: 1175 [Sink Removal] execution time: 17s __________ [Flow Accumulation (Top-Down)] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Elevation: Filtered Grid [no sinks] Sink Routes: &lt;not set&gt; Weights: &lt;not set&gt; Flow Accumulation: Flow Accumulation Input for Mean over Catchment: &lt;not set&gt; Material for Accumulation: &lt;not set&gt; Step: 1 Flow Accumulation Unit: cell area Flow Path Length: &lt;not set&gt; Channel Direction: &lt;not set&gt; Method: Multiple Flow Direction Thresholded Linear Flow: false Convergence: 1.1 Contour Length: false [Flow Accumulation (Top-Down)] execution time: 12s __________ total execution time: 29000 milliseconds (29s) [2024-12-24/15:48:52] [Flow Accumulation (One Step)] Execution succeeded (29s) [2024-12-24/15:52:00] [Relative Heights and Slope Positions] Execution started... __________ [Relative Heights and Slope Positions] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Elevation: Filtered Grid Slope Height: Slope Height Valley Depth: Valley Depth Normalized Height: Normalized Height Standardized Height: Standardized Height Mid-Slope Positon: Mid-Slope Positon w: 0.5 t: 10 e: 2 [2024-12-24/15:52:00] Pass 1 [2024-12-24/15:53:51] Pass 2 __________ total execution time: 257000 milliseconds (04m 17s) [2024-12-24/15:56:18] [Relative Heights and Slope Positions] Execution succeeded (04m 17s) [2024-12-24/22:38:34] [TPI Based Landform Classification] Execution started... __________ [TPI Based Landform Classification] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Elevation: Filtered Grid Landforms: Landforms Small Scale: 0; 100 Large Scale: 0; 1000 Weighting Function: no distance weighting __________ [Topographic Position Index (TPI)] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Elevation: Filtered Grid Topographic Position Index: Topographic Position Index Standardize: true Scale: 0; 100 Weighting Function: no distance weighting [Topographic Position Index (TPI)] execution time: 03s __________ [Topographic Position Index (TPI)] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Elevation: Filtered Grid Topographic Position Index: Topographic Position Index Standardize: true Scale: 0; 1000 Weighting Function: no distance weighting [Topographic Position Index (TPI)] execution time: 05m 07s __________ total execution time: 311000 milliseconds (05m 11s) [2024-12-24/22:43:45] [TPI Based Landform Classification] Execution succeeded (05m 11s) [2024-12-24/22:44:56] [Terrain Surface Texture] Execution started... __________ [Terrain Surface Texture] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Elevation: Filtered Grid Texture: Texture Flat Area Threshold: 1 Scale (Cells): 10 Method: resampling Weighting Function: gaussian Bandwidth: 0.7 __________ total execution time: 3000 milliseconds (03s) [2024-12-24/22:45:00] [Terrain Surface Texture] Execution succeeded (03s) [2024-12-24/22:45:43] [Terrain Ruggedness Index (TRI)] Execution started... __________ [Terrain Ruggedness Index (TRI)] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Elevation: Filtered Grid Terrain Ruggedness Index (TRI): Terrain Ruggedness Index (TRI) Search Mode: Circle Search Radius: 1 Weighting Function: no distance weighting __________ total execution time: 0 milliseconds (less than 1 millisecond) [2024-12-24/22:45:43] [Terrain Ruggedness Index (TRI)] Execution succeeded (less than 1 millisecond) [2024-12-24/22:46:50] [Topographic Position Index (TPI)] Execution started... __________ [Topographic Position Index (TPI)] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Elevation: Filtered Grid Topographic Position Index: Topographic Position Index Standardize: false Scale: 0; 100 Weighting Function: no distance weighting __________ total execution time: 3000 milliseconds (03s) [2024-12-24/22:46:53] [Topographic Position Index (TPI)] Execution succeeded (03s) [2024-12-24/22:47:22] [Topographic Wetness Index (One Step)] Execution started... __________ [Topographic Wetness Index (One Step)] Parameters: Elevation: Filtered Grid Topographic Wetness Index: Topographic Wetness Index Flow Distribution: Multiple Flow Direction __________ [Sink Removal] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y DEM: Filtered Grid Sink Route: &lt;not set&gt; Preprocessed DEM: Preprocessed DEM Method: Fill Sinks Threshold: false number of processed sinks: 1175 [Sink Removal] execution time: 16s __________ [Flow Accumulation (Top-Down)] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Elevation: Filtered Grid [no sinks] Sink Routes: &lt;not set&gt; Weights: &lt;not set&gt; Flow Accumulation: Flow Accumulation Input for Mean over Catchment: &lt;not set&gt; Material for Accumulation: &lt;not set&gt; Step: 1 Flow Accumulation Unit: cell area Flow Path Length: &lt;not set&gt; Channel Direction: &lt;not set&gt; Method: Multiple Flow Direction Thresholded Linear Flow: false Convergence: 1.1 Contour Length: false [Flow Accumulation (Top-Down)] execution time: 11s __________ [Flow Width and Specific Catchment Area] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Elevation: Filtered Grid [no sinks] Flow Width: Flow Width Total Catchment Area (TCA): Flow Accumulation Specific Catchment Area (SCA): Specific Catchment Area (SCA) Coordinate Unit: meter Method: Aspect [Flow Width and Specific Catchment Area] execution time: 01s __________ [Slope, Aspect, Curvature] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Elevation: Filtered Grid [no sinks] Slope: Slope Aspect: Aspect General Curvature: &lt;not set&gt; Profile Curvature: &lt;not set&gt; Plan Curvature: &lt;not set&gt; Tangential Curvature: &lt;not set&gt; Longitudinal Curvature: &lt;not set&gt; Cross-Sectional Curvature: &lt;not set&gt; Minimal Curvature: &lt;not set&gt; Maximal Curvature: &lt;not set&gt; Total Curvature: &lt;not set&gt; Flow Line Curvature: &lt;not set&gt; Method: 9 parameter 2nd order polynom (Zevenbergen &amp; Thorne 1987) Unit: radians Unit: radians [Slope, Aspect, Curvature] execution time: less than 1 millisecond __________ [Topographic Wetness Index] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Slope: Slope Catchment Area: Specific Catchment Area (SCA) Transmissivity: &lt;not set&gt; Topographic Wetness Index: Topographic Wetness Index Area Conversion: no conversion (areas already given as specific catchment area) Method (TWI): Standard [Topographic Wetness Index] execution time: 01s __________ total execution time: 29000 milliseconds (29s) [2024-12-24/22:47:51] [Topographic Wetness Index (One Step)] Execution succeeded (29s) 2024-12-24/23:17:45] [LS Factor (One Step)] Execution started... __________ [LS Factor (One Step)] Parameters: DEM: Filtered Grid LS Factor: LS Factor Feet Conversion: false Method: Moore et al. 1991 Preprocessing: none Minimum Slope: 0.0001 __________ [Slope, Aspect, Curvature] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Elevation: Filtered Grid Slope: Slope Aspect: Aspect General Curvature: &lt;not set&gt; Profile Curvature: &lt;not set&gt; Plan Curvature: &lt;not set&gt; Tangential Curvature: &lt;not set&gt; Longitudinal Curvature: &lt;not set&gt; Cross-Sectional Curvature: &lt;not set&gt; Minimal Curvature: &lt;not set&gt; Maximal Curvature: &lt;not set&gt; Total Curvature: &lt;not set&gt; Flow Line Curvature: &lt;not set&gt; Method: 9 parameter 2nd order polynom (Zevenbergen &amp; Thorne 1987) Unit: radians Unit: radians [Slope, Aspect, Curvature] execution time: less than 1 millisecond __________ [Flow Accumulation (Top-Down)] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Elevation: Filtered Grid Sink Routes: &lt;not set&gt; Weights: &lt;not set&gt; Flow Accumulation: Flow Accumulation Input for Mean over Catchment: &lt;not set&gt; Material for Accumulation: &lt;not set&gt; Step: 1 Flow Accumulation Unit: cell area Flow Path Length: &lt;not set&gt; Channel Direction: &lt;not set&gt; Method: Multiple Flow Direction Thresholded Linear Flow: false Convergence: 1.1 Contour Length: false [Flow Accumulation (Top-Down)] execution time: 12s __________ [Flow Width and Specific Catchment Area] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Elevation: Filtered Grid Flow Width: Flow Width Total Catchment Area (TCA): Flow Accumulation Specific Catchment Area (SCA): Specific Catchment Area (SCA) Coordinate Unit: meter Method: Aspect [Flow Width and Specific Catchment Area] execution time: 01s __________ [LS Factor] Parameters: Grid System: 24.998458; 3170x 2329y; 262890.038674x 4067491.089629y Slope: Slope Catchment Area: Specific Catchment Area (SCA) LS Factor: LS Factor Area to Length Conversion: no conversion (areas already given as specific catchment area) Feet Adjustment: false Method (LS): Moore et al. 1991 Rill/Interrill Erosivity: 1 Stability: stable [LS Factor] execution time: less than 1 millisecond __________ total execution time: 13000 milliseconds (13s) [2024-12-24/23:17:59] [LS Factor (One Step)] Execution succeeded (13s) 7.1.2.2 Remote sensing images and indexes The Landsat 8 images were collected via a Google Earth Engine script on a period covering 2020, the median of the composite image from Tier 1 TOA collection was used. The Sentinel 2 image were collected via a Google Earth Engine script on a period covering 2021, the median of the composite image from MultiSpectral Instrument Level-2A collection was used.The land surface temperature (LST) and other MODIS component were computed also on Google Earth with a time covering from 2020 to 2021. the median of the MODIS Terra collection was used. The javascript codes for scraping these images are available in the supplementary file inside the “7 - DSM/code” folder. We computed the following indexes: Normalized Difference Vegetation Index (NDVI) (McFeeters 1996); Normalized Difference Water Index (NDWI); Enhanced Vegetation Index (EVI); Soil Adjusted Vegetation Index (SAVI); Normilized Difference Moisture Index (NDMI); COmbined Specteral Response Index (COSRI); Transformed Vegetation Index (TVI);Land Surface Water Index (LSWI) \\[ NDVI = (NIR - Red) / (NIR + Red) \\] \\[ NDWI = (Green - NIR) / (Green + NIR) \\] \\[ EVI = 2.5\\frac{NIR - Red} {NIR+(6*Red)-(7.5*Blue+1)} \\] \\[ Landsat~8~and~MODIS~SAVI = 1.5\\frac{NIR - Red}{NIR +Red+0.5} \\] \\[ Sentinel~2~SAVI = \\frac{(NIR - Red)*0.5}{NIR +Red+0.5} \\] \\[ NDMI = \\frac{NIR - SWIR1}{NIR+SWIR1} \\] \\[ CORSI = \\frac{Blue - Green}{Red+NIR}*NDVI \\] \\[ TVI = (\\frac{NIR - Red}{NIR+Red}+0.5)^{0.5}*100 \\] \\[ LSWI = \\frac{NIR - SWIR}{NIR+SWIR} \\] \\[ Landsat~8~and~MODIS~Brightness~index = NIR^2 - Red^2 \\] \\[ Sentinel~2~Brightness~index = \\frac{(NIR^2 - Green^2)^{0.5}}{2} \\] \\[ Landsat~8~Clay~index = \\frac{SWIR1}{SWIR2} \\] \\[ Sentinel~2~Clay~index = \\frac{RedEdge1}{RedEdge3} \\] \\[ Landsat~8~Salinity~index = \\frac{Red-NIR}{Green+NIR} \\] \\[ Sentinel~2~Salinity~index = \\frac{Red-NIR}{Blue+NIR} \\] \\[ Landsat~8~Carbonate~index = \\frac{Red}{Green} \\] \\[ Sentinel~2~Carbonate~index = \\frac{Red}{Blue} \\] \\[ Landsat~8~Gypsum~index = \\frac{SWIR1-NIR}{SWIR1+NIR} \\] \\[ Sentinel~2~Gypsum~index = \\frac{SWIR2-RedEdge1}{SWIR2+RedEdge1} \\] 7.1.3 Soil properties The soil 10 varaibles measurments for the five soil depth increment came from the predictions of the previous chapter and can be found online at https://doi.org/10.1594/PANGAEA.973700. 7.1.4 Preparation of the data All raster were sampled to 25 x 25 m tiles to match the DEM. We used bilinear method excepted for the discrete maps (geology and geomorphology) where we used ngb resampling. # 1.1 Import data ============================================================== DEM &lt;- raster(&quot;./data/DEM_GLO_25.tif&quot;) rlist&lt;-list.files(&quot;./data/remote&quot;, full.names=T) rlist2&lt;-list.files(&quot;./data/remote/&quot;, full.names=F) outpath&lt;-&quot;./data/remote/resample/&quot; outfiles &lt;- paste0(outpath, rlist2) # 1.1 Resample loop ============================================================ for (i in (1:length(rlist))){ x&lt;-raster(rlist[i]) x &lt;- projectRaster(x, crs=CRS(&quot;+init=epsg:32638&quot;)) x&lt;-resample(x,DEM,method = &quot;bilinear&quot;) #Careful to implement &quot;ngb&quot; option for discrete values x&lt;-crop(x,DEM) writeRaster(x,filename=outfiles[i], format=&quot;GTiff&quot;,overwrite=T) } # 1.2 Export as a stack_raster ================================================= rlist &lt;- list.files(&quot;./data/remote/resample/&quot;, full.names=T) x &lt;- stack(rlist, DEM) x &lt;- rast(x) terra::writeRaster(x, &quot;./export/Stack_layers_depth.tif&quot;) 7.2 Soil depth mapping preparation 7.2.1 Preparation of the environment # 0.1 Prepare environment ====================================================== # Folder check getwd() # Set folder direction setwd() # Clean up workspace rm(list = ls(all.names = TRUE)) # 0.2 Install packages ========================================================= install.packages(&quot;pacman&quot;) #Install and load the &quot;pacman&quot; package (allow easier download of packages) library(pacman) pacman::p_load(dplyr, tidyr,ggplot2, mapview, sf, sp, terra, raster, corrplot, viridis, Boruta, caret, quantregForest, readr, rpart, Cubist, reshape2, usdm) # 0.3 Show session infos ======================================================= sessionInfo() ## R version 4.4.0 (2024-04-24 ucrt) ## Platform: x86_64-w64-mingw32/x64 ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=French_France.utf8 LC_CTYPE=French_France.utf8 ## [3] LC_MONETARY=French_France.utf8 LC_NUMERIC=C ## [5] LC_TIME=French_France.utf8 ## ## time zone: Europe/Berlin ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] usdm_2.1-7 reshape2_1.4.4 Cubist_0.4.4 ## [4] rpart_4.1.23 quantregForest_1.3-7.1 RColorBrewer_1.1-3 ## [7] randomForest_4.7-1.2 caret_6.0-94 lattice_0.22-6 ## [10] Boruta_8.0.0 viridis_0.6.5 viridisLite_0.4.2 ## [13] corrplot_0.95 terra_1.7-83 tidyr_1.3.1 ## [16] dplyr_1.1.4 pacman_0.5.1 sf_1.0-18 ## [19] mapview_2.11.2 stringr_1.5.1 DT_0.33 ## [22] readr_2.1.5 raster_3.6-30 sp_2.1-4 ## [25] ggplot2_3.5.1 bookdown_0.41 tufte_0.13 ## [28] rmarkdown_2.29 knitr_1.49 ## ## loaded via a namespace (and not attached): ## [1] mathjaxr_1.6-0 rstudioapi_0.17.1 jsonlite_1.8.9 ## [4] magrittr_2.0.3 farver_2.1.2 vctrs_0.6.5 ## [7] base64enc_0.1-3 htmltools_0.5.8.1 curl_5.2.3 ## [10] pROC_1.18.5 sass_0.4.9 parallelly_1.38.0 ## [13] KernSmooth_2.23-22 bslib_0.8.0 htmlwidgets_1.6.4 ## [16] plyr_1.8.9 lubridate_1.9.3 cachem_1.1.0 ## [19] uuid_1.2-1 lifecycle_1.0.4 iterators_1.0.14 ## [22] pkgconfig_2.0.3 Matrix_1.7-0 R6_2.5.1 ## [25] fastmap_1.2.0 future_1.34.0 digest_0.6.37 ## [28] colorspace_2.1-1 leafem_0.2.3 crosstalk_1.2.1 ## [31] labeling_0.4.3 fansi_1.0.6 timechange_0.3.0 ## [34] compiler_4.4.0 proxy_0.4-27 bit64_4.5.2 ## [37] withr_3.0.2 brew_1.0-10 DBI_1.2.3 ## [40] MASS_7.3-60.2 lava_1.8.0 leaflet_2.2.2 ## [43] classInt_0.4-10 ModelMetrics_1.2.2.2 tools_4.4.0 ## [46] units_0.8-5 future.apply_1.11.3 nnet_7.3-19 ## [49] glue_1.7.0 satellite_1.0.5 nlme_3.1-164 ## [52] grid_4.4.0 cluster_2.1.6 generics_0.1.3 ## [55] recipes_1.1.0 gtable_0.3.6 leaflet.providers_2.0.0 ## [58] tzdb_0.4.0 class_7.3-22 data.table_1.16.2 ## [61] hms_1.1.3 utf8_1.2.4 foreach_1.5.2 ## [64] pillar_1.9.0 vroom_1.6.5 splines_4.4.0 ## [67] survival_3.5-8 bit_4.5.0 tidyselect_1.2.1 ## [70] gridExtra_2.3 svglite_2.1.3 stats4_4.4.0 ## [73] xfun_0.48 hardhat_1.4.0 leafpop_0.1.0 ## [76] timeDate_4041.110 stringi_1.8.4 yaml_2.3.10 ## [79] evaluate_1.0.1 codetools_0.2-20 tcltk_4.4.0 ## [82] tibble_3.2.1 cli_3.6.3 systemfonts_1.1.0 ## [85] munsell_0.5.1 jquerylib_0.1.4 Rcpp_1.0.13 ## [88] globals_0.16.3 png_0.1-8 parallel_4.4.0 ## [91] gower_1.0.1 listenv_0.9.1 ipred_0.9-15 ## [94] scales_1.3.0 prodlim_2024.06.25 e1071_1.7-16 ## [97] purrr_1.0.2 crayon_1.5.3 rlang_1.1.4 7.2.2 Prepare the data # 01 Import data sets ########################################################## # 01.1 Import soils infos ====================================================== # From the data accessible at the https://doi.org/10.1594/PANGAEA.973700 soil_infos &lt;- read.csv(&quot;./data/MIR_spectra_prediction.csv&quot;, sep=&quot;;&quot;) soil_infos$Depth..bot &lt;- as.factor(soil_infos$Depth..bot) soil_infos &lt;- soil_infos[,-c(5,7,8)] soil_list &lt;- split(soil_infos, soil_infos$Depth..bot) depths &lt;- c(&quot;0_10&quot;, &quot;10_30&quot;, &quot;30_50&quot;, &quot;50_70&quot;, &quot;70_100&quot;) names(soil_list) &lt;- depths soil_infos &lt;- soil_list for (i in 1:length(soil_infos)) { soil_infos[[i]] &lt;- soil_infos[[i]][,-c(2,5)] colnames(soil_infos[[i]]) &lt;- c(&quot;Site_name&quot;,&quot;Latitude&quot;,&quot;Longitude&quot;,&quot;pH&quot;,&quot;CaCO3&quot;,&quot;Nt&quot;,&quot;Ct&quot;,&quot;Corg&quot;,&quot;EC&quot;,&quot;Sand&quot;,&quot;Silt&quot;,&quot;Clay&quot;,&quot;MWD&quot;) write.csv(soil_infos[[i]], paste0(&quot;./data/Infos_&quot;,names(soil_infos[i]),&quot;_soil.csv&quot;)) } # 01.2 Plot the soil information =============================================== # Short overview of the values head(soil_infos[[1]]) ## Site_name pH CaCO3 Nt Ct Corg EC Sand ## 1 B07_2022_001 7.136580 28.60175 0.07898301 4.024251 0.69 288.5740 6.929541 ## 6 B07_2022_002 7.181695 41.91543 0.19087473 7.194387 2.36 268.3455 16.825245 ## 11 B07_2022_003 7.351164 56.95427 0.16207257 8.575062 1.94 250.7080 20.833241 ## 16 B07_2022_004 7.115174 20.34019 0.12628409 3.362697 1.00 256.7996 7.668607 ## 21 B07_2022_006 7.156837 32.51514 0.07670799 4.459705 0.71 243.6403 3.328909 ## 26 B07_2022_007 7.083956 22.32292 0.25814694 5.081921 2.69 277.2499 42.091774 ## Silt Clay MWD ## 1 47.12790 45.31941 0.05649625 ## 6 51.92765 31.77073 0.09199126 ## 11 57.17239 22.23783 0.11557732 ## 16 40.30110 42.13086 0.03590966 ## 21 51.47959 39.81202 0.02227241 ## 26 39.60886 18.71990 0.17134684 head(soil_infos[[2]]) ## Site_name pH CaCO3 Nt Ct Corg EC Sand ## 2 B07_2022_001 7.153711 28.26386 0.05322302 4.012612 0.33 261.7479 2.866008 ## 7 B07_2022_002 7.236350 55.42848 0.07334440 7.466923 0.91 228.7914 12.849202 ## 12 B07_2022_003 7.390680 60.11458 0.11389843 8.146147 1.32 228.6317 21.337265 ## 17 B07_2022_004 7.126203 22.25178 0.05708643 3.398175 0.26 260.1007 3.226446 ## 22 B07_2022_006 7.169161 31.19109 0.04682456 4.423646 0.27 242.1606 1.495163 ## 27 B07_2022_007 7.209257 22.87736 0.13164444 4.188129 1.07 218.7847 33.877903 ## Silt Clay MWD ## 2 50.30470 44.23866 0.05390326 ## 7 57.23070 30.09416 0.05518336 ## 12 56.72284 23.69718 0.11119357 ## 17 36.83939 44.31406 0.03544280 ## 22 51.61467 42.26332 0.01943206 ## 27 41.46440 19.15729 0.15482803 head(soil_infos[[3]]) ## Site_name pH CaCO3 Nt Ct Corg EC Sand ## 3 B07_2022_001 7.090760 28.76550 0.04760096 4.052045 0.39 269.2073 4.553220 ## 8 B07_2022_002 7.293602 68.91563 0.05991993 7.722308 0.63 198.3698 21.184769 ## 13 B07_2022_003 7.443392 64.38919 0.07333452 8.092261 0.94 213.8872 20.312691 ## 18 B07_2022_004 7.081050 23.54215 0.05051329 3.212894 0.31 229.8570 4.879420 ## 23 B07_2022_006 7.212613 32.50340 0.04132612 4.418419 0.31 220.0376 4.653284 ## 28 B07_2022_007 7.194686 23.56323 0.10865030 4.270283 0.88 236.8771 35.625011 ## Silt Clay MWD ## 3 49.45483 48.43240 0.03200903 ## 8 56.64009 24.95408 0.11507840 ## 13 58.92308 24.24682 0.11101878 ## 18 39.49672 42.72040 0.02931181 ## 23 54.98576 42.94632 0.03238971 ## 28 44.03695 18.65538 0.15935865 head(soil_infos[[4]]) ## Site_name pH CaCO3 Nt Ct Corg EC Sand ## 4 B07_2022_001 7.030727 26.95205 0.05334169 3.922293 0.38 273.3466 5.421180 ## 9 B07_2022_002 7.410029 75.14541 0.06501439 8.443180 0.81 202.2455 19.361340 ## 14 B07_2022_003 7.338138 57.90329 0.05897546 7.472987 0.59 199.8969 25.564911 ## 19 B07_2022_004 7.157351 21.52464 0.04772137 3.252805 0.27 246.5842 3.950831 ## 24 B07_2022_006 7.272707 34.48200 0.04269594 4.789804 0.18 216.1190 9.785641 ## 29 B07_2022_007 7.209351 23.53400 0.13007006 4.336351 1.07 216.3550 33.592701 ## Silt Clay MWD ## 4 46.96042 49.90807 0.03194526 ## 9 58.17092 20.39612 0.12236132 ## 14 54.95970 24.46929 0.12375436 ## 19 36.93913 48.87111 0.01785739 ## 24 55.20991 37.14780 0.02840232 ## 29 43.05497 18.72247 0.14998715 head(soil_infos[[5]]) ## Site_name pH CaCO3 Nt Ct Corg EC Sand ## 5 B07_2022_001 7.175221 30.26523 0.05208322 4.121973 0.44 268.2132 6.013255 ## 10 B07_2022_002 7.350596 68.79961 0.05562541 8.222191 0.68 194.6627 21.810980 ## 15 B07_2022_003 7.346992 57.47734 0.04710348 7.719401 0.48 204.7379 22.397934 ## 20 B07_2022_004 7.190866 23.55989 0.04618357 3.455671 0.22 234.1318 6.584578 ## 25 B07_2022_006 7.157984 21.64242 0.04920712 3.573874 0.28 229.3348 5.615658 ## 30 B07_2022_007 7.178517 23.70816 0.12945329 4.285172 1.17 222.8795 37.773411 ## Silt Clay MWD ## 5 49.27372 48.28483 0.04350151 ## 10 58.15880 19.81733 0.14863542 ## 15 57.70358 24.45869 0.10108105 ## 20 38.69909 45.82492 0.03141789 ## 25 43.26391 40.29404 0.03506210 ## 30 40.40684 18.16751 0.17889768 # Histogramm ploting with normal and sqrt values for (i in 1:length(soil_infos)) { windows(width = 12, height = 9) par(mfrow = c(3, 4)) for (j in 4:length(soil_infos[[1]])) { hist(sqrt(soil_infos[[i]][, j]), main = paste0(&quot;Distribution of &quot;, names(soil_infos[[i]][j]), &quot; for &quot;, names(soil_infos[i]) ,&quot; soil&quot;), xlab = paste0(&quot;Transformed Square Root of &quot;, names(soil_infos[[i]][j])), col = &quot;skyblue&quot;, border = &quot;white&quot;) } savePlot(paste0(&quot;./export/preprocess/Histogram_sqrt_&quot;, names(soil_infos[i]), &quot;_soil.png&quot;), type = &quot;png&quot;) par(mfrow = c(1, 1)) dev.off() windows(width = 12, height = 9) par(mfrow = c(3, 4)) for (j in 4:length(soil_infos[[1]])) { hist(soil_infos[[i]][, j], main = paste0(&quot;Distribution of &quot;, names(soil_infos[[i]][j]), &quot; for &quot;, names(soil_infos[i]) ,&quot; soil&quot;), xlab = paste0(&quot;Distribution of &quot;, names(soil_infos[[i]][j])), col = &quot;skyblue&quot;, border = &quot;white&quot;) } savePlot(paste0(&quot;./export/preprocess/Histogram_&quot;, names(soil_infos[i]), &quot;_soil.png&quot;), type = &quot;png&quot;) par(mfrow = c(1, 1)) dev.off() } dev.off() Histogram for 30 - 50 cm Histogram for 50 - 70 cm Histogram for 70 - 100 cm Histogram SQRTfor 0 - 10 cm Histogram SQRT for 10 - 30 cm Histogram SQRT for 30 - 50 cm Histogram SQRT for 50 - 70 cm Histogram SQRT for 70 - 100 cm # 01.3 Set coordinates ========================================================= # Create a spatial dataframe and convert to WGS84 UTM 38 N coordinates soil_infos_sp &lt;- soil_infos for (i in 1:length(soil_infos)) { soil_infos_sp[[i]] &lt;- st_as_sf(soil_infos_sp[[i]], coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), crs = 4326) soil_infos_sp[[i]] &lt;-st_transform(soil_infos_sp[[i]], crs = 32638) } mapview(soil_infos_sp[[1]]) + mapview(soil_infos_sp[[2]], col.regions = &quot;red&quot;) + mapview(soil_infos_sp[[3]], col.regions = &quot;green&quot;) + mapview(soil_infos_sp[[4]], col.regions = &quot;pink&quot;) + mapview(soil_infos_sp[[5]], col.regions = &quot;darkgrey&quot;) 7.2.3 Prepare the covariates # 01.4 Import covariates raster ================================================ Landsat &lt;- raster::stack(list.files(&quot;./data/Landsat/&quot;, full.names = TRUE)) names(Landsat) Sentinel &lt;- raster::stack(list.files(&quot;./data/Sentinel/&quot;, full.names = TRUE)) names(Sentinel) Terrain &lt;- raster::stack(list.files(&quot;./data/Terrain/&quot;, full.names = TRUE)) names(Terrain) Others &lt;- raster::stack(list.files(&quot;./data/Others/&quot;, full.names = TRUE)) names(Others) Modis &lt;- raster::stack(list.files(&quot;./data/MODIS/&quot;, full.names = TRUE)) names(Modis) # RS Landsat 8 Landsat$EVI &lt;- 2.5 * ((Landsat$Landsat8_NIR_2020_Median - Landsat$Landsat8_red_2020_Median)/(Landsat$Landsat8_NIR_2020_Median + (6*Landsat$Landsat8_red_2020_Median) - (7.5*Landsat$Landsat8_blue_2020_Median + 1))) Landsat$SAVI &lt;- ((Landsat$Landsat8_NIR_2020_Median - Landsat$Landsat8_red_2020_Median)/(Landsat$Landsat8_NIR_2020_Median + Landsat$Landsat8_red_2020_Median + 0.5)) * (1.5) #Enhanced Vegetation Index Landsat$NDMI &lt;- (Landsat$Landsat8_NIR_2020_Median - Landsat$Landsat8_SIR1_2020_Median)/(Landsat$Landsat8_NIR_2020_Median + Landsat$Landsat8_SIR1_2020_Median) # normilized difference moisture index Landsat$COSRI &lt;- ((Landsat$Landsat8_blue_2020_Median - Landsat$Landsat8_green_2020_Median)/(Landsat$Landsat8_red_2020_Median + Landsat$Landsat8_NIR_2020_Median)) * (Landsat$Landsat8_NDVI_2020_Median) # Combined Specteral Response Index Landsat$BrightnessIndex &lt;- ((Landsat$Landsat8_NIR_2020_Median)^2 - (Landsat$Landsat8_red_2020_Median)^2) Landsat$ClayIndex &lt;- (Landsat$Landsat8_SIR1_2020_Median / Landsat$Landsat8_SIR2_2020_Median) Landsat$SalinityIndex &lt;- (Landsat$Landsat8_red_2020_Median - Landsat$Landsat8_NIR_2020_Median)/(Landsat$Landsat8_green_2020_Median + Landsat$Landsat8_NIR_2020_Median) Landsat$CarbonateIndex &lt;- (Landsat$Landsat8_red_2020_Median / Landsat$Landsat8_green_2020_Median) Landsat$GypsumIndex &lt;- (Landsat$Landsat8_SIR1_2020_Median - Landsat$Landsat8_NIR_2020_Median)/(Landsat$Landsat8_SIR1_2020_Median + Landsat$Landsat8_NIR_2020_Median) # RS Sentinel 2 Sentinel$EVI &lt;- ((Sentinel$Sentinel2_NIR_2021_MedianComposite - Sentinel$Sentinel2_red_2021_MedianComposite)/((Sentinel$Sentinel2_NIR_2021_MedianComposite + 6 * Sentinel$Sentinel2_red_2021_MedianComposite) - (7.5 * Sentinel$Sentinel2_blue_2021_MedianComposite + 1))) * 2.5 Sentinel$TVI &lt;- ((((Sentinel$Sentinel2_NIR_2021_MedianComposite - Sentinel$Sentinel2_red_2021_MedianComposite)/(Sentinel$Sentinel2_NIR_2021_MedianComposite + Sentinel$Sentinel2_red_2021_MedianComposite)) + 0.5) ^ 0.5) *100 Sentinel$SAVI &lt;- ((Sentinel$Sentinel2_NIR_2021_MedianComposite - Sentinel$Sentinel2_red_2021_MedianComposite) * 0.5) /(Sentinel$Sentinel2_NIR_2021_MedianComposite + Sentinel$Sentinel2_red_2021_MedianComposite + 0.5) Sentinel$LSWI &lt;- (Sentinel$Sentinel2_NIR_2021_MedianComposite - (Sentinel$Sentinel2_SWIR1_2021_MedianComposite+Sentinel$Sentinel2_SWIR2_2021_MedianComposite))/(Sentinel$Sentinel2_NIR_2021_MedianComposite - (Sentinel$Sentinel2_SWIR1_2021_MedianComposite+Sentinel$Sentinel2_SWIR2_2021_MedianComposite)) Sentinel$BrightnessIndex &lt;- (((Sentinel$Sentinel2_NIR_2021_MedianComposite * Sentinel$Sentinel2_red_2021_MedianComposite) + (Sentinel$Sentinel2_green_2021_MedianComposite * Sentinel$Sentinel2_green_2021_MedianComposite))^0.5) / 2 Sentinel$ClayIndex &lt;- (Sentinel$Sentinel2_redEdge1_2021_MedianComposite / Sentinel$Sentinel2_redEdge3_2021_MedianComposite) Sentinel$SalinityIndex &lt;- (Sentinel$Sentinel2_red_2021_MedianComposite - Sentinel$Sentinel2_NIR_2021_MedianComposite)/(Sentinel$Sentinel2_blue_2021_MedianComposite + Sentinel$Sentinel2_NIR_2021_MedianComposite) Sentinel$CarbonateIndex &lt;- (Sentinel$Sentinel2_red_2021_MedianComposite / Sentinel$Sentinel2_blue_2021_MedianComposite) Sentinel$GypsumIndex &lt;- (Sentinel$Sentinel2_SWIR2_2021_MedianComposite - Sentinel$Sentinel2_redEdge1_2021_MedianComposite)/(Sentinel$Sentinel2_SWIR2_2021_MedianComposite + Sentinel$Sentinel2_redEdge1_2021_MedianComposite) # RS MODIS Modis$SAVI &lt;- ((Modis$MODIS_NIR_band - Modis$MODIS_Red_band)/(Modis$MODIS_NIR_band + Modis$MODIS_Red_band + 0.5)) * (1.5) Modis$BrightnessIndex &lt;- ((Modis$MODIS_Red_band)^2 - (Modis$MODIS_NIR_band)^2) df_names &lt;- data.frame() for (i in 1:length(names(Terrain))) { c &lt;- paste0(&quot;TE.&quot;,i) df_names[i,1] &lt;- c df_names[i,2] &lt;- names(Terrain)[i] } t &lt;- nrow(df_names) for (i in 1:length(names(Landsat))) { c &lt;- paste0(&quot;LA.&quot;,i) df_names[i+t,1] &lt;- c df_names[i+t,2] &lt;- names(Landsat)[i] } t &lt;- nrow(df_names) for (i in 1:length(names(Sentinel))) { c &lt;- paste0(&quot;SE.&quot;,i) df_names[i+t,1] &lt;- c df_names[i+t,2] &lt;- names(Sentinel)[i] } t &lt;- nrow(df_names) for (i in 1:length(names(Modis))) { c &lt;- paste0(&quot;MO.&quot;,i) df_names[i+t,1] &lt;- c df_names[i+t,2] &lt;- names(Modis)[i] } t &lt;- nrow(df_names) for (i in 1:length(names(Others))) { c &lt;- paste0(&quot;OT.&quot;,i) df_names[i+t,1] &lt;- c df_names[i+t,2] &lt;- names(Others)[i] } write.table(df_names,&quot;./data/Covariates_names_DSM.txt&quot;) x &lt;- raster::stack(Terrain, Landsat, Sentinel, Modis, Others) names(x) &lt;- df_names[,1] x &lt;- rast(x) terra::writeRaster(x, &quot;./data/Stack_layers_DSM.tif&quot;, overwrite = TRUE) covariates &lt;- stack(&quot;./data/Stack_layers_DSM.tif&quot;) # 01.5 Plot the covariates maps ================================================ reduce &lt;- aggregate(covariates, fact=10, fun=mean) plot(reduce) # 01.6 Extract the values ====================================================== # Extract the values of each band for the sampling location df_cov &lt;- soil_infos_sp for (i in 1:length(df_cov)) { df_cov[[i]] &lt;- raster::extract(covariates, df_cov[[i]], method=&#39;simple&#39;) df_cov[[i]] &lt;- as.data.frame(df_cov[[i]]) write.csv(df_cov[[i]], paste0(&quot;./data/df_&quot;,names(df_cov[i]),&quot;_cov_DSM.csv&quot;)) } # 01.7 Export and save data ==================================================== save(df_cov, soil_infos_sp, file = &quot;./export/save/Pre_process.RData&quot;) rm(list = ls()) # 02 Check the data ############################################################ # 02.1 Import the data and merge =============================================== load(file = &quot;./export/save/Pre_process.RData&quot;) SoilCov &lt;- df_cov for (i in 1:length(SoilCov)) { ID &lt;- 1:nrow(SoilCov[[i]]) SoilCov[[i]] &lt;- cbind(df_cov[[i]], ID, st_drop_geometry(soil_infos_sp[[i]])) cat(&quot;There is &quot;, sum(is.na(SoilCov[[i]])== TRUE), &quot;Na values in &quot;, names(SoilCov[i]),&quot; soil list&quot;) } # 02.2 Plot and export the correlation matrix ================================== for (i in 1:length(df_cov)) { pdf(paste0(&quot;./export/preprocess/Correlation_&quot;,names(df_cov[i]), &quot;.pdf&quot;), # File name width = 40, height = 40, # Width and height in inches bg = &quot;white&quot;, # Background color colormodel = &quot;cmyk&quot;) # Color model # Correlation of the data corrplot(cor(df_cov[[i]]), method = &quot;color&quot;, col = viridis(200), type = &quot;upper&quot;, addCoef.col = &quot;black&quot;, # Add coefficient of correlation tl.col = &quot;black&quot;, tl.srt = 45, # Text label color and rotation number.cex = 0.7, # Size of the text labels cl.cex = 0.7, # Size of the color legend text cl.lim = c(-1, 1)) # Color legend limits dev.off() } Regarding the correlation plot the Landsat and Sentinel bands are the ones with the higher correlation followed by terrain derivatives from DEM. # 02.3 Select with VIF correlation ============================================= vif &lt;- df_cov vif_plot &lt;- df_cov for (i in 1:length(df_cov)) { vif[[i]] &lt;-vifcor(df_cov[[i]], th=0.8) vif_df &lt;- as.data.frame(vif[[i]]@results) write.table(vif_df, paste0(&quot;./export/VIF/vif_results_&quot;,names(df_cov[i]) ,&quot;_soil.txt&quot;)) vif_plot[[i]] &lt;- ggplot(vif_df, aes(x = reorder(Variables, VIF), y = VIF)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;lightblue&quot;) + coord_flip() + theme_minimal() + labs(title = paste0(&quot;VIF Values for &quot;, names(df_cov[i]) ,&quot; soil&quot;), x = &quot;Variables&quot;, y = &quot;VIF&quot;) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) ggsave(paste0(&quot;./export/VIF/VIF_&quot;, names(df_cov[i]),&quot;_soil.png&quot;), vif_plot[[i]], width = 12, height = 8) ggsave(paste0(&quot;./export/VIF/VIF_&quot;, names(df_cov[i]),&quot;_soil.pdf&quot;), vif_plot[[i]], width = 12, height = 8) } # 02.4 Statistics ============================================================== # Here we decided to split every run by soil depth to have a better vision # on the running process. #=============================================================================== x &lt;- &quot;0_10&quot; depth &lt;- names(SoilCov[x]) # Basic statistics names(SoilCov[[x]]) ## [1] &quot;TE.1&quot; &quot;TE.2&quot; &quot;TE.3&quot; &quot;TE.4&quot; &quot;TE.5&quot; &quot;TE.6&quot; &quot;TE.7&quot; &quot;TE.8&quot; &quot;TE.9&quot; ## [10] &quot;TE.10&quot; &quot;TE.11&quot; &quot;TE.12&quot; &quot;TE.13&quot; &quot;TE.14&quot; &quot;TE.15&quot; &quot;TE.16&quot; &quot;TE.17&quot; &quot;TE.18&quot; ## [19] &quot;TE.19&quot; &quot;TE.20&quot; &quot;TE.21&quot; &quot;TE.22&quot; &quot;TE.23&quot; &quot;TE.24&quot; &quot;LA.1&quot; &quot;LA.2&quot; &quot;LA.3&quot; ## [28] &quot;LA.4&quot; &quot;LA.5&quot; &quot;LA.6&quot; &quot;LA.7&quot; &quot;LA.8&quot; &quot;LA.9&quot; &quot;LA.10&quot; &quot;LA.11&quot; &quot;LA.12&quot; ## [37] &quot;LA.13&quot; &quot;LA.14&quot; &quot;LA.15&quot; &quot;LA.16&quot; &quot;LA.17&quot; &quot;LA.18&quot; &quot;SE.1&quot; &quot;SE.2&quot; &quot;SE.3&quot; ## [46] &quot;SE.4&quot; &quot;SE.5&quot; &quot;SE.6&quot; &quot;SE.7&quot; &quot;SE.8&quot; &quot;SE.9&quot; &quot;SE.10&quot; &quot;SE.11&quot; &quot;SE.12&quot; ## [55] &quot;SE.13&quot; &quot;SE.14&quot; &quot;SE.15&quot; &quot;SE.16&quot; &quot;SE.17&quot; &quot;SE.18&quot; &quot;SE.19&quot; &quot;SE.20&quot; &quot;SE.21&quot; ## [64] &quot;MO.1&quot; &quot;MO.2&quot; &quot;MO.3&quot; &quot;MO.4&quot; &quot;MO.5&quot; &quot;MO.6&quot; &quot;MO.7&quot; &quot;MO.8&quot; &quot;OT.1&quot; ## [73] &quot;OT.2&quot; &quot;OT.3&quot; &quot;OT.4&quot; &quot;OT.5&quot; &quot;OT.6&quot; &quot;OT.7&quot; &quot;OT.8&quot; &quot;OT.9&quot; &quot;pH&quot; ## [82] &quot;CaCO3&quot; &quot;Nt&quot; &quot;Ct&quot; &quot;Corg&quot; &quot;EC&quot; &quot;Sand&quot; &quot;Silt&quot; &quot;Clay&quot; &quot;MWD&quot; nrow(SoilCov[[x]]) ## [1] 122 # If necessary SoilCov[[x]] &lt;- na.omit(SoilCov[[x]]) head(SoilCov[[x]]) ## TE.1 TE.2 TE.3 TE.4 TE.5 TE.6 TE.7 ## 1 5.129029 481.9058 1.24618530 48.68576 483.1519 4275.461 -0.0036442482 ## 6 4.271050 607.5202 5.06909180 54.65018 612.5893 4434.438 0.0002648764 ## 11 3.534097 366.2365 0.00000000 22.06347 351.5441 5463.821 0.0000000000 ## 16 4.414689 381.2467 18.52066040 39.91128 399.7674 1973.995 -0.0032089406 ## 21 2.600626 375.2304 5.19729614 49.38589 380.4277 15852.666 -0.0069409925 ## 26 3.597533 446.2390 0.07165527 43.89017 446.3106 26560.883 -0.0065811337 ## TE.8 TE.9 TE.10 TE.11 TE.12 TE.13 ## 1 0.07334368 1.6368924 1.547569 0.09604046 -2.094395e-04 1.503411 ## 6 0.47363004 0.4449549 1.519993 0.07047901 1.636734e-05 1.504419 ## 11 0.75827569 2.2581604 1.570573 0.05645949 2.236110e-04 1.503633 ## 16 0.08321811 0.1176461 1.479559 0.72559083 1.592405e-03 1.484723 ## 21 0.01267918 0.3939767 1.535936 0.23193364 3.370031e-04 1.450923 ## 26 0.05783909 2.5950627 1.553927 0.14401424 -1.694468e-04 1.479041 ## TE.14 TE.15 TE.16 TE.17 TE.18 TE.19 TE.20 ## 1 -0.0009426068 3.042071 0.092795573 323.9177 98 1.6050999 4.304254 ## 6 0.0002291707 7.852433 0.068446726 328.5376 114 1.2863352 13.778360 ## 11 -0.0009171535 8.805406 0.001889392 310.3445 19 0.0298721 10.554357 ## 16 0.0011994429 12.084249 0.177230388 374.3112 81 2.8886068 32.724075 ## 21 -0.0002807949 7.214093 0.152611300 324.0304 114 2.8588665 29.393518 ## 26 -0.0005500755 3.480745 0.072091810 327.0627 82 1.4929968 24.623165 ## TE.21 TE.22 TE.23 TE.24 LA.1 LA.2 ## 1 -2.0777993 7.239330 4275.461 28.632814 1.464855e-05 1.464425e-05 ## 6 1.3326528 7.572299 4434.438 103.562767 1.740259e-05 2.043818e-05 ## 11 -0.8523821 11.391269 5463.821 147.154312 1.711387e-05 1.906098e-05 ## 16 1.3779540 5.866146 1973.995 4.570108 1.486618e-05 1.581989e-05 ## 21 -3.2010865 8.007936 15852.666 23.890034 1.736328e-05 1.881617e-05 ## 26 -1.7142496 9.305170 26560.883 20.688707 1.564758e-05 1.657248e-05 ## LA.3 LA.4 LA.5 LA.6 LA.7 LA.8 ## 1 0.1839304 -0.2564859 2.418240e-05 1.569579e-05 1.735969e-05 2.350239e-05 ## 6 0.1680534 -0.3067069 3.821417e-05 2.315058e-05 2.743678e-05 3.876102e-05 ## 11 0.2085298 -0.3084752 3.458530e-05 2.049527e-05 2.357212e-05 4.089066e-05 ## 16 0.1674306 -0.2806663 2.844225e-05 1.740484e-05 1.994137e-05 3.111950e-05 ## 21 0.1821253 -0.2782128 3.140979e-05 2.060692e-05 2.283800e-05 3.456463e-05 ## 26 0.2138029 -0.3095231 3.211739e-05 1.798635e-05 2.043029e-05 3.531461e-05 ## LA.9 LA.10 LA.11 LA.12 LA.13 ## 1 1.891810e-05 -1.705708e-05 2.046642e-05 0.014260544 1.903089e-05 ## 6 2.661204e-05 -2.694543e-05 3.232794e-05 -0.007104237 -7.770504e-03 ## 11 2.954793e-05 -2.753426e-05 3.303570e-05 -0.083541393 -6.981575e-03 ## 16 2.276484e-05 -2.125298e-05 2.550018e-05 -0.044949129 -3.300304e-03 ## 21 2.649527e-05 -2.143031e-05 2.571260e-05 -0.047819041 -4.877787e-03 ## 26 2.475316e-05 -2.921883e-05 3.505760e-05 -0.047414035 -3.763186e-03 ## LA.14 LA.15 LA.16 LA.17 LA.18 SE.1 SE.2 ## 1 2.834295e-10 1.242323 -0.1757223 1.185427 -0.014260544 0.1688569 0.1897706 ## 6 7.075462e-10 1.456522 -0.1837504 1.342428 0.007104237 0.1423388 0.1518370 ## 11 6.404981e-10 1.383875 -0.2052926 1.236669 0.083541393 0.1895031 0.2120295 ## 16 4.113034e-10 1.366998 -0.1920576 1.260525 0.044949129 0.1566231 0.1694655 ## 21 4.650010e-10 1.304558 -0.1706646 1.213743 0.047819041 0.1866443 0.2020181 ## 26 6.141297e-10 1.426671 -0.2400314 1.232784 0.047414035 0.1606215 0.1700812 ## SE.3 SE.4 SE.5 SE.6 SE.7 SE.8 SE.9 ## 1 0.07201554 -0.2112371 0.2999823 0.2492155 0.2579755 0.2815987 0.3071862 ## 6 0.08480766 -0.2181940 0.2363885 0.1997403 0.2090901 0.2237255 0.2430989 ## 11 0.08532100 -0.2010739 0.3194418 0.2689117 0.2834284 0.3009649 0.3256730 ## 16 0.08935766 -0.2092203 0.2581550 0.2231526 0.2316061 0.2464942 0.2664944 ## 21 0.06520155 -0.1610888 0.2775168 0.2477271 0.2538366 0.2676156 0.2862030 ## 26 0.07894805 -0.1826547 0.2478257 0.2105495 0.2192732 0.2359228 0.2554140 ## SE.10 SE.11 SE.12 SE.13 SE.14 SE.15 SE.16 ## 1 0.3169008 0.2362464 0.10915425 -0.2693763 76.97000 0.02419315 -0.02742584 ## 6 0.3132872 0.2580443 0.09622070 -0.1448064 76.42190 0.01957433 -0.13989830 ## 11 0.4320035 0.3459771 0.11684434 -0.2586719 76.54306 0.02321403 -0.14979354 ## 16 0.3395096 0.2836413 0.09473214 -0.1514987 75.67850 0.01783458 -0.13612077 ## 21 0.3654137 0.3206125 0.09738074 -0.1171066 74.61340 0.01452813 -0.13671292 ## 26 0.3315342 0.2899157 0.09205230 -0.1343697 76.24451 0.01944761 -0.14448448 ## SE.17 SE.18 SE.19 SE.20 SE.21 MO.1 MO.2 ## 1 0.8398017 0.1664130 -0.10828190 1.475898 -0.04396622 1387.392 30.68088 ## 6 0.8601031 0.1325432 -0.09676673 1.403274 0.10479676 1394.211 30.02961 ## 11 0.8702855 0.1808716 -0.09928410 1.419036 0.09937742 1456.483 30.35895 ## 16 0.8690846 0.1469069 -0.08438831 1.424774 0.10099064 1408.809 30.91485 ## 21 0.8869111 0.1654990 -0.06417975 1.327268 0.11624325 1299.647 31.73000 ## 26 0.8585010 0.1423966 -0.09126323 1.310843 0.13873525 1493.202 30.50342 ## MO.3 MO.4 MO.5 MO.6 MO.7 MO.8 OT.1 OT.2 OT.3 ## 1 12.83822 2182.765 2778.179 1758.568 0.3370803 -4625717 22.53508 13 4 ## 6 15.24922 1946.358 3543.490 2174.825 0.3589899 -7826458 178.91341 13 4 ## 11 13.19583 2081.331 3249.374 1976.490 0.3653260 -6651919 152.21561 5 5 ## 16 13.96882 2023.423 3260.174 1996.885 0.3604205 -6641180 387.91537 8 5 ## 21 13.86000 1800.269 3014.198 1973.598 0.3129126 -5190302 122.66399 6 5 ## 26 13.78470 2321.455 3053.413 1891.073 0.3525814 -5747173 824.36816 8 10 ## OT.4 OT.5 OT.6 OT.7 OT.8 OT.9 pH CaCO3 ## 1 5 2113.386 734.4635 211009.5 156.4436 25.31880 7.136580 28.60175 ## 6 11 2068.635 737.7375 210497.5 150.0130 24.60271 7.181695 41.91543 ## 11 11 2121.717 663.9340 211284.9 149.1383 25.51540 7.351164 56.95427 ## 16 11 2127.010 646.8065 210657.2 154.1835 25.58259 7.115174 20.34019 ## 21 11 2143.583 738.9758 211259.9 156.6656 25.58248 7.156837 32.51514 ## 26 11 2127.571 730.7488 211230.2 156.1779 25.53767 7.083956 22.32292 ## Nt Ct Corg EC Sand Silt Clay MWD ## 1 0.07898301 4.024251 0.69 288.5740 6.929541 47.12790 45.31941 0.05649625 ## 6 0.19087473 7.194387 2.36 268.3455 16.825245 51.92765 31.77073 0.09199126 ## 11 0.16207257 8.575062 1.94 250.7080 20.833241 57.17239 22.23783 0.11557732 ## 16 0.12628409 3.362697 1.00 256.7996 7.668607 40.30110 42.13086 0.03590966 ## 21 0.07670799 4.459705 0.71 243.6403 3.328909 51.47959 39.81202 0.02227241 ## 26 0.25814694 5.081921 2.69 277.2499 42.091774 39.60886 18.71990 0.17134684 sapply(SoilCov[[x]], class) ## TE.1 TE.2 TE.3 TE.4 TE.5 TE.6 TE.7 TE.8 ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## TE.9 TE.10 TE.11 TE.12 TE.13 TE.14 TE.15 TE.16 ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## TE.17 TE.18 TE.19 TE.20 TE.21 TE.22 TE.23 TE.24 ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## LA.1 LA.2 LA.3 LA.4 LA.5 LA.6 LA.7 LA.8 ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## LA.9 LA.10 LA.11 LA.12 LA.13 LA.14 LA.15 LA.16 ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## LA.17 LA.18 SE.1 SE.2 SE.3 SE.4 SE.5 SE.6 ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## SE.7 SE.8 SE.9 SE.10 SE.11 SE.12 SE.13 SE.14 ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## SE.15 SE.16 SE.17 SE.18 SE.19 SE.20 SE.21 MO.1 ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## MO.2 MO.3 MO.4 MO.5 MO.6 MO.7 MO.8 OT.1 ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## OT.2 OT.3 OT.4 OT.5 OT.6 OT.7 OT.8 OT.9 ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## pH CaCO3 Nt Ct Corg EC Sand Silt ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## Clay MWD ## &quot;numeric&quot; &quot;numeric&quot; summary(SoilCov[[x]]) ## TE.1 TE.2 TE.3 TE.4 ## Min. :0.1376 Min. :329.3 Min. :-0.02185 Min. : 0.00 ## 1st Qu.:2.6140 1st Qu.:381.9 1st Qu.: 2.29662 1st Qu.:47.29 ## Median :3.6089 Median :459.9 Median : 6.40428 Median :50.28 ## Mean :3.5803 Mean :479.8 Mean : 9.72547 Mean :46.47 ## 3rd Qu.:4.7091 3rd Qu.:555.2 3rd Qu.:14.62994 3rd Qu.:53.45 ## Max. :6.1539 Max. :863.3 Max. :62.55542 Max. :58.42 ## TE.5 TE.6 TE.7 TE.8 ## Min. :319.7 Min. : 625 Min. :-0.022208 Min. :0.00000 ## 1st Qu.:394.6 1st Qu.: 1221 1st Qu.:-0.002778 1st Qu.:0.04001 ## Median :475.0 Median : 2839 Median : 0.000000 Median :0.22147 ## Mean :487.7 Mean : 82620 Mean : 0.000377 Mean :0.69886 ## 3rd Qu.:566.1 3rd Qu.: 14080 3rd Qu.: 0.002316 3rd Qu.:0.98402 ## Max. :881.8 Max. :3359398 Max. : 0.046221 Max. :3.90044 ## TE.9 TE.10 TE.11 TE.12 ## Min. :0.0000 Min. :1.203 Min. :0.01283 Min. :-3.464e-03 ## 1st Qu.:0.1589 1st Qu.:1.493 1st Qu.:0.09528 1st Qu.:-3.187e-04 ## Median :0.6459 Median :1.523 Median :0.21327 Median : 0.000e+00 ## Mean :1.2274 Mean :1.512 Mean :0.29389 Mean : 3.064e-05 ## 3rd Qu.:1.8743 3rd Qu.:1.546 3rd Qu.:0.46994 3rd Qu.: 2.998e-04 ## Max. :6.4568 Max. :1.571 Max. :0.91567 Max. : 2.625e-03 ## TE.13 TE.14 TE.15 TE.16 ## Min. :1.357 Min. :-3.744e-03 Min. : 2.646 Min. :0.001889 ## 1st Qu.:1.495 1st Qu.:-2.915e-04 1st Qu.: 5.096 1st Qu.:0.037638 ## Median :1.515 Median : 1.789e-05 Median : 7.458 Median :0.066298 ## Mean :1.506 Mean : 3.034e-05 Mean : 8.990 Mean :0.091901 ## 3rd Qu.:1.534 3rd Qu.: 2.827e-04 3rd Qu.:10.249 3rd Qu.:0.132428 ## Max. :1.562 Max. : 3.042e-03 Max. :43.161 Max. :0.540178 ## TE.17 TE.18 TE.19 TE.20 ## Min. :308.4 Min. : 3.00 Min. :0.02987 Min. : 0.00 ## 1st Qu.:321.0 1st Qu.: 97.25 1st Qu.:0.81358 1st Qu.: 5.13 ## Median :340.1 Median :106.00 Median :1.31059 Median :13.59 ## Mean :363.8 Mean : 92.60 Mean :1.73184 Mean :15.71 ## 3rd Qu.:373.2 3rd Qu.:114.00 3rd Qu.:2.19768 3rd Qu.:22.46 ## Max. :715.9 Max. :115.00 Max. :9.92913 Max. :45.26 ## TE.21 TE.22 TE.23 TE.24 ## Min. :-6.1740 Min. : 3.804 Min. : 625 Min. : 3.238 ## 1st Qu.:-0.9127 1st Qu.: 6.118 1st Qu.: 1221 1st Qu.: 9.649 ## Median : 0.0000 Median : 7.069 Median : 2839 Median : 21.558 ## Mean : 0.2799 Mean : 8.000 Mean : 82620 Mean : 47.522 ## 3rd Qu.: 1.3432 3rd Qu.: 9.239 3rd Qu.: 14080 3rd Qu.: 75.462 ## Max. :20.4683 Max. :16.091 Max. :3359398 Max. :263.734 ## LA.1 LA.2 LA.3 LA.4 ## Min. :1.250e-05 Min. :1.194e-05 Min. :-0.08857 Min. :-0.4223 ## 1st Qu.:1.481e-05 1st Qu.:1.498e-05 1st Qu.: 0.17010 1st Qu.:-0.3243 ## Median :1.577e-05 Median :1.633e-05 Median : 0.19247 Median :-0.2971 ## Mean :1.578e-05 Mean :1.673e-05 Mean : 0.20434 Mean :-0.2967 ## 3rd Qu.:1.678e-05 3rd Qu.:1.838e-05 3rd Qu.: 0.22061 3rd Qu.:-0.2772 ## Max. :2.263e-05 Max. :2.415e-05 Max. : 0.38374 Max. : 0.1818 ## LA.5 LA.6 LA.7 ## Min. :1.019e-05 Min. :1.032e-05 Min. :1.086e-05 ## 1st Qu.:2.799e-05 1st Qu.:1.570e-05 1st Qu.:1.752e-05 ## Median :3.121e-05 Median :1.737e-05 Median :2.011e-05 ## Mean :3.114e-05 Mean :1.791e-05 Mean :2.056e-05 ## 3rd Qu.:3.455e-05 3rd Qu.:1.995e-05 3rd Qu.:2.290e-05 ## Max. :4.285e-05 Max. :2.981e-05 Max. :2.900e-05 ## LA.8 LA.9 LA.10 ## Min. :7.792e-06 Min. :5.847e-06 Min. :-5.250e-05 ## 1st Qu.:2.710e-05 1st Qu.:2.007e-05 1st Qu.:-3.115e-05 ## Median :3.082e-05 Median :2.222e-05 Median :-2.543e-05 ## Mean :3.057e-05 Mean :2.218e-05 Mean :-2.644e-05 ## 3rd Qu.:3.446e-05 3rd Qu.:2.468e-05 3rd Qu.:-2.158e-05 ## Max. :4.207e-05 Max. :2.978e-05 Max. : 1.696e-06 ## LA.11 LA.12 LA.13 ## Min. :-2.036e-06 Min. :-0.083541 Min. :-0.0101262 ## 1st Qu.: 2.589e-05 1st Qu.:-0.034397 1st Qu.:-0.0050879 ## Median : 3.051e-05 Median :-0.001571 Median :-0.0035247 ## Mean : 3.173e-05 Mean : 0.010560 Mean :-0.0028621 ## 3rd Qu.: 3.737e-05 3rd Qu.: 0.030765 3rd Qu.:-0.0004862 ## Max. : 6.299e-05 Max. : 0.233983 Max. : 0.0092064 ## LA.14 LA.15 LA.16 LA.17 ## Min. :-1.428e-11 Min. :1.151 Min. :-0.40334 Min. :0.8243 ## 1st Qu.: 4.148e-10 1st Qu.:1.296 1st Qu.:-0.24289 1st Qu.:1.1819 ## Median : 5.501e-10 Median :1.364 Median :-0.20662 Median :1.2362 ## Mean : 5.582e-10 Mean :1.385 Mean :-0.22022 Mean :1.2224 ## 3rd Qu.: 6.663e-10 3rd Qu.:1.481 3rd Qu.:-0.18390 3rd Qu.:1.2784 ## Max. : 1.210e-09 Max. :1.685 Max. : 0.02904 Max. :1.3565 ## LA.18 SE.1 SE.2 SE.3 ## Min. :-0.233983 Min. :0.1212 Min. :0.1221 Min. :-0.02075 ## 1st Qu.:-0.030765 1st Qu.:0.1430 1st Qu.:0.1497 1st Qu.: 0.07850 ## Median : 0.001571 Median :0.1538 Median :0.1639 Median : 0.09367 ## Mean :-0.010560 Mean :0.1540 Mean :0.1648 Mean : 0.10599 ## 3rd Qu.: 0.034397 3rd Qu.:0.1603 3rd Qu.:0.1729 3rd Qu.: 0.10938 ## Max. : 0.083541 Max. :0.2520 Max. :0.2625 Max. : 0.35203 ## SE.4 SE.5 SE.6 SE.7 ## Min. :-0.39196 Min. :0.1375 Min. :0.1228 Min. :0.1441 ## 1st Qu.:-0.24238 1st Qu.:0.2375 1st Qu.:0.1897 1st Qu.:0.2009 ## Median :-0.22125 Median :0.2602 Median :0.2125 Median :0.2250 ## Mean :-0.22630 Mean :0.2617 Mean :0.2120 Mean :0.2242 ## 3rd Qu.:-0.20880 3rd Qu.:0.2790 3rd Qu.:0.2299 3rd Qu.:0.2425 ## Max. : 0.00786 Max. :0.3682 Max. :0.3036 Max. :0.3140 ## SE.8 SE.9 SE.10 SE.11 ## Min. :0.1360 Min. :0.1456 Min. :0.1413 Min. :0.1140 ## 1st Qu.:0.2248 1st Qu.:0.2455 1st Qu.:0.2873 1st Qu.:0.2322 ## Median :0.2462 Median :0.2669 Median :0.3126 Median :0.2451 ## Mean :0.2465 Mean :0.2684 Mean :0.3133 Mean :0.2470 ## 3rd Qu.:0.2637 3rd Qu.:0.2865 3rd Qu.:0.3425 3rd Qu.:0.2689 ## Max. :0.3418 Max. :0.3736 Max. :0.4434 Max. :0.3560 ## SE.12 SE.13 SE.14 SE.15 ## Min. :0.05736 Min. :-1.08440 Min. :68.69 Min. :-0.005083 ## 1st Qu.:0.09146 1st Qu.:-0.25612 1st Qu.:76.12 1st Qu.: 0.018988 ## Median :0.09765 Median :-0.18609 Median :77.03 Median : 0.022585 ## Mean :0.09879 Mean :-0.21846 Mean :77.74 Mean : 0.025447 ## 3rd Qu.:0.10658 3rd Qu.:-0.14495 3rd Qu.:78.15 3rd Qu.: 0.026811 ## Max. :0.13276 Max. : 0.01809 Max. :95.19 Max. : 0.091893 ## SE.16 SE.17 SE.18 SE.19 ## Min. :-0.16007 Min. :0.5361 Min. :0.1033 Min. :-0.40757 ## 1st Qu.:-0.11926 1st Qu.:0.8312 1st Qu.:0.1285 1st Qu.:-0.12675 ## Median :-0.09639 Median :0.8495 Median :0.1436 Median :-0.10810 ## Mean :-0.08919 Mean :0.8366 Mean :0.1437 Mean :-0.11897 ## 3rd Qu.:-0.07352 3rd Qu.:0.8636 3rd Qu.:0.1541 3rd Qu.:-0.09032 ## Max. : 0.15774 Max. :0.9899 Max. :0.2042 Max. : 0.02788 ## SE.20 SE.21 MO.1 MO.2 ## Min. :0.9826 Min. :-0.11676 Min. : 871.2 Min. :22.65 ## 1st Qu.:1.3109 1st Qu.: 0.02502 1st Qu.:1317.3 1st Qu.:29.68 ## Median :1.3642 Median : 0.05684 Median :1420.2 Median :30.51 ## Mean :1.3713 Mean : 0.04697 Mean :1444.2 Mean :30.28 ## 3rd Qu.:1.4287 3rd Qu.: 0.08394 3rd Qu.:1505.9 3rd Qu.:31.59 ## Max. :1.6922 Max. : 0.14548 Max. :2520.6 Max. :33.96 ## MO.3 MO.4 MO.5 MO.6 ## Min. :11.36 Min. :1489 Min. :1427 Min. : 818.6 ## 1st Qu.:13.05 1st Qu.:1958 1st Qu.:2826 1st Qu.:1569.6 ## Median :13.55 Median :2072 Median :3038 Median :1821.3 ## Mean :13.53 Mean :2182 Mean :3025 Mean :1769.7 ## 3rd Qu.:13.89 3rd Qu.:2248 3rd Qu.:3285 3rd Qu.:1975.7 ## Max. :17.07 Max. :4061 Max. :4032 Max. :2297.1 ## MO.7 MO.8 OT.1 OT.2 ## Min. :0.2154 Min. :-11586902 Min. : 0.6937 Min. : 5.00 ## 1st Qu.:0.3379 1st Qu.: -6961708 1st Qu.: 70.3692 1st Qu.: 8.00 ## Median :0.3781 Median : -6065366 Median : 223.8042 Median :13.00 ## Mean :0.3945 Mean : -6102028 Mean : 326.7659 Mean :11.48 ## 3rd Qu.:0.4311 3rd Qu.: -4973060 3rd Qu.: 495.6101 3rd Qu.:13.00 ## Max. :0.6201 Max. : -1366687 Max. :1476.1378 Max. :16.00 ## OT.3 OT.4 OT.5 OT.6 ## Min. : 3.000 Min. : 5.000 Min. :1951 Min. :605.9 ## 1st Qu.: 4.000 1st Qu.: 5.000 1st Qu.:2082 1st Qu.:689.6 ## Median : 4.000 Median : 5.000 Median :2116 Median :736.7 ## Mean : 5.033 Mean : 7.197 Mean :2103 Mean :732.5 ## 3rd Qu.: 5.000 3rd Qu.:11.000 3rd Qu.:2135 3rd Qu.:780.3 ## Max. :10.000 Max. :11.000 Max. :2199 Max. :834.1 ## OT.7 OT.8 OT.9 pH ## Min. :209760 Min. :127.9 Min. :24.00 Min. :7.009 ## 1st Qu.:210724 1st Qu.:150.6 1st Qu.:25.12 1st Qu.:7.116 ## Median :211154 Median :154.3 Median :25.46 Median :7.183 ## Mean :211206 Mean :152.4 Mean :25.51 Mean :7.191 ## 3rd Qu.:211499 3rd Qu.:156.0 3rd Qu.:25.74 3rd Qu.:7.240 ## Max. :214558 Max. :158.7 Max. :28.53 Max. :7.513 ## CaCO3 Nt Ct Corg ## Min. : 3.818 Min. :0.03411 Min. : 2.223 Min. :0.2200 ## 1st Qu.:25.389 1st Qu.:0.08047 1st Qu.: 4.010 1st Qu.:0.8125 ## Median :28.963 Median :0.10717 Median : 4.522 Median :1.1000 ## Mean :29.084 Mean :0.12186 Mean : 4.675 Mean :1.3077 ## 3rd Qu.:33.049 3rd Qu.:0.13455 3rd Qu.: 5.141 3rd Qu.:1.6275 ## Max. :57.269 Max. :0.64192 Max. :10.668 Max. :7.0200 ## EC Sand Silt Clay ## Min. :136.2 Min. : 3.329 Min. :26.64 Min. :16.16 ## 1st Qu.:254.8 1st Qu.: 8.577 1st Qu.:41.87 1st Qu.:32.56 ## Median :269.9 Median :11.918 Median :46.43 Median :39.98 ## Mean :268.1 Mean :16.081 Mean :45.29 Mean :37.25 ## 3rd Qu.:282.6 3rd Qu.:18.876 3rd Qu.:49.02 3rd Qu.:43.34 ## Max. :345.4 Max. :61.401 Max. :57.63 Max. :55.51 ## MWD ## Min. :0.02227 ## 1st Qu.:0.05626 ## Median :0.07404 ## Mean :0.08156 ## 3rd Qu.:0.09544 ## Max. :0.24052 7.2.4 Run the first models # 03 Check covariates influences ############################################### SoilCovMLCon &lt;- SoilCov[[x]][,-c(81,82)] # Remove ID and site name NumCovLayer = 80 # define number of covariate layer after hot coding StartTargetCov = NumCovLayer + 1 # start column after all covariates NumDataCol= ncol(SoilCovMLCon) # number of column in all data set preproc &lt;- preProcess(SoilCovMLCon[,1:NumCovLayer], method=c(&quot;range&quot;)) SoilCovMLConTrans &lt;- predict(preproc, SoilCovMLCon[,1:NumCovLayer]) SoilCovMLConTrans &lt;- cbind(SoilCovMLConTrans, SoilCovMLCon[,c(StartTargetCov:NumDataCol)]) # 03.1 Develop models ======================================================== FormulaMLCon = list() for (i in 1:(NumDataCol - NumCovLayer)) { FormulaMLCon[[i]] = as.formula(paste(names(SoilCovMLConTrans)[NumCovLayer+i],&quot; ~ &quot;,paste(names(SoilCovMLConTrans)[1:NumCovLayer],collapse=&quot;+&quot;))) } # Define traincontrol TrainControl &lt;- trainControl(method=&quot;repeatedcv&quot;, 10, 3, allowParallel = TRUE, savePredictions=TRUE) seed=1070 # Train different ML algorithms #rpart (CART) FitRpartCon = list() start_time &lt;- proc.time() for (i in 1:length(FormulaMLCon)) { set.seed(seed) FitRpartCon[[i]] &lt;- train(FormulaMLCon[[i]], data=SoilCovMLConTrans, method=&quot;rpart&quot;, metric=&quot;RMSE&quot;, trControl=TrainControl) print(names(SoilCovMLConTrans)[i+NumCovLayer]) } end_time &lt;- proc.time() print(end_time - start_time) print(&quot;CART done&quot;) #Knn FitKnnCon = list() start_time &lt;- proc.time() for (i in 1:length(FormulaMLCon)) { set.seed(seed) FitKnnCon[[i]] &lt;- train(FormulaMLCon[[i]], data=SoilCovMLConTrans, method=&quot;knn&quot;, metric=&quot;RMSE&quot;, trControl=TrainControl) print(names(SoilCovMLConTrans)[i+NumCovLayer]) } end_time &lt;- proc.time() print(end_time - start_time) print(&quot;Knn done&quot;) # SVM FitSvrCon = list() start_time &lt;- proc.time() for (i in 1:length(FormulaMLCon)) { set.seed(seed) FitSvrCon [[i]] &lt;- train(FormulaMLCon[[i]], data=SoilCovMLConTrans, method=&quot;svmRadial&quot;, metric=&quot;RMSE&quot;, trControl=TrainControl) print(names(SoilCovMLConTrans)[i+NumCovLayer]) } end_time &lt;- proc.time() print(end_time - start_time) print(&quot;SVM done&quot;) # Cubist FitCubCon = list() start_time &lt;- proc.time() for (i in 1:length(FormulaMLCon)) { set.seed(seed) FitCubCon [[i]] &lt;- train(FormulaMLCon[[i]], data=SoilCovMLConTrans, method=&quot;cubist&quot;, metric=&quot;RMSE&quot;, trControl=TrainControl) print(names(SoilCovMLConTrans)[i+NumCovLayer]) } end_time &lt;- proc.time() print(end_time - start_time) print(&quot;Cubist done&quot;) # QRF FitQRaFCon = list() start_time &lt;- proc.time() for (i in 1:length(FormulaMLCon)) { set.seed(seed) FitQRaFCon [[i]] &lt;- train(FormulaMLCon[[i]], data=SoilCovMLConTrans, method=&quot;qrf&quot;, metric=&quot;RMSE&quot;, trControl=TrainControl) print(names(SoilCovMLConTrans)[i+NumCovLayer]) } end_time &lt;- proc.time() print(end_time - start_time) print(&quot;QRF done&quot;) # 03.2 Combine models statistics =============================================== # Look at the primary results of ML ModelConList = list() for (i in 1:length(FormulaMLCon)) { ModelConList[[i]] &lt;- list(CART=FitRpartCon[[i]], Knn=FitKnnCon[[i]],SVM=FitSvrCon[[i]], Cubist=FitCubCon[[i]], QRF=FitQRaFCon[[i]]) } ResultsModelCon = list() for (i in 1:length(ModelConList)) { ResultsModelCon[[i]] &lt;- resamples(ModelConList[[i]]) } SummaryModelCon = list() for (i in 1:length(ResultsModelCon)) { SummaryModelCon[[i]] &lt;- summary(ResultsModelCon[[i]]) } # Scale the models ScalesMolel &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;)) BwplotModelCon = list() for (i in 1:length(ResultsModelCon)){ BwplotModelCon[[i]] &lt;- bwplot(ResultsModelCon[[i]], scales=ScalesMolel, main = paste0(&quot;Comparative models of &quot;,names(SoilCovMLCon)[NumCovLayer+i], &quot; for &quot;, depth, &quot; soil&quot;)) png(paste0(&quot;./export/preprocess/&quot;, depth,&quot;/Boxplot_first_run_model_&quot;,names(SoilCovMLConTrans)[NumCovLayer+i], &quot;_for_&quot;,depth,&quot;_soil.png&quot;), # File name width = 800, height = 800) plot(BwplotModelCon[[i]]) dev.off() } # Calculate Error indices Error1Con = list() for (i in 1:length(FormulaMLCon)) { Error1Con[[i]] &lt;- NaN*seq(length(FormulaMLCon)) for(j in 1:(3 * length(ModelConList[[i]]))) { Error1Con[[i]][j] &lt;- mean(SummaryModelCon[[i]]$values[[j]]) }} ErrorIndex2Con &lt;- data.frame(NaN) for (i in 1:length(Error1Con)) { ErrorIndexCon &lt;- data.frame(matrix(Error1Con[[i]], nrow = length(ModelConList[[1]]), ncol = 3, byrow=T)) colnames(ErrorIndexCon) &lt;- c(paste(&quot;MAE&quot;,names(SoilCovMLCon)[NumCovLayer+i]), paste(&quot;RMSE&quot;,names(SoilCovMLCon)[NumCovLayer+i]), paste(&quot;R2&quot;,names(SoilCovMLCon)[NumCovLayer+i])) ErrorIndex2Con &lt;- cbind(ErrorIndex2Con,ErrorIndexCon) rownames(ErrorIndex2Con) &lt;- names(ModelConList[[1]]) } write.csv(data.frame(ErrorIndex2Con), paste0(&quot;./export/preprocess/&quot;, depth,&quot;/First_run_models_results_for_&quot;,depth,&quot;_soil.csv&quot;)) # 03.3 Look at models covariates influences ==================================== ModelsPlots = list() for (i in 1:length(ModelConList)) { AllVarImportance &lt;- data.frame() # Cart does not have a variables influence for (j in 2:5) { var_importance &lt;- varImp(ModelConList[[i]][[j]], scale = TRUE) importance_df &lt;- as.data.frame(var_importance$importance) importance_df$Variable &lt;- rownames(importance_df) importance_df$Model &lt;- names(ModelConList[[i]][j]) AllVarImportance &lt;- rbind(AllVarImportance, importance_df) } AvgVarImportance &lt;- AllVarImportance %&gt;% group_by(Variable) %&gt;% summarise(AvgImportance = mean(Overall, na.rm = TRUE)) %&gt;% arrange(desc(AvgImportance)) # Select top 20 variables Top20Var &lt;- AvgVarImportance %&gt;% top_n(20, wt = AvgImportance) AllVarImportanceTop20 &lt;- AllVarImportance %&gt;% filter(Variable %in% Top20Var$Variable) AllVarImportanceLong &lt;- melt(AllVarImportanceTop20, id.vars = c(&quot;Variable&quot;, &quot;Model&quot;), variable.name = &quot;Metric&quot;, value.name = &quot;Importance&quot;) ModelsPlots[[i]] &lt;- ggplot(AllVarImportanceLong, aes(x = reorder(Variable, Importance), y = Importance, fill = Model)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + coord_flip() + labs(title = paste0(&quot;Top 20 covariates influence accros all models of &quot;, names(SoilCovMLConTrans)[NumCovLayer+i], &quot; for &quot;, depth, &quot; soil&quot;), x = &quot;Covariates&quot;, y = &quot;Importance&quot;) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + scale_fill_brewer(palette = &quot;Set3&quot;) ggsave(paste0(&quot;./export/preprocess/&quot;, depth,&quot;/First_run_model_top_20_covariates_influence_of_&quot;,names(SoilCovMLConTrans)[NumCovLayer+i], &quot;_for_&quot;,depth,&quot;_soil.png&quot;), ModelsPlots[[i]], width = 30, height = 10) ggsave(paste0(&quot;./export/preprocess/&quot;, depth,&quot;/First_run_model_top_20_covariates_influence_of_&quot;,names(SoilCovMLConTrans)[NumCovLayer+i], &quot;_for_&quot;,depth,&quot;_soil.pdf&quot;), ModelsPlots[[i]], width = 30, height = 10) plot(ModelsPlots[[i]]) } 7.2.5 Boruta and RFE selections # 03.5 Boruta selction ========================================================= Boruta = list() BorutaLabels = list() Boruta_covariates = list() # Individual plots for (i in 1:length(FormulaMLCon)) { set.seed(seed) Boruta[[i]] &lt;- Boruta(FormulaMLCon[[i]], data = SoilCovMLConTrans) BorutaBank &lt;- TentativeRoughFix(Boruta[[i]]) pdf(paste0(&quot;./export/boruta/&quot;, depth,&quot;/Boruta_&quot;,names(SoilCovMLConTrans)[NumCovLayer+i], &quot;_for_&quot;,depth,&quot;_soil.pdf&quot;), # File name width = 8, height = 8, # Width and height in inches bg = &quot;white&quot;, # Background color colormodel = &quot;cmyk&quot;) # Color model plot(BorutaBank, xlab = &quot;&quot;, xaxt = &quot;n&quot;, main=paste0(&quot;Feature Importance - Boruta &quot;,names(SoilCovMLConTrans)[NumCovLayer+i],&quot; for &quot;, depth ,&quot; cm depth&quot;)) lz &lt;- lapply(1:ncol(BorutaBank$ImpHistory), function(j)BorutaBank$ImpHistory[is.finite(BorutaBank$ImpHistory[,j]),j]) names(lz) &lt;- c(names(SoilCovMLConTrans)[1:NumCovLayer],c(&quot;sh_Max&quot;,&quot;sh_Mean&quot;,&quot;sh_Min&quot;)) Labels &lt;- sort(sapply(lz,median)) axis(side = 1,las=2,labels = names(Labels),at = 1:ncol(BorutaBank$ImpHistory), cex.axis = 1) dev.off() # Close the device BorutaLabels[[i]] &lt;- sapply(lz,median) confirmed_features &lt;- getSelectedAttributes(BorutaBank, withTentative = FALSE) Boruta_covariates[[i]] &lt;- cbind(SoilCovMLConTrans[, confirmed_features], SoilCovMLConTrans[, NumCovLayer + i]) colnames(Boruta_covariates[[i]]) &lt;- c(confirmed_features,names(SoilCovMLConTrans)[NumCovLayer+i]) write.csv(data.frame(Boruta_covariates[[i]]), paste0(&quot;./export/boruta/&quot;, depth,&quot;/Boruta_results_&quot;,names(SoilCovMLConTrans)[NumCovLayer+i], &quot;_for_&quot;,depth,&quot;_soil.csv&quot;)) print(names(SoilCovMLConTrans)[NumCovLayer+i]) } # Combinned plot BorutaResultCon = data.frame();BorutaResultCon = data.frame(BorutaLabels[[1]]) for (i in 2:length(FormulaMLCon)) { BorutaResultCon[i] = data.frame(BorutaLabels[[i]])} BorutaResultCon = BorutaResultCon[c(1:NumCovLayer),] names(BorutaResultCon) &lt;- names(SoilCovMLConTrans)[c(StartTargetCov:NumDataCol)] BorutaResultConT = data.frame();BorutaResultConT = data.frame(Boruta[[1]]$finalDecision == &quot;Confirmed&quot;) for (i in 2:length(FormulaMLCon)) { BorutaResultConT[i] = data.frame(Boruta[[i]]$finalDecision == &quot;Confirmed&quot;)} names(BorutaResultConT) &lt;- names(SoilCovMLConTrans)[c(StartTargetCov:NumDataCol)] BorutaCovPlot = gather(BorutaResultCon,key,value);BorutaCovPlotT = gather(BorutaResultConT,key,value) BorutaCovPlot$cov = rep(row.names(BorutaResultCon), (NumDataCol-NumCovLayer)) names(BorutaCovPlot) = c(&quot;Y&quot;,&quot;Z&quot;,&quot;X&quot;);BorutaCovPlot$Z.1 = BorutaCovPlotT$value BorutaCovPlot$Y = factor(BorutaCovPlot$Y);BorutaCovPlot$X = factor(BorutaCovPlot$X) BorutaCovPlot$Z.1 &lt;- as.logical(BorutaCovPlot$Z.1) BorutaCovPlot$Y = factor(BorutaCovPlot$Y, levels = rev(unique(BorutaCovPlot$Y))) FigCovImpoBr = ggplot(BorutaCovPlot, aes(x = X, y = Y)) + geom_tile(aes(fill = Z, colour = Z.1), size = 1,show.legend=F) + labs(title = paste0(&quot;Boruta combinned plot for &quot;, depth ,&quot; depth&quot;) , x = &quot;Covariates&quot;, y = &quot;Soil properties&quot;)+ theme_classic() + scale_fill_gradient(limits = c(min(BorutaCovPlot$Z), max(BorutaCovPlot$Z)), low=&quot;#ffffd9&quot;, high=&quot;#081d58&quot;) + theme(axis.text.x = element_text(colour = &quot;black&quot;, size=10, angle = 90, hjust = 1), axis.text.y = element_text(colour = &quot;black&quot;, size=10)) + geom_text(aes(label = round(Z, 1)),cex=3) + scale_color_manual(values = c(&#39;#00000000&#39;, &#39;red&#39;)) + coord_flip() + coord_equal() ggsave(paste0(&quot;./export/boruta/&quot;, depth,&quot;/Boruta_final_combinned_plot_&quot;, depth,&quot;_soil.png&quot;), FigCovImpoBr, width = 30, height = 10) ggsave(paste0(&quot;./export/boruta/&quot;, depth,&quot;/Boruta_final_combinned_plot_&quot;, depth,&quot;_soil.pdf&quot;), FigCovImpoBr, width = 30, height = 10) plot(FigCovImpoBr) # 03.6 RFE covariate influence ================================================= TrainControlRFE &lt;- rfeControl(functions = rfFuncs,method = &quot;repeatedcv&quot;, repeats = 3,verbose = FALSE) ResultRFECon =list() subsets= c(seq(1,NumCovLayer,5)) for (i in 1:length(FormulaMLCon)) { set.seed(seed) ResultRFECon[[i]] &lt;- rfe(FormulaMLCon[[i]], data=SoilCovMLConTrans, sizes = subsets,rfeControl = TrainControlRFE) print(names(SoilCovMLConTrans)[i+NumCovLayer]) } RFE_covariates &lt;- list() for (i in 1:length(ResultRFECon)) { RFEpredictors=predictors(ResultRFECon[[i]]) RFE_covariates[[i]] &lt;- cbind(SoilCovMLConTrans[, RFEpredictors], SoilCovMLConTrans[, NumCovLayer + i]) colnames(RFE_covariates[[i]]) &lt; c(RFEpredictors,names(SoilCovMLConTrans)[NumCovLayer+i]) write.table(data.frame(RFEpredictors), paste0(&quot;./export/RFE/&quot;, depth,&quot;/RFE_results_&quot;,names(SoilCovMLConTrans)[NumCovLayer+i], &quot;_for_&quot;,depth,&quot;_soil.txt&quot;)) } PlotResultRFE =list() for (i in 1:length(ResultRFECon)) { trellis.par.set(caretTheme()) PlotResultRFE[[i]] = plot(ResultRFECon[[i]], type = c(&quot;g&quot;, &quot;o&quot;), main=paste0(&quot;RFE of &quot;,names(SoilCovMLConTrans)[NumCovLayer+i],&quot; for &quot;,depth, &quot; soil&quot;), xlab=&quot;Optimal variables number&quot;) pdf(paste0(&quot;./export/RFE/&quot;, depth,&quot;/RFE_&quot;,names(SoilCovMLConTrans)[NumCovLayer+i], &quot;_for_&quot;,depth,&quot;_soil.pdf&quot;), # File name width = 12, height = 12, # Width and height in inches bg = &quot;white&quot;, # Background color colormodel = &quot;cmyk&quot;) # Color model plot(PlotResultRFE[[i]]) dev.off() } # 03.7 Export results ========================================================== # Change the name of list regarding each soil depth: first, second, third, # fourth and fifth. #=============================================================================== First_depth_preprocess &lt;- list( Cov = SoilCovMLConTrans, Cov_original = SoilCovMLCon, Models = ModelConList, Models_plots = ModelsPlots, Selected_cov_boruta = Boruta_covariates, Selected_cov_RFE = RFE_covariates, Boruta_full_fig = FigCovImpoBr, Boruta = Boruta, RFE = ResultRFECon, RFE_fig = PlotResultRFE ) save(First_depth_preprocess, file = paste0(&quot;./export/save/Selected_cov_&quot;, depth,&quot;.RData&quot;)) rm(list = ls()) 7.3 Models developpment 7.3.1 Preparation of the environment # 0 Environment setup ########################################################## # 0.1 Prepare environment ====================================================== # Folder check getwd() # Set folder direction setwd() # Clean up workspace rm(list = ls(all.names = TRUE)) # 0.2 Install packages ========================================================= install.packages(&quot;pacman&quot;) #Install and load the &quot;pacman&quot; package (allow easier download of packages) library(pacman) pacman::p_load(ggplot2, caret, patchwork, quantregForest, Cubist, caretEnsemble, readr, grid, gridExtra, dplyr) # 0.3 Show session infos ======================================================= sessionInfo() ## R version 4.4.0 (2024-04-24 ucrt) ## Platform: x86_64-w64-mingw32/x64 ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=French_France.utf8 LC_CTYPE=French_France.utf8 ## [3] LC_MONETARY=French_France.utf8 LC_NUMERIC=C ## [5] LC_TIME=French_France.utf8 ## ## time zone: Europe/Berlin ## tzcode source: internal ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] dplyr_1.1.4 gridExtra_2.3 caretEnsemble_4.0.1 ## [4] patchwork_1.3.0 Cubist_0.4.4 quantregForest_1.3-7.1 ## [7] RColorBrewer_1.1-3 randomForest_4.7-1.2 caret_6.0-94 ## [10] lattice_0.22-6 viridisLite_0.4.2 pacman_0.5.1 ## [13] stringr_1.5.1 readr_2.1.5 ggplot2_3.5.1 ## [16] bookdown_0.41 tufte_0.13 rmarkdown_2.29 ## [19] knitr_1.49 ## ## loaded via a namespace (and not attached): ## [1] mathjaxr_1.6-0 rstudioapi_0.17.1 jsonlite_1.8.9 ## [4] magrittr_2.0.3 farver_2.1.2 vctrs_0.6.5 ## [7] base64enc_0.1-3 terra_1.7-83 htmltools_0.5.8.1 ## [10] curl_5.2.3 raster_3.6-30 pROC_1.18.5 ## [13] sass_0.4.9 parallelly_1.38.0 KernSmooth_2.23-22 ## [16] bslib_0.8.0 htmlwidgets_1.6.4 plyr_1.8.9 ## [19] lubridate_1.9.3 cachem_1.1.0 uuid_1.2-1 ## [22] lifecycle_1.0.4 iterators_1.0.14 pkgconfig_2.0.3 ## [25] Matrix_1.7-0 R6_2.5.1 fastmap_1.2.0 ## [28] future_1.34.0 digest_0.6.37 colorspace_2.1-1 ## [31] leafem_0.2.3 crosstalk_1.2.1 labeling_0.4.3 ## [34] fansi_1.0.6 timechange_0.3.0 compiler_4.4.0 ## [37] proxy_0.4-27 bit64_4.5.2 withr_3.0.2 ## [40] brew_1.0-10 DBI_1.2.3 MASS_7.3-60.2 ## [43] lava_1.8.0 leaflet_2.2.2 classInt_0.4-10 ## [46] ModelMetrics_1.2.2.2 tools_4.4.0 units_0.8-5 ## [49] future.apply_1.11.3 nnet_7.3-19 glue_1.7.0 ## [52] satellite_1.0.5 nlme_3.1-164 sf_1.0-18 ## [55] cluster_2.1.6 reshape2_1.4.4 generics_0.1.3 ## [58] recipes_1.1.0 gtable_0.3.6 leaflet.providers_2.0.0 ## [61] tzdb_0.4.0 class_7.3-22 data.table_1.16.2 ## [64] hms_1.1.3 sp_2.1-4 utf8_1.2.4 ## [67] foreach_1.5.2 pillar_1.9.0 vroom_1.6.5 ## [70] splines_4.4.0 survival_3.5-8 bit_4.5.0 ## [73] tidyselect_1.2.1 svglite_2.1.3 stats4_4.4.0 ## [76] xfun_0.48 hardhat_1.4.0 leafpop_0.1.0 ## [79] timeDate_4041.110 stringi_1.8.4 yaml_2.3.10 ## [82] evaluate_1.0.1 codetools_0.2-20 kernlab_0.9-33 ## [85] tcltk_4.4.0 tibble_3.2.1 cli_3.6.3 ## [88] rpart_4.1.23 systemfonts_1.1.0 munsell_0.5.1 ## [91] jquerylib_0.1.4 Rcpp_1.0.13 globals_0.16.3 ## [94] png_0.1-8 parallel_4.4.0 gower_1.0.1 ## [97] listenv_0.9.1 ipred_0.9-15 scales_1.3.0 ## [100] prodlim_2024.06.25 e1071_1.7-16 purrr_1.0.2 ## [103] crayon_1.5.3 rlang_1.1.4 7.3.2 Tune the model # 04 Tuning all the models #################################################### # Here we decided to split every run by soil depth to have a better vision # on the running process. # 04.1 Prepare data ============================================================ depth &lt;- &quot;0_10&quot; load(file = paste0(&quot;./export/save/Selected_cov_&quot;,depth,&quot;.RData&quot;)) SoilCovML &lt;- First_depth_preprocess$Selected_cov_boruta seed=1070 # Formula for the selected covariates with Boruta FormulaML &lt;- list() for (i in 1:length(SoilCovML)) { SoilCovMLBoruta &lt;- SoilCovML[[i]] StartTargetCov = ncol(SoilCovMLBoruta) NumCovLayer = StartTargetCov - 1 FormulaML[[i]] &lt;- as.formula(paste(names(SoilCovMLBoruta)[StartTargetCov],&quot; ~ &quot;,paste(names(SoilCovMLBoruta)[1:NumCovLayer],collapse=&quot;+&quot;))) } # 04.2 Split the test and train data =========================================== # Split the data with a 80/20 ration Train_data &lt;- list() Test_data &lt;- list() for (i in 1:length(FormulaML)) { set.seed(seed) split &lt;- createDataPartition(SoilCovML[[i]][,length(SoilCovML[[i]])], p = 0.8, list = FALSE, times = 1) Train_data[[i]] &lt;- SoilCovML[[i]][ split,] Test_data[[i]] &lt;- SoilCovML[[i]][-split,] } # 04.3 Define train control ==================================================== set.seed(seed) TrainControl &lt;- trainControl(method=&quot;repeatedcv&quot;, 10, 3, allowParallel = TRUE, savePredictions=TRUE) SVMrgrid &lt;- expand.grid(sigma = seq(0.01, 0.5, by = 0.05), C = c(0.25, 0.5, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512)) Cubistgrid &lt;- expand.grid(committees = c(1, 5, 10, 15, 20), neighbors = c(0, 1, 2, 3, 5, 7, 9)) # 04.4 Run the models ========================================================== #rpart (CART) FitRpartCon = list() start_time &lt;- proc.time() for (i in 1:length(FormulaML)) { set.seed(seed) FitRpartCon[[i]] &lt;- train(FormulaML[[i]], data=Train_data[[i]], method=&quot;rpart&quot;, tuneLength = 20, metric=&quot;RMSE&quot;, trControl=TrainControl) print(names(Train_data[[i]])[length(Train_data[[i]])]) } end_time &lt;- proc.time() print(end_time - start_time) print(&quot;CART done&quot;) #Knn FitKnnCon = list() start_time &lt;- proc.time() for (i in 1:length(FormulaML)) { set.seed(seed) FitKnnCon[[i]] &lt;- train(FormulaML[[i]], data=Train_data[[i]], method=&quot;knn&quot;, tuneLength = 30, metric=&quot;RMSE&quot;, trControl=TrainControl) print(names(Train_data[[i]])[length(Train_data[[i]])]) } end_time &lt;- proc.time() print(end_time - start_time) print(&quot;Knn done&quot;) # SVM FitSvrCon = list() start_time &lt;- proc.time() for (i in 1:length(FormulaML)) { set.seed(seed) FitSvrCon [[i]] &lt;- train(FormulaML[[i]], data=Train_data[[i]], method=&quot;svmRadial&quot;, tuneGrid = SVMrgrid, metric=&quot;RMSE&quot;, trControl=TrainControl) print(names(Train_data[[i]])[length(Train_data[[i]])]) } end_time &lt;- proc.time() print(end_time - start_time) print(&quot;SVM done&quot;) # Cubist FitCubCon = list() start_time &lt;- proc.time() for (i in 1:length(FormulaML)) { set.seed(seed) FitCubCon [[i]] &lt;- train(FormulaML[[i]], data=Train_data[[i]], method=&quot;cubist&quot;, tuneGrid = Cubistgrid, metric=&quot;RMSE&quot;, trControl=TrainControl) print(names(Train_data[[i]])[length(Train_data[[i]])]) } end_time &lt;- proc.time() print(end_time - start_time) print(&quot;Cubist done&quot;) # QRF FitQRaFCon = list() start_time &lt;- proc.time() for (i in 1:length(FormulaML)) { set.seed(seed) FitQRaFCon [[i]] &lt;- train(FormulaML[[i]], data=Train_data[[i]], method=&quot;qrf&quot;, tuneGrid = expand.grid(mtry = seq(1, length(Train_data[[i]]-1), by = 1)), metric=&quot;RMSE&quot;, trControl=TrainControl) print(names(Train_data[[i]])[length(Train_data[[i]])]) } end_time &lt;- proc.time() print(end_time - start_time) print(&quot;QRF done&quot;) # 04.5 Combine models statistics =============================================== # Look at the primary results of ML ModelList = list() for (i in 1:length(FormulaML)) { ModelList[[i]] &lt;- list(CART=FitRpartCon[[i]], Knn=FitKnnCon[[i]],SVM=FitSvrCon[[i]], Cubist=FitCubCon[[i]], QRF=FitQRaFCon[[i]]) } # Look at the primary results of ML ResultsModelCon = list() for (i in 1:length(ModelConList)) { ResultsModelCon[[i]] &lt;- resamples(ModelConList[[i]]) write.csv(as.data.frame(modelCor(ResultsModelCon[[i]])), paste0(&quot;./export/models/&quot;, depth, &quot;/Models_correlations_of_&quot;,names(Train_data[[i]])[length(Train_data[[i]])], &quot;_for_&quot;, depth, &quot;_soil.csv&quot;)) } # 04.6 Check the correlation between the models ================================ for (i in 1:length(ModelConList)) { png(paste0(&quot;./export/models/&quot;, depth,&quot;/Correlation between the models of &quot;, names(Train_data[[i]])[length(Train_data[[i]])], &quot; for &quot;, depth, &quot; soil.png&quot;), width = 1600, height = 1600) splom(ResultsModelCon[[i]]) dev.off() pdf(paste0(&quot;./export/models/&quot;, depth,&quot;/Correlation between the models of &quot;, names(Train_data[[i]])[length(Train_data[[i]])], &quot; for &quot;, depth, &quot; soil.pdf&quot;), width = 15, height = 15, bg = &quot;white&quot;, colormodel = &quot;cmyk&quot;) splom(ResultsModelCon[[i]]) dev.off() } # 05 Create an ensemble learning ############################################### # 05.1 Run the model with an RF method ========================================= EnsembleModel &lt;- list() set.seed(seed) for (i in 1:length(ModelConList)) { EnsembleModel[[i]] &lt;- caretStack(ModelSecondList[[i]], method=&quot;rf&quot;, metric=&quot;RMSE&quot;, trControl=TrainControl, tuneGrid = expand.grid(mtry = seq(1, length(Train_data[[i]]-1), by = 1))) } # 05.2 Plot models best tunning ================================================ for (i in 1:length(ModelConLisn)){ gg1 &lt;- plot(ModelConList[[i]]$CART, main = &quot;CART tuning parameters&quot;) gg2 &lt;- plot(ModelConList[[i]]$Knn, main = &quot;Knn tuning parameters&quot;) gg3 &lt;- plot(ModelConList[[i]]$SVM, main = &quot;SVMr tuning parameters&quot;) gg4 &lt;- plot(ModelConList[[i]]$Cubist, main = &quot;Cubist tuning parameters&quot;) gg5 &lt;- plot(ModelConList[[i]]$QRF, main = &quot;QRF tuning parameters&quot;) gg6 &lt;- plot(EnsembleModel[[i]]$ens_model, main = &quot;Ensemble tuning parameters&quot;) png(paste0(&quot;./export/models/&quot;, depth,&quot;/Models_tuning_parameters_&quot;,names(Train_data[[i]])[length(Train_data[[i]])], &quot;_for_&quot;,depth,&quot;_soil.png&quot;), # File name width = 1900, height = 1200) grid.arrange(arrangeGrob(gg1, gg2, gg3, gg4, gg5, gg6, nrow = 2, ncol = 3), top = textGrob(paste0(&quot;Models tuning parameters for &quot;,names(Train_data[[i]])[length(Train_data[[i]])] ,&quot; at &quot;, depth , &quot; depth&quot;), gp = gpar(fontsize = 16, fontface = &quot;bold&quot;))) dev.off() pdf(paste0(&quot;./export/models/&quot;, depth,&quot;/Models_tuning_parameters_&quot;,names(Train_data[[i]])[length(Train_data[[i]])], &quot;_for_&quot;,depth,&quot;_soil.pdf&quot;), # File name width = 19, height = 12, bg = &quot;white&quot;, colormodel = &quot;cmyk&quot;) grid.arrange(arrangeGrob(gg1, gg2, gg3, gg4, gg5, gg6, nrow = 2, ncol = 3), top = textGrob(paste0(&quot;Models tuning parameters for &quot;,names(Train_data[[i]])[length(Train_data[[i]])] ,&quot; at &quot;, depth , &quot; depth&quot;), gp = gpar(fontsize = 16, fontface = &quot;bold&quot;))) dev.off() } Models tuning parameters for pH at 0 - 10 cm # 05.3 Combine all models ====================================================== # Look at the primary results of ML ModelSecondList = list() for (i in 1:length(FormulaML)) { ModelSecondList[[i]] &lt;- list(CART=FitRpartCon[[i]], Knn=FitKnnCon[[i]],SVM=FitSvrCon[[i]], Cubist=FitCubCon[[i]], QRF=FitQRaFCon[[i]], Ensemble = EnsembleModel[[i]]$ens_model) } ResultsSecondList = list() for (i in 1:length(ModelSecondList)) { ResultsSecondList[[i]] &lt;- resamples(ModelSecondList[[i]]) } SummarySecondList = list() for (i in 1:length(ModelSecondList)) { SummarySecondList[[i]] &lt;- summary(ResultsSecondList[[i]]) } # Scale and plot the models performances ScalesModel &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;)) BwplotModelCon = list() for (i in 1:length(ResultsSecondList)){ BwplotModelCon[[i]] &lt;- bwplot(ResultsSecondList[[i]], scales=ScalesModel, main = paste0(&quot;Boxplot of the different models for &quot;,names(Train_data[[i]])[length(Train_data[[i]])], &quot; at &quot;, depth , &quot; cm interval&quot;)) png(paste0(&quot;./export/models/&quot;, depth,&quot;/Boxplot_final_model_&quot;,names(Train_data[[i]])[length(Train_data[[i]])], &quot;_for_&quot;,depth,&quot;_soil.png&quot;), width = 1600, height = 1300) plot(BwplotModelCon[[i]]) dev.off() pdf(paste0(&quot;./export/models/&quot;, depth,&quot;/Boxplot_final_models_&quot;,names(Train_data[[i]])[length(Train_data[[i]])], &quot;_for_&quot;,depth,&quot;_soil.pdf&quot;), width = 10, height = 8, bg = &quot;white&quot;, colormodel = &quot;cmyk&quot;) plot(BwplotModelCon[[i]]) dev.off() } # 05.4 Calculate error of all models =========================================== # Calculate Error indices Error1Con &lt;- list() for (i in 1:length(FormulaML)) { Error1Con[[i]] &lt;- NaN*seq(length(FormulaML)) for(j in 1:(3 * length(ModelSecondList[[i]]))) { Error1Con[[i]][j] &lt;- mean(SummarySecondList[[i]]$values[[j]]) } } ErrorIndex2Con &lt;- data.frame(NaN) for (i in 1:length(Error1Con)) { ErrorIndexCon &lt;- data.frame(matrix(Error1Con[[i]], nrow = length(ModelSecondList[[1]]), ncol = 3, byrow=T)) colnames(ErrorIndexCon) &lt;- c(paste(&quot;MAE&quot;,names(Train_data[[i]])[length(Train_data[[i]])]), paste(&quot;RMSE&quot;,names(Train_data[[i]])[length(Train_data[[i]])]), paste(&quot;R2&quot;,names(Train_data[[i]])[length(Train_data[[i]])])) ErrorIndex2Con &lt;- cbind(ErrorIndex2Con,ErrorIndexCon) rownames(ErrorIndex2Con) &lt;- c(names(ModelSecondList[[1]])) } write.csv(data.frame(ErrorIndex2Con), paste0(&quot;./export/models/&quot;, depth,&quot;/Models_results_for_&quot;,depth,&quot;_soil.csv&quot;)) # 05.5 Plot variable importance ================================================ ModelsPlots = list() for (i in 1:length(ModelSecondList)) { AllVarImportance &lt;- data.frame() # Cart does not have a variables influence and Ensemble does not use the same variables for (j in 2:5) { var_importance &lt;- varImp(ModelSecondList[[i]][[j]], scale = TRUE) importance_df &lt;- as.data.frame(var_importance$importance) importance_df$Variable &lt;- rownames(importance_df) importance_df$Model &lt;- names(ModelSecondList[[i]][j]) AllVarImportance &lt;- rbind(AllVarImportance, importance_df) } AvgVarImportance &lt;- AllVarImportance %&gt;% group_by(Variable) %&gt;% summarise(AvgImportance = mean(Overall, na.rm = TRUE)) %&gt;% arrange(desc(AvgImportance)) # Select top 20 variables Top20Var &lt;- AvgVarImportance %&gt;% top_n(20, wt = AvgImportance) AllVarImportanceTop20 &lt;- AllVarImportance %&gt;% filter(Variable %in% Top20Var$Variable) AllVarImportanceLong &lt;- melt(AllVarImportanceTop20, id.vars = c(&quot;Variable&quot;, &quot;Model&quot;), variable.name = &quot;Metric&quot;, value.name = &quot;Importance&quot;) ModelsPlots[[i]] &lt;- ggplot(AllVarImportanceLong, aes(x = reorder(Variable, Importance), y = Importance, fill = Model)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + coord_flip() + labs(title = paste0(&quot;Top 20 covariates influence accros all models of &quot;, names(Train_data[[i]])[length(Train_data[[i]])], &quot; for &quot;, depth, &quot; soil&quot;), x = &quot;Covariates&quot;, y = &quot;Importance&quot;) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + scale_fill_brewer(palette = &quot;Set3&quot;) ggsave(paste0(&quot;./export/models/&quot;, depth,&quot;/Final_models_top_20_covariates_influence_of_&quot;,names(Train_data[[i]])[length(Train_data[[i]])], &quot;_for_&quot;,depth,&quot;_soil.png&quot;), ModelsPlots[[i]], width = 30, height = 10) ggsave(paste0(&quot;./export/models/&quot;, depth,&quot;/Final_models_top_20_covariates_influence_of_&quot;,names(Train_data[[i]])[length(Train_data[[i]])], &quot;_for_&quot;,depth,&quot;_soil.pdf&quot;), ModelsPlots[[i]], width = 30, height = 10) } ## Warning: Removed 3 rows containing missing values or values outside the scale range ## (`geom_bar()`). ## Warning: Removed 11 rows containing missing values or values outside the scale range ## (`geom_bar()`). # 05.6 Export results ========================================================== # Change the name of list regarding each soil depth: first, second, third, # fourth and fifth. #=============================================================================== First_depth_models &lt;- list( Cov = SoilCovML, Models = ModelSecondList, Models_plots = ModelsPlots, Boxplot_models = BwplotModelCon, Train_data = Train_data, Test_data = Test_data, Results = ErrorIndex2Con ) save(First_depth_models, file = paste0(&quot;./export/save/Models_&quot;,depth,&quot;_DSM.RData&quot;)) rm(list= ls()) 7.4 Model evaluation We used five metrics to evaluate the model: RMSE= Root mean square error R\\(^2\\)= Coefficient of determination, also called rsquared. MAE = Mean absolute error CCC = Concordance correlation coefficient PICP = Prediction interval coverage probability set at 90% interval \\[ \\\\[0.5cm] RMSE = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{N}} \\] \\[ \\\\[0.5cm] R^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y}_i)^2} \\] \\[ \\\\[0.5cm] MAE = \\frac{1}{n} \\sum_{i=1}^{n}|Y_i - \\hat{Y}_i| \\] \\[ \\\\[0.5cm] CCC = \\frac{2S_{XY}}{S_{X}^2+S_{Y}^2+(\\bar{X}-\\bar{Y})^2} \\] \\[ \\\\[0.5cm] PICP = \\frac{1}{v} ~ count ~ j \\\\ j = PL^{L}_{j} \\leq t_j \\leq PL^{U}_{j} \\] 7.4.1 Preparation of the environment # 0 Environment setup ########################################################## # 0.1 Prepare environment ====================================================== # Folder check getwd() # Set folder direction setwd() # Clean up workspace rm(list = ls(all.names = TRUE)) # 0.2 Install packages ========================================================= install.packages(&quot;pacman&quot;) #Install and load the &quot;pacman&quot; package (allow easier download of packages) library(pacman) pacman::p_load(ggplot2, DescTools, caret, grid, gridExtra, raster, readr, dplyr,terra, quantregForest, Cubist, caretEnsemble) # 0.3 Show session infos ======================================================= sessionInfo() ## R version 4.4.0 (2024-04-24 ucrt) ## Platform: x86_64-w64-mingw32/x64 ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=French_France.utf8 LC_CTYPE=French_France.utf8 ## [3] LC_MONETARY=French_France.utf8 LC_NUMERIC=C ## [5] LC_TIME=French_France.utf8 ## ## time zone: Europe/Berlin ## tzcode source: internal ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] terra_1.7-83 raster_3.6-30 sp_2.1-4 ## [4] DescTools_0.99.57 dplyr_1.1.4 gridExtra_2.3 ## [7] caretEnsemble_4.0.1 Cubist_0.4.4 quantregForest_1.3-7.1 ## [10] RColorBrewer_1.1-3 randomForest_4.7-1.2 caret_6.0-94 ## [13] lattice_0.22-6 viridisLite_0.4.2 pacman_0.5.1 ## [16] stringr_1.5.1 readr_2.1.5 ggplot2_3.5.1 ## [19] bookdown_0.41 tufte_0.13 rmarkdown_2.29 ## [22] knitr_1.49 ## ## loaded via a namespace (and not attached): ## [1] mathjaxr_1.6-0 rstudioapi_0.17.1 jsonlite_1.8.9 ## [4] magrittr_2.0.3 farver_2.1.2 vctrs_0.6.5 ## [7] base64enc_0.1-3 htmltools_0.5.8.1 curl_5.2.3 ## [10] cellranger_1.1.0 pROC_1.18.5 sass_0.4.9 ## [13] parallelly_1.38.0 KernSmooth_2.23-22 bslib_0.8.0 ## [16] htmlwidgets_1.6.4 plyr_1.8.9 rootSolve_1.8.2.4 ## [19] lubridate_1.9.3 cachem_1.1.0 uuid_1.2-1 ## [22] lifecycle_1.0.4 iterators_1.0.14 pkgconfig_2.0.3 ## [25] Matrix_1.7-0 R6_2.5.1 fastmap_1.2.0 ## [28] future_1.34.0 Exact_3.3 digest_0.6.37 ## [31] colorspace_2.1-1 patchwork_1.3.0 leafem_0.2.3 ## [34] crosstalk_1.2.1 labeling_0.4.3 fansi_1.0.6 ## [37] timechange_0.3.0 httr_1.4.7 compiler_4.4.0 ## [40] proxy_0.4-27 bit64_4.5.2 withr_3.0.2 ## [43] brew_1.0-10 DBI_1.2.3 MASS_7.3-60.2 ## [46] lava_1.8.0 leaflet_2.2.2 classInt_0.4-10 ## [49] gld_2.6.6 ModelMetrics_1.2.2.2 tools_4.4.0 ## [52] units_0.8-5 future.apply_1.11.3 nnet_7.3-19 ## [55] glue_1.7.0 satellite_1.0.5 nlme_3.1-164 ## [58] sf_1.0-18 cluster_2.1.6 reshape2_1.4.4 ## [61] generics_0.1.3 recipes_1.1.0 gtable_0.3.6 ## [64] leaflet.providers_2.0.0 tzdb_0.4.0 class_7.3-22 ## [67] lmom_3.2 data.table_1.16.2 hms_1.1.3 ## [70] utf8_1.2.4 foreach_1.5.2 pillar_1.9.0 ## [73] vroom_1.6.5 splines_4.4.0 survival_3.5-8 ## [76] bit_4.5.0 tidyselect_1.2.1 svglite_2.1.3 ## [79] stats4_4.4.0 xfun_0.48 expm_1.0-0 ## [82] hardhat_1.4.0 leafpop_0.1.0 timeDate_4041.110 ## [85] stringi_1.8.4 boot_1.3-30 yaml_2.3.10 ## [88] evaluate_1.0.1 codetools_0.2-20 kernlab_0.9-33 ## [91] tcltk_4.4.0 tibble_3.2.1 cli_3.6.3 ## [94] rpart_4.1.23 systemfonts_1.1.0 munsell_0.5.1 ## [97] jquerylib_0.1.4 readxl_1.4.3 Rcpp_1.0.13 ## [100] globals_0.16.3 png_0.1-8 parallel_4.4.0 ## [103] gower_1.0.1 listenv_0.9.1 mvtnorm_1.3-1 ## [106] ipred_0.9-15 scales_1.3.0 prodlim_2024.06.25 ## [109] e1071_1.7-16 purrr_1.0.2 crayon_1.5.3 ## [112] rlang_1.1.4 # 06 Evaluation process of the models ########################################## # Here we decided to split every run by soil depth to have a better vision # on the running process. # 06.1 Preparation ============================================================= depth &lt;- &quot;0_10&quot; load(paste0(&quot;./export/save/Models_&quot;,depth,&quot;_DSM.RData&quot;)) Evaluation &lt;- First_depth_models # 06.2 Run the loop for predictions ============================================ model_preds &lt;- list() Q1.Q3 &lt;- list() for (i in 1:length(Evaluation$Models)) { model_preds[[i]] &lt;- list() X_test &lt;- Evaluation$Test_data[[i]][,1:length(Evaluation$Test_data[[i]])-1] for (j in 1:5) { model_preds[[i]][[j]]&lt;- as.vector(predict(Evaluation$Models[[i]][[j]], X_test)) } Q1.Q3[[i]] &lt;- predict(Evaluation$Models[[i]]$QRF$finalModel, X_test, what = c(0.05, 0.5, 0.95)) Ensemble.pred &lt;- as.data.frame(do.call(cbind,model_preds[[i]])) colnames(Ensemble.pred) &lt;- c(&quot;CART&quot;, &quot;Knn&quot;, &quot;SVM&quot;, &quot;Cubist&quot;, &quot;QRF&quot;) model_preds[[i]][[6]]&lt;- as.vector(predict(Evaluation$Models[[i]][[6]], Ensemble.pred)) } Metrics &lt;- list() for (i in 1:length(model_preds)) { empty_matrix &lt;- matrix(NA, nrow = 6, ncol = 5) Final_stats &lt;- data.frame(empty_matrix) colnames(Final_stats) &lt;- c(&quot;RMSE&quot;, &quot;R?&quot;, &quot;MAE&quot;, &quot;CCC&quot;, &quot;PICP&quot;) row.names(Final_stats) &lt;- c(&quot;CART&quot;, &quot;Knn&quot;, &quot;SVM&quot;, &quot;Cubist&quot;, &quot;QRF&quot;, &quot;Ensemble&quot;) for (j in 1:length(model_preds[[i]])) { RMSE &lt;- postResample(model_preds[[i]][[j]], Evaluation$Test_data[[i]][[length(Evaluation$Test_data[[i]])]]) ccc &lt;- CCC(model_preds[[i]][[j]], Evaluation$Test_data[[i]][[length(Evaluation$Test_data[[i]])]]) PICP &lt;- NA Final_stats[j,] &lt;- as.data.frame(t(c(RMSE, ccc$rho.c$est, PICP))) } PCIP &lt;- (Evaluation$Test_data[[i]][[length(Evaluation$Test_data[[i]])]] &gt;= Q1.Q3[[i]][,1]) &amp; (Evaluation$Test_data[[i]][[length(Evaluation$Test_data[[i]])]] &lt;= Q1.Q3[[i]][,3]) Final_stats[[5,5]] &lt;- mean(PCIP)*100 Metrics[[i]] &lt;- Final_stats } MetricsDF &lt;- data.frame(NaN) for (i in 1:length(Metrics)) { ErrorIndexCon &lt;- data.frame(Metrics[[i]]) colnames(ErrorIndexCon) &lt;- c(paste(&quot;RMSE&quot;,names(Evaluation$Cov[[i]][length(Evaluation$Cov[[i]])])), paste(&quot;R2&quot;,names(Evaluation$Cov[[i]][length(Evaluation$Cov[[i]])])), paste(&quot;MAE&quot;,names(Evaluation$Cov[[i]][length(Evaluation$Cov[[i]])])), paste(&quot;CCC&quot;,names(Evaluation$Cov[[i]][length(Evaluation$Cov[[i]])])), paste(&quot;PICP&quot;,names(Evaluation$Cov[[i]][length(Evaluation$Cov[[i]])]))) MetricsDF &lt;- cbind(MetricsDF ,ErrorIndexCon) rownames(MetricsDF) &lt;- rownames(Metrics[[1]]) } write.csv(data.frame(MetricsDF), paste0(&quot;./export/evaluation/&quot;, depth,&quot;/Models_metrics_for_&quot;,depth,&quot;_soil.csv&quot;)) # 06.3 Visualisation of the predictions ======================================== print(Metrics[[10]]) ## RMSE R? MAE CCC PICP ## CART 0.03472395 0.07971972 0.02501697 0.25913658 NA ## Knn 0.03475470 0.01461940 0.02418251 0.08792975 NA ## SVM 0.03408925 0.09920161 0.02452248 0.27886069 NA ## Cubist 0.03572083 0.09438268 0.02619801 0.28602770 NA ## QRF 0.03093176 0.17086620 0.02259714 0.36812960 79.16667 ## Ensemble 0.03541726 0.08890428 0.02416400 0.28003599 NA # Selection on the best model based on the metrics. Mainly lowest RMSE but also an appreciation of the R2 and CCC if a gap was visible Selected_model &lt;- data.frame(t(c(&quot;QRF&quot;,&quot;CART&quot;, &quot;CART&quot;, &quot;QRF&quot;, &quot;CART&quot;, &quot;QRF&quot;, &quot;Knn&quot;, &quot;QRF&quot;, &quot;QRF&quot;, &quot;QRF&quot;))) b &lt;- c(&quot;QRF&quot;,&quot;Cubist&quot;, &quot;QRF&quot;, &quot;QRF&quot;, &quot;QRF&quot;, &quot;Cubist&quot;, &quot;Ensemble&quot;, &quot;SVM&quot;, &quot;Knn&quot;, &quot;CART&quot;) c &lt;- c(&quot;QRF&quot;,&quot;Ensemble&quot;, &quot;Cubist&quot;, &quot;Ensemble&quot;, &quot;Knn&quot;, &quot;Cubist&quot;, &quot;Ensemble&quot;, &quot;SVM&quot;, &quot;QRF&quot;, &quot;Knn&quot;) d &lt;- c(&quot;QRF&quot;,&quot;SVM&quot;, &quot;QRF&quot;, &quot;SVM&quot;, &quot;QRF&quot;, &quot;Ensemble&quot;, &quot;Knn&quot;, &quot;SVM&quot;, &quot;QRF&quot;, &quot;Cubist&quot;) e &lt;- c(&quot;QRF&quot;,&quot;CART&quot;, &quot;Cubist&quot;, &quot;Ensemble&quot;, &quot;Cubist&quot;, &quot;SVM&quot;, &quot;QRF&quot;, &quot;QRF&quot;, &quot;QRF&quot;, &quot;Ensemble&quot;) Selected_model &lt;- rbind(Selected_model, b, c, d, e) colnames(Selected_model) &lt;- c(&quot;pH&quot;, &quot;CaCO3&quot;, &quot;Nt&quot;, &quot;Ct&quot;, &quot;Corg&quot;, &quot;EC&quot;, &quot;Sand&quot;, &quot;Silt&quot;, &quot;Clay&quot;, &quot;MWD&quot;) row.names(Selected_model) &lt;- c(&quot;0_10&quot;, &quot;10_30&quot;, &quot;30_50&quot;, &quot;50_70&quot;, &quot;70_100&quot;) print(Selected_model) ## pH CaCO3 Nt Ct Corg EC Sand Silt Clay MWD ## 0_10 QRF CART CART QRF CART QRF Knn QRF QRF QRF ## 10_30 QRF Cubist QRF QRF QRF Cubist Ensemble SVM Knn CART ## 30_50 QRF Ensemble Cubist Ensemble Knn Cubist Ensemble SVM QRF Knn ## 50_70 QRF SVM QRF SVM QRF Ensemble Knn SVM QRF Cubist ## 70_100 QRF CART Cubist Ensemble Cubist SVM QRF QRF QRF Ensemble write.table(Selected_model, &quot;./export/evaluation/Final_models_selected.txt&quot;, row.names = TRUE, col.names = TRUE, sep = &quot;;&quot;) Selected_model &lt;- read.delim(&quot;./export/evaluation/Final_models_selected.txt&quot;, sep = &quot;;&quot;) plot_graph &lt;- function(Obs, Pred, legend) { model &lt;- lm(Pred ~ Obs) min_value &lt;- min(c(Obs, Pred)) max_value &lt;- max(c(Obs, Pred)) ggplot(data.frame(Obs, Pred), aes(Obs, Pred)) + geom_point(color = &quot;blue&quot;) + coord_fixed() + scale_x_continuous(limits = c(min_value, max_value)) + scale_y_continuous(limits = c(min_value, max_value)) + geom_abline(aes(slope = 1, intercept = 0, color = &quot;myline1&quot;)) + geom_abline(aes(slope= model$coefficients[2],intercept = model$coefficients[1], color = &quot;myline2&quot;)) + scale_colour_manual(name=&#39;Lines&#39;,labels = c(&quot;1:1 Line&quot;, &quot;Linear Regression&quot;), values=c(myline1=&quot;black&quot;, myline2=&quot;red&quot;)) + ggtitle( paste(legend, &quot; Linear regression model at &quot;, depth, &quot; for &quot;, model_name, &quot; model&quot;)) + xlab(paste(&quot;Observed values from&quot;, legend)) + ylab(paste(&quot;Predicted values for&quot;, legend)) + theme_bw() + theme(plot.title = element_text(hjust = 0.5)) + annotate(&quot;text&quot;, x = -Inf, y = Inf, label = round(Metrics[[i]][model_name,2], digits = 4), hjust = -0.7, vjust = 1.5) + annotate(&quot;text&quot;, x = -Inf, y = Inf, label = &quot;R²&quot;, hjust = -0.5, vjust = 1.5) + annotate(&quot;text&quot;, x = -Inf, y = Inf, label = round(Metrics[[i]][model_name,1], digits = 4), hjust = -1.2, vjust = 3.5) + annotate(&quot;text&quot;, x = -Inf, y = Inf, label = &quot;RMSE&quot;, hjust = -0.2, vjust = 3.5) + annotate(&quot;text&quot;, x = -Inf, y = Inf, label = round(Metrics[[i]][model_name,2], digits = 4), hjust = -1, vjust = 5.5) + annotate(&quot;text&quot;, x = -Inf, y = Inf, label = &quot;MAE&quot;, hjust = -0.2, vjust = 5.5) + annotate(&quot;text&quot;, x = -Inf, y = Inf, label = round(Metrics[[i]][model_name,4], digits = 4), hjust = -1, vjust = 7.5) + annotate(&quot;text&quot;, x = -Inf, y = Inf, label = &quot;CCC&quot;, hjust = -0.2, vjust = 7.5) } regression_plot &lt;- list() for (i in 1:length(Evaluation$Models)) { names(model_preds[[i]]) &lt;- c(&quot;CART&quot;, &quot;Knn&quot;, &quot;SVM&quot;, &quot;Cubist&quot;, &quot;QRF&quot;, &quot;Ensemble&quot;) model_name &lt;- Selected_model[depth,i] Obs &lt;- model_preds[[i]][[model_name]] Pred &lt;- Evaluation$Test_data[[i]][[length(Evaluation$Test_data[[i]])]] legend &lt;- names(Evaluation$Cov[[i]][length(Evaluation$Cov[[i]])]) p &lt;- plot_graph(Obs, Pred, legend) regression_plot[[i]] &lt;- p } png(paste0(&quot;./export/evaluation/&quot;, depth,&quot;/Linear_regression_of_best_models_for_&quot;,depth,&quot;_soil.png&quot;), width = 1900, height = 1900) grid.arrange(do.call(arrangeGrob, c(regression_plot, nrow = 4, ncol = 3)), top = textGrob(paste0(&quot;Linear regression for best models at &quot;, depth , &quot; depth&quot;), gp = gpar(fontsize = 16, fontface = &quot;bold&quot;))) dev.off() pdf(paste0(&quot;./export/evaluation/&quot;, depth,&quot;/Linear_regression_of_best_models_for_&quot;,depth,&quot;_soil.pdf&quot;), width = 19, height = 19, bg = &quot;white&quot;, colormodel = &quot;cmyk&quot;) grid.arrange(do.call(arrangeGrob, c(regression_plot, nrow = 4, ncol = 3)), top = textGrob(paste0(&quot;Linear regression for best models at &quot;, depth , &quot; depth&quot;), gp = gpar(fontsize = 16, fontface = &quot;bold&quot;))) dev.off() # 06.4 Export results ========================================================== # Change the name of list regarding each soil depth: first, second, third, # fourth and fifth. #=============================================================================== First_depth_evaluation = list( Metrics = Metrics, Plots = regression_plot ) save(First_depth_evaluation, file = paste0(&quot;./export/save/Evaluation_&quot;,depth,&quot;_DSM.RData&quot;)) 7.5 Prediction of the area First we are normalising all the covariates raster values # 07 Prediction map for DSM #################################################### # 07.1 Normalise the values of the predictors ================================== rm(list = ls(all.names = TRUE)) depth_list &lt;- c(&quot;0_10&quot;, &quot;10_30&quot;, &quot;30_50&quot;, &quot;50_70&quot;, &quot;70_100&quot;) for (t in 1:length(depth_list)) { depth &lt;- depth_list[t] start_time &lt;- proc.time() raster_stack &lt;- stack(&quot;./data/Stack_layers_DSM.tif&quot;) cov &lt;- read.delim(paste0(&quot;./data/df_&quot;, depth,&quot;_cov_DSM.csv&quot;), sep = &quot;,&quot;) cov &lt;- cov[2:81] cov[] &lt;- lapply(cov , as.numeric) process_layer &lt;- function(layer) { # Convert in numeric layer &lt;- as.numeric(layer) # Replace the NAs by median median_value &lt;- median(layer, na.rm = TRUE) layer[is.na(layer)] &lt;- median_value return(layer) } scaling_params &lt;- lapply(1:ncol(cov), function(i) { list(min = min(cov[[i]], na.rm = TRUE), max = max(cov[[i]], na.rm = TRUE)) }) scale_layer &lt;- function(layer, min_val, max_val) { (layer - min_val) / (max_val - min_val) } stack_scaled &lt;- stack() # Apply transformation for (i in 1:nlayers(raster_stack)) { min_val &lt;- scaling_params[[i]]$min max_val &lt;- scaling_params[[i]]$max # Normalised the layer layer_processed &lt;- calc(raster_stack[[i]], process_layer) layer_scaled &lt;- calc(layer_processed, function(x) scale_layer(x, min_val, max_val)) stack_scaled &lt;- addLayer(stack_scaled, layer_scaled) cat(round((i/nlayers(raster_stack))*100, 1),&quot;% \\n&quot;) } stack_scaled &lt;- rast(stack_scaled) sum(is.na(values(stack_scaled))) names(stack_scaled) &lt;- names(raster_stack) terra::writeRaster(stack_scaled, paste0(&quot;./export/predictions_DSM/&quot;, depth, &quot;/Stack_raster_normalised_&quot;, depth, &quot;_DSM.tif&quot;), overwrite = TRUE) end_time &lt;- proc.time() print(paste0(depth[t], ((end_time[3] - start_time[3])/60)*t, &quot; /&quot;, ((end_time[3] - start_time[3])/60)*5)) } # 07.2 Prepare predictions ==================================================== # Here we decided to split every run by soil depth to have a better vision # on the running process. rm(list = ls(all.names = TRUE)) depth &lt;- &quot;0_10&quot; load(paste0(&quot;./export/save/Models_&quot;,depth,&quot;_DSM.RData&quot;)) raster_stack_normalised &lt;- stack(paste0(&quot;./export/predictions_DSM/&quot;, depth,&quot;/Stack_raster_normalised_&quot;,depth,&quot;_DSM.tif&quot;)) selected_model &lt;- read.delim(&quot;./export/evaluation/Final_models_selected.txt&quot;, sep =&quot;;&quot;) Prediction &lt;- First_depth_models Due to the highly detailed data we had to split the prediction into different blocks. This process allows the computer to run separately each prediction. If the selected model is an Ensemble learning, then all the other models are computed first. # 07.3 Prediction loop ======================================================== for (i in i:length(Prediction$Cov)){ variable &lt;- names(Prediction$Cov[[i]][length(Prediction$Cov[[i]])]) model &lt;- selected_model[depth,variable] if(model != &quot;Ensemble&quot;){ # Create an empty raster for storing predicted values predicted_raster &lt;- raster_stack_normalised[[1]] # Use the first layer as a template predicted_raster &lt;- writeStart(predicted_raster, paste0(&quot;./export/predictions_DSM/&quot;,depth,&quot;/DSM_&quot;,variable,&quot;_from_&quot;,model,&quot;_for_&quot;,depth,&quot;_soil.tif&quot;), overwrite = TRUE) # Subset the stack, keeping only the desired layers block_info &lt;- blockSize(raster_stack_normalised) names &lt;- names(Prediction$Cov[[i]][1:length(Prediction$Cov[[i]])-1]) raster_subset &lt;- subset(raster_stack_normalised, names) # Loop through each block of the raster stack for (g in 1:block_info$n) { start_time &lt;- proc.time() block &lt;- getValuesBlock(raster_subset , row = block_info$row[g], nrows = block_info$nrows[g]) block &lt;- as.data.frame(block) # Process the block predicted_block &lt;- predict(Prediction$Models[[i]][[model]], newdata = block) # Write the predicted block to the output raster predicted_raster &lt;- writeValues(predicted_raster, predicted_block, block_info$row[g]) end_time &lt;- proc.time() cat(round((g/block_info$n)*100, 1),&quot;% &quot;, ((end_time[3] - start_time[3])*g)/60, &quot; /&quot;, ((end_time[3] - start_time[3])*block_info$n/60), &quot; min \\n&quot;) } predicted_raster &lt;- writeStop(predicted_raster) print(variable) } else{ # Create a loop to produce every_models dir.create(paste0(&quot;./export/predictions_DSM/&quot;,depth,&quot;/Ensemble_&quot;,variable,&quot;/&quot;)) for (j in 1:5) { model &lt;- names(Prediction$Models[[i]][j]) # Create an empty raster for storing predicted values predicted_raster &lt;- raster_stack_normalised[[1]] # Use the first layer as a template predicted_raster &lt;- writeStart(predicted_raster, paste0(&quot;./export/predictions_DSM/&quot;,depth,&quot;/Ensemble_&quot;,variable,&quot;/&quot;,model,&quot;_DSM_of_&quot;,variable,&quot;_for_&quot;,depth,&quot;_soil.tif&quot;), overwrite = TRUE) # Loop through each block of the raster stack block_info &lt;- blockSize(raster_stack_normalised) names &lt;- names(Prediction$Cov[[i]][1:length(Prediction$Cov[[i]])-1]) raster_subset &lt;- subset(raster_stack_normalised, names) # Loop through each block of the raster stack for (g in 1:block_info$n) { start_time &lt;- proc.time() block &lt;- getValuesBlock(raster_subset , row = block_info$row[g], nrows = block_info$nrows[g]) block &lt;- as.data.frame(block) # Process the block (apply your model&#39;s predictions here) predicted_block &lt;- predict(Prediction$Models[[i]][[model]], newdata = block) # Write the predicted block to the output raster predicted_raster &lt;- writeValues(predicted_raster, predicted_block, block_info$row[g]) end_time &lt;- proc.time() cat(round((g/block_info$n)*100, 1),&quot;% &quot;, ((end_time[3] - start_time[3])*g)/60, &quot; /&quot;, ((end_time[3] - start_time[3])*block_info$n/60), &quot; min \\n&quot;) } predicted_raster &lt;- writeStop(predicted_raster) cat(variable, model, &quot;\\n&quot;) } model &lt;- &quot;Ensemble&quot; #Select the files all_files &lt;- list.files(paste0(&quot;./export/predictions_DSM/&quot;,depth,&quot;/Ensemble_&quot;,variable), full.names = TRUE) raster_pred_ensemble &lt;- stack(all_files) predicted_raster &lt;- raster_pred_ensemble[[1]] # Use the first layer as a template predicted_raster &lt;- writeStart(predicted_raster, paste0(&quot;./export/predictions_DSM/&quot;,depth,&quot;/DSM_&quot;,variable,&quot;_from_&quot;,model,&quot;_for_&quot;,depth,&quot;_soil.tif&quot;), overwrite = TRUE) block_info &lt;- blockSize(raster_pred_ensemble) # Loop through each block of the raster stack for (g in 1:block_info$n) { start_time &lt;- proc.time() # Read block of raster data block &lt;- getValuesBlock(raster_pred_ensemble, row = block_info$row[g], nrows = block_info$nrows[g]) block &lt;- as.data.frame(block) # Reorganise the order of the columns colnames(block) &lt;- c(&quot;CART&quot;, &quot;Cubist&quot;, &quot;Knn&quot;, &quot;QRF&quot;, &quot;SVM&quot;) block &lt;- block %&gt;% select(&quot;CART&quot;, &quot;Knn&quot;, &quot;SVM&quot;, &quot;Cubist&quot;, &quot;QRF&quot;) predicted_block &lt;- predict(Prediction$Models[[i]][[model]], newdata = block) # Write the predicted block to the output raster predicted_raster &lt;- writeValues(predicted_raster, predicted_block, block_info$row[g]) end_time &lt;- proc.time() cat(round((g/block_info$n)*100, 1),&quot;% &quot;, ((end_time[3] - start_time[3])*g)/60, &quot; /&quot;, ((end_time[3] - start_time[3])*block_info$n/60), &quot; min \\n&quot;) } predicted_raster &lt;- writeStop(predicted_raster) cat(variable, model, &quot;\\n&quot;) } } The uncertainty of the model is based on the following equation from Yan et al. (2018). \\[ \\\\[0.5cm] Uncertainty = \\frac{Qp ~ 0.95- Qp ~ 0.05}{Qp ~ 0.5} \\] # 07.4 Prediction of uncertainty =============================================== for (i in i:length(Prediction$Cov)){ variable &lt;- names(Prediction$Cov[[i]][length(Prediction$Cov[[i]])]) model &lt;- selected_model[depth,variable] uncertainty_raster &lt;- raster_stack_normalised[[1]] uncertainty_raster &lt;- writeStart(uncertainty_raster, paste0(&quot;./export/uncertainy_DSM/&quot;,depth,&quot;/Uncertainty_QRF_&quot;,variable,&quot;_for_&quot;,depth,&quot;_soil.tif&quot;), overwrite = TRUE) # Subset the stack, keeping only the desired layers block_info &lt;- blockSize(raster_stack_normalised) names &lt;- names(Prediction$Cov[[i]][1:length(Prediction$Cov[[i]])-1]) raster_subset &lt;- subset(raster_stack_normalised, names) # Loop through each block of the raster stack for (g in 1:block_info$n) { start_time &lt;- proc.time() block &lt;- getValuesBlock(raster_subset , row = block_info$row[g], nrows = block_info$nrows[g]) block &lt;- as.data.frame(block) # Process the block predicted_block &lt;- predict(Prediction$Models[[i]]$QRF$finalModel, newdata = block, what = c(0.05, 0.5, 0.95)) # Write the predicted block to the output raster values &lt;- (predicted_block[,3] - predicted_block[,1]) /predicted_block[,2] uncertainty_raster &lt;- writeValues(uncertainty_raster, values, block_info$row[g]) end_time &lt;- proc.time() cat(round((g/block_info$n)*100, 1),&quot;% &quot;, ((end_time[3] - start_time[3])*g)/60, &quot; /&quot;, ((end_time[3] - start_time[3])*block_info$n/60), &quot; min \\n&quot;) } uncertainty_raster&lt;- writeStop(uncertainty_raster) print(variable) } 7.6 Visualisation and comparison with SoilGrid product 7.6.1 Preparation of the environment # 0.1 Prepare environment ====================================================== # Folder check getwd() # Set folder direction setwd(dir=&quot;./depth&quot;) # Clean up workspace rm(list = ls(all.names = TRUE)) # 0.2 Install packages ========================================================= install.packages(&quot;pacman&quot;) #Install and load the &quot;pacman&quot; package (allow easier download of packages) library(pacman) pacman::p_load(raster, terra, sf, sp, viridis, ggplot2, remotes, RColorBrewer, wesanderson, grid, gridExtra, colorspace, mapview, biscale, ncdf4,SpaDES, ggspatial,soiltexture, cowplot, hrbrthemes) devtools::install_github(&quot;ducciorocchini/cblindplot&quot;) library(cblindplot) # 0.3 Show session infos ======================================================= sessionInfo() ## R version 4.4.0 (2024-04-24 ucrt) ## Platform: x86_64-w64-mingw32/x64 ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=French_France.utf8 LC_CTYPE=French_France.utf8 ## [3] LC_MONETARY=French_France.utf8 LC_NUMERIC=C ## [5] LC_TIME=French_France.utf8 ## ## time zone: Europe/Berlin ## tzcode source: internal ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] hrbrthemes_0.8.7 cowplot_1.1.3 soiltexture_1.5.3 ## [4] ggspatial_1.1.9 SpaDES.tools_2.0.7 SpaDES.core_2.1.0 ## [7] quickPlot_1.0.2 reproducible_2.1.0 SpaDES_2.0.11 ## [10] ncdf4_1.23 biscale_1.0.0 mapview_2.11.2 ## [13] colorspace_2.1-1 wesanderson_0.3.7 remotes_2.5.0 ## [16] viridis_0.6.5 sf_1.0-18 terra_1.7-83 ## [19] raster_3.6-30 sp_2.1-4 gridExtra_2.3 ## [22] RColorBrewer_1.1-3 randomForest_4.7-1.2 lattice_0.22-6 ## [25] viridisLite_0.4.2 pacman_0.5.1 stringr_1.5.1 ## [28] ggplot2_3.5.1 bookdown_0.41 tufte_0.13 ## [31] rmarkdown_2.29 knitr_1.49 ## ## loaded via a namespace (and not attached): ## [1] splines_4.4.0 tibble_3.2.1 cellranger_1.1.0 ## [4] hardhat_1.4.0 pROC_1.18.5 rpart_4.1.23 ## [7] lifecycle_1.0.4 tcltk_4.4.0 globals_0.16.3 ## [10] vroom_1.6.5 MASS_7.3-60.2 crosstalk_1.2.1 ## [13] backports_1.5.0 magrittr_2.0.3 sass_0.4.9 ## [16] jquerylib_0.1.4 yaml_2.3.10 lobstr_1.1.2 ## [19] gld_2.6.6 DBI_1.2.3 lubridate_1.9.3 ## [22] expm_1.0-0 purrr_1.0.2 nnet_7.3-19 ## [25] ipred_0.9-15 gdtools_0.4.0 satellite_1.0.5 ## [28] lava_1.8.0 listenv_0.9.1 units_0.8-5 ## [31] brew_1.0-10 parallelly_1.38.0 svglite_2.1.3 ## [34] codetools_0.2-20 RApiSerialize_0.1.4 Require_1.0.1 ## [37] tidyselect_1.2.1 farver_2.1.2 stats4_4.4.0 ## [40] base64enc_0.1-3 mathjaxr_1.6-0 jsonlite_1.8.9 ## [43] caret_6.0-94 e1071_1.7-16 survival_3.5-8 ## [46] iterators_1.0.14 systemfonts_1.1.0 foreach_1.5.2 ## [49] tools_4.4.0 DescTools_0.99.57 Rcpp_1.0.13 ## [52] glue_1.7.0 Rttf2pt1_1.3.12 prodlim_2024.06.25 ## [55] xfun_0.48 leaflet.providers_2.0.0 dplyr_1.1.4 ## [58] withr_3.0.2 fastmap_1.2.0 boot_1.3-30 ## [61] fansi_1.0.6 digest_0.6.37 timechange_0.3.0 ## [64] R6_2.5.1 utf8_1.2.4 generics_0.1.3 ## [67] fontLiberation_0.1.0 data.table_1.16.2 recipes_1.1.0 ## [70] class_7.3-22 httr_1.4.7 htmlwidgets_1.6.4 ## [73] whisker_0.4.1 ModelMetrics_1.2.2.2 pkgconfig_2.0.3 ## [76] gtable_0.3.6 Exact_3.3 timeDate_4041.110 ## [79] sys_3.4.3 htmltools_0.5.8.1 fontBitstreamVera_0.1.1 ## [82] scales_1.3.0 lmom_3.2 png_0.1-8 ## [85] gower_1.0.1 rstudioapi_0.17.1 tzdb_0.4.0 ## [88] reshape2_1.4.4 uuid_1.2-1 checkmate_2.3.2 ## [91] nlme_3.1-164 curl_5.2.3 proxy_0.4-27 ## [94] cachem_1.1.0 rootSolve_1.8.2.4 KernSmooth_2.23-22 ## [97] parallel_4.4.0 extrafont_0.19 pillar_1.9.0 ## [100] vctrs_0.6.5 stringfish_0.16.0 cluster_2.1.6 ## [103] extrafontdb_1.0 evaluate_1.0.1 mvtnorm_1.3-1 ## [106] cli_3.6.3 compiler_4.4.0 rlang_1.1.4 ## [109] crayon_1.5.3 leafpop_0.1.0 future.apply_1.11.3 ## [112] labeling_0.4.3 classInt_0.4-10 plyr_1.8.9 ## [115] fs_1.6.4 stringi_1.8.4 fpCompare_0.2.4 ## [118] munsell_0.5.1 leaflet_2.2.2 fontquiver_0.2.1 ## [121] Matrix_1.7-0 hms_1.1.3 patchwork_1.3.0 ## [124] bit64_4.5.2 leafem_0.2.3 future_1.34.0 ## [127] kernlab_0.9-33 qs_0.27.2 igraph_2.1.1 ## [130] RcppParallel_5.1.9 bslib_0.8.0 bit_4.5.0 ## [133] readxl_1.4.3 7.6.2 Visualisation of the final products Visualisations of the final products can also be reached on Shiny app website at https://mathias-bellat.shinyapps.io/Northern-Kurdistan-map/. # 08 Prepare visualisations ################################################### # Here we decided to split every run by soil depth to have a better vision # on the running process. # 08.1 Prepare data ============================================================ depth &lt;- &quot;0_10&quot; layers &lt;- list.files(paste0(&quot;./export/predictions_DSM/&quot;, depth,&quot;/&quot;), pattern = &quot;*tif&quot; , full.names = TRUE) layers &lt;- layers[1:10] # Remove the normalised stack raster_stack &lt;- stack(layers) raster_stack &lt;- raster_stack[[c(8,1,7,4,3,5,9,10,2,6)]] # Re_order the raster position survey &lt;- st_read(&quot;./data/Survey_Area.gpkg&quot;, layer = &quot;Survey_Area&quot;) # 08.2 Normalise the texture band on 100% ====================================== tiles.sand &lt;- splitRaster(raster_stack[[7]], nx = 5, ny = 5) tiles.silt &lt;- splitRaster(raster_stack[[8]], nx = 5, ny = 5) tiles.clay &lt;- splitRaster(raster_stack[[9]], nx = 5, ny = 5) results &lt;- list() for (i in 1:length(tiles.sand)) { x &lt;- stack(tiles.sand[[i]],tiles.silt[[i]], tiles.clay[[i]]) x_df &lt;- raster::as.data.frame(x, xy = TRUE) texture_df &lt;- x_df[,c(3:5)] colnames(texture_df) &lt;- c(&quot;SAND&quot;,&quot;SILT&quot;, &quot;CLAY&quot;) texture_df &lt;- TT.normalise.sum(texture_df, css.names = c(&quot;SAND&quot;,&quot;SILT&quot;, &quot;CLAY&quot;)) x_df &lt;- cbind(x_df[,c(1:2)], texture_df) x_normalise &lt;- rasterFromXYZ(x_df) results[[i]] &lt;- x_normalise print(paste0(i, &quot; / &quot;, length(tiles.silt))) } raster_final &lt;- do.call(merge, results) crs(raster_final) &lt;- &quot;EPSG:32638&quot; raster_stack &lt;- stack(raster_stack[[1:6]], raster_stack[[10]]) raster_stack &lt;- stack(raster_stack, raster_final) raster_stack &lt;- raster_stack[[c(1:6,8:10,7)]] names(raster_stack) &lt;- c(&quot;pH&quot;, &quot;CaCO3&quot;, &quot;Nt&quot;, &quot;Ct&quot;, &quot;Corg&quot;, &quot;EC&quot;, &quot;Sand&quot;, &quot;Silt&quot;, &quot;Clay&quot;, &quot;MWD&quot;) # 08.3 Export final maps ======================================================= # For GeoTiff format crs(raster_stack) &lt;- &quot;EPSG:32638&quot; x_croped &lt;- crop(raster_stack, survey) x_masked &lt;- mask(x_croped, survey) x_repro &lt;- projectRaster(x_masked, crs = &quot;EPSG:4326&quot;) x &lt;- rast(x_repro) writeRaster(x, paste0(&quot;./export/final_maps/&quot;, depth, &quot;_prediction_map.tif&quot;), overwrite=TRUE) #By default already GeoTiff # For netCDF format CDF_df &lt;- lapply(1:nlayers(x_repro), function(i) { rast(x_repro[[i]]) # Convertir chaque couche en SpatRaster }) names(CDF_df) &lt;- c(&quot;pH&quot;, &quot;CaCO3&quot;, &quot;Nt&quot;, &quot;Ct&quot;, &quot;Corg&quot;, &quot;EC&quot;, &quot;Sand&quot;, &quot;Silt&quot;, &quot;Clay&quot;, &quot;MWD&quot;) soil_to_netcdf &lt;- function(soil_list, output_file, overwrite = FALSE) { # Check if file exists and handle overwrite if (file.exists(output_file) &amp;&amp; !overwrite) { stop(&quot;File already exists and overwrite = FALSE&quot;) } # If file exists and overwrite is TRUE, remove the existing file if (file.exists(output_file) &amp;&amp; overwrite) { file.remove(output_file) } # Get dimensions and CRS from first raster r &lt;- soil_list[[1]] nx &lt;- ncol(r) ny &lt;- nrow(r) crs_string &lt;- crs(r) # Create longitude and latitude vectors ext &lt;- ext(r) lon &lt;- seq(from = ext[1], to = ext[2], length.out = nx) lat &lt;- seq(from = ext[3], to = ext[4], length.out = ny) # Changed back to ascending order # Define dimensions londim &lt;- ncdim_def(&quot;longitude&quot;, &quot;degrees_east&quot;, lon) latdim &lt;- ncdim_def(&quot;latitude&quot;, &quot;degrees_north&quot;, lat) # Define units for each soil property units_list &lt;- list( pH = &quot;pH units&quot;, CaCO3 = &quot;%&quot;, Nt = &quot;%&quot;, Ct = &quot;%&quot;, Corg = &quot;%&quot;, EC = &quot;\\U00B5S/m&quot;, Sand = &quot;%&quot;, Silt = &quot;%&quot;, Clay = &quot;%&quot;, MWD = &quot;mm&quot; ) # Create list of variables with appropriate units var_list &lt;- list() for (var_name in names(soil_list)) { var_list[[var_name]] &lt;- ncvar_def( name = var_name, units = units_list[[var_name]], dim = list(londim, latdim), missval = NA ) } # Create netCDF file ncout &lt;- nc_create(output_file, var_list) # Write data for (var_name in names(soil_list)) { # Convert SpatRaster to matrix and handle orientation values_matrix &lt;- t(as.matrix(soil_list[[var_name]], wide=TRUE)) values_matrix &lt;- values_matrix[,ncol(values_matrix):1] ncvar_put(ncout, var_list[[var_name]], vals = values_matrix) } # Add global attributes ncatt_put(ncout, 0, &quot;title&quot;, &quot;Soil properties for 70 - 100 cm depth &quot;) ncatt_put(ncout,0,&quot;institution&quot;,&quot;Tuebingen University, CRC1070 ResourceCultures&quot;) ncatt_put(ncout, 0, &quot;description&quot;, &quot;Soil physicochemical properties in the Northern Kurdsitan region of Irak at 70 - 100 cm depth increment&quot;) ncatt_put(ncout,0,&quot;author&quot;, &quot;Mathias Bellat PhD. candidate at Tuebingen University (mathias.bellat@uni-tuebingen.de)&quot;) ncatt_put(ncout, 0, &quot;creation_date&quot;, format(Sys.time(), &quot;%Y-%m-%d %H:%M:%S&quot;)) # Add CRS information if (!is.na(crs_string)) { ncatt_put(ncout, 0, &quot;crs&quot;, crs_string) ncatt_put(ncout, 0, &quot;spatial_ref&quot;, crs_string) # Add standard CF grid mapping attributes ncatt_put(ncout, 0, &quot;Conventions&quot;, &quot;CF-1.6&quot;) ncatt_put(ncout, &quot;longitude&quot;, &quot;standard_name&quot;, &quot;longitude&quot;) ncatt_put(ncout, &quot;longitude&quot;, &quot;axis&quot;, &quot;X&quot;) ncatt_put(ncout, &quot;latitude&quot;, &quot;standard_name&quot;, &quot;latitude&quot;) ncatt_put(ncout, &quot;latitude&quot;, &quot;axis&quot;, &quot;Y&quot;) } # Add variable descriptions var_descriptions &lt;- list( pH = &quot;Soil pH (Kcl)&quot;, CaCO3 = &quot;Calcium carbonate content&quot;, Nt = &quot;Total nitrogen content&quot;, Ct = &quot;Total carbon content&quot;, Corg = &quot;Organic carbon content&quot;, EC = &quot;Electrical conductivity&quot;, Sand = &quot;Sand content&quot;, Silt = &quot;Silt content&quot;, Clay = &quot;Clay content&quot;, MWD = &quot;Mean weight diameter&quot; ) # Add variable-specific attributes for (var_name in names(soil_list)) { ncatt_put(ncout, var_list[[var_name]], &quot;long_name&quot;, var_descriptions[[var_name]]) } # Close the file nc_close(ncout) } soil_to_netcdf(CDF_df, paste0(&quot;./export/final_maps/&quot;, depth,&quot;_prediction_map.nc&quot;), overwrite = TRUE) # 08.4 Visualise for colorblind =============================================== raster_resize &lt;- aggregate(raster_stack, fact=5, fun=mean) wes_palette_hcl &lt;- function(palette_name, n = 7) { wes_colors &lt;- wes_palette(palette_name, n = n, type = &quot;continuous&quot;) hex_colors &lt;- as.character(wes_colors) return(hex_colors) } palette_wes &lt;- wes_palette_hcl(&quot;Zissou1&quot;, n = 7) color_blind_graph &lt;- list() for (i in 1:nlayers(raster_resize)) { gg1 &lt;- cblind.plot(raster_resize[[i]], cvd = palette_wes) gg2 &lt;- cblind.plot(raster_resize[[i]], cvd = &quot;deuteranopia&quot;) gg3 &lt;- cblind.plot(raster_resize[[i]], cvd = &quot;tritanopia&quot;) gg4 &lt;- cblind.plot(raster_resize[[i]], cvd = &quot;protanopia&quot;) plots_with_labels &lt;- arrangeGrob( arrangeGrob(gg1, bottom = textGrob(&quot;Original&quot;, gp = gpar(fontsize = 12))), arrangeGrob(gg2, bottom = textGrob(&quot;Deuteranopia&quot;, gp = gpar(fontsize = 12))), arrangeGrob(gg3, bottom = textGrob(&quot;Tritanopia&quot;, gp = gpar(fontsize = 12))), arrangeGrob(gg4, bottom = textGrob(&quot;Protanopia&quot;, gp = gpar(fontsize = 12))), nrow = 2, ncol = 2 ) color_blind_graph[[i]] &lt;- grid.arrange(plots_with_labels, top = textGrob( paste0(&quot;Predictions maps of &quot;,names(raster_stack[[i]]) ,&quot; at &quot;, depth , &quot; depth&quot;), gp = gpar(fontsize = 16, fontface = &quot;bold&quot;))) png(paste0(&quot;./export/visualisations/&quot;, depth,&quot;/Prediction_map_&quot;,names(raster_resize[[i]]), &quot;_at_&quot;,depth,&quot;_depth.png&quot;), # File name width = 1500, height = 1300) grid.arrange(plots_with_labels, top = textGrob( paste0(&quot;Predictions maps of &quot;,names(raster_stack[[i]]) ,&quot; at &quot;, depth , &quot; depth&quot;), gp = gpar(fontsize = 16, fontface = &quot;bold&quot;))) dev.off() pdf(paste0(&quot;./export/visualisations/&quot;, depth,&quot;/Prediction_map_&quot;,names(raster_resize[[i]]), &quot;_at_&quot;,depth,&quot;_depth.pdf&quot;), # File name width = 15, height = 13, bg = &quot;white&quot;, colormodel = &quot;cmyk&quot;) grid.arrange(plots_with_labels, top = textGrob( paste0(&quot;Predictions maps of &quot;,names(raster_stack[[i]]) ,&quot; at &quot;, depth , &quot; depth&quot;), gp = gpar(fontsize = 16, fontface = &quot;bold&quot;))) dev.off() } Colorblind visulation for pH at 0 - 10 cm deth # 08.5 Combined plots of each variables ======================================== raster_resize_croped &lt;- crop(raster_resize, survey) raster_resize_masked &lt;- mask(raster_resize_croped, survey) rasterdf &lt;- raster::as.data.frame(raster_resize_masked, xy = TRUE) rasterdf &lt;- rasterdf[complete.cases(rasterdf),] legend &lt;- c(&quot;pH [KCl]&quot;, &quot;CaCO3 [%]&quot;, &quot;Nt [%]&quot; , &quot;Ct [%]&quot;, &quot;Corg [%]&quot;, &quot;EC [µS/cm]&quot;, &quot;Sand [%]&quot;, &quot;Silt [%]&quot;, &quot;Clay [%]&quot;, &quot;MWD [mm]&quot;) bounds &lt;- st_bbox(survey) xlim_new &lt;- c(bounds[&quot;xmin&quot;] - 3000, bounds[&quot;xmax&quot;] + 3000) ylim_new &lt;- c(bounds[&quot;ymin&quot;] - 3000, bounds[&quot;ymax&quot;] + 3000) generate_raster_plot &lt;- function(rasterdf, value, depth, legend) { palette_wes &lt;- wes_palette(&quot;Zissou1&quot;, type = &quot;continuous&quot;) t &lt;- value +2 ggplot() + geom_raster(data = rasterdf, aes(x = x, y = y,fill = rasterdf[[t]] )) + ggtitle(paste0(&quot;Prediction map of &quot;, names(rasterdf[t]) , &quot; for &quot;,depth , &quot; cm soil depth&quot;)) + scale_fill_gradientn(colors = palette_wes, name = legend) + annotation_scale( location = &quot;br&quot;, width_hint = 0.3, height = unit(0.3, &quot;cm&quot;), line_col = &quot;black&quot;, text_col = &quot;black&quot;, bar_cols = c(&quot;white&quot;, &quot;red&quot;) ) + annotate(&quot;text&quot;, label = paste(&quot;Projection: WGS84 UTM 38N&quot;), x = Inf, y = -Inf, hjust = 1.5, vjust = -3, size = 3, color = &quot;black&quot;) + annotation_north_arrow(location = &quot;tr&quot;, which_north = &quot;true&quot;, height = unit(1, &quot;cm&quot;), width = unit(0.75, &quot;cm&quot;)) + theme_bw() + theme( panel.grid = element_blank(), axis.text.y = element_text(angle = 90, vjust = 0.5, hjust = 0.5), axis.title = element_blank(), legend.position = &quot;right&quot;, plot.margin = margin(0, 0, 0, 0) )+ coord_equal(ratio = 1) } stack_graph &lt;- list() for (i in 1:nlayers(raster_resize_croped)) { stack_graph[[i]] &lt;- generate_raster_plot(rasterdf, i, depth, legend[i]) } png(paste0(&quot;./export/visualisations/&quot;, depth,&quot;/Prediction_maps_at_&quot;,depth,&quot;_depth.png&quot;), # File name width = 2000, height = 1700) grid.arrange(grobs = stack_graph, ncol = 3, top = textGrob(paste0(&quot;Prediction maps at &quot;, depth , &quot; cm depth&quot;), gp = gpar(fontsize = 12, fontface = &quot;bold&quot;))) dev.off() pdf(paste0(&quot;./export/visualisations/&quot;, depth,&quot;/Prediction_maps_at_&quot;,depth,&quot;_depth.pdf&quot;), width = 35, height = 30, bg = &quot;white&quot;, colormodel = &quot;cmyk&quot;) grid.arrange(grobs = stack_graph, ncol = 3, top = textGrob(paste0(&quot;Prediction maps at &quot;, depth , &quot; cm depth&quot;), gp = gpar(fontsize = 12, fontface = &quot;bold&quot;))) dev.off() See the annexes of the publication for the simplified maps. # 08.6 Simplified version for publication ======================================= generate_clean_raster_plot &lt;- function(rasterdf, i, legend, depth) { t &lt;- i + 2 p &lt;- ggplot() + geom_raster(data = rasterdf, aes(x = x, y = y, fill = rasterdf[[t]])) + ggtitle(paste0(&quot;Prediction map of &quot;, colnames(rasterdf)[t])) + scale_fill_gradientn(colors = palette_wes, name = legend) + theme_void() + theme( legend.position = &quot;right&quot;, plot.title = element_text(size = 8), legend.title = element_text(size = 6), legend.text = element_text(size = 6), legend.key.size = unit(0.3, &quot;cm&quot;), plot.margin = margin(0, 0, 0, 0) ) + coord_equal(ratio = 1) return(p) } light_graph &lt;- list() for (i in 1:nlayers(raster_resize_croped)) { light_graph[[i]] &lt;- generate_clean_raster_plot(rasterdf, i, legend[i], depth) } png(paste0(&quot;./export/visualisations/&quot;, depth,&quot;/Prediction_maps_publication_at_&quot;,depth,&quot;_depth.png&quot;), # File name width = 1000, height = 800) grid.arrange(grobs = light_graph, ncol = 3, top = textGrob(paste0(&quot;Prediction maps at &quot;, depth , &quot; cm depth&quot;), gp = gpar(fontsize = 10, fontface = &quot;bold&quot;))) dev.off() pdf(paste0(&quot;./export/visualisations/&quot;, depth,&quot;/Prediction_maps_publication_at_&quot;,depth,&quot;_depth.pdf&quot;), width = 13, height = 10, bg = &quot;white&quot;, colormodel = &quot;cmyk&quot;) grid.arrange(grobs = light_graph, ncol = 3, top = textGrob(paste0(&quot;Prediction maps at &quot;, depth , &quot; cm depth&quot;), gp = gpar(fontsize = 10, fontface = &quot;bold&quot;))) dev.off() 7.6.3 SoilGrid evaluation We choose to compute top, sub and lower soil to evaluate the SoilGrid product with our predictions. Our interval were based on top-soil with 0 - 30 cm; sub-soil with 30 - 70 for our own prediction and 30 - 60 cm for the SoilGrid; lower-soil with 70 - 100 cm for our own prediction and 60 - 100 for SoilGrid product. We had to standardised the values of our prediction to compare them with the SoilGrid product (Committee 2015). # 09 Evaluation with SoilGrid ################################################# # 09.1 Prepare data ============================================================ rm(list = ls(all.names = TRUE)) # For 0-5 cm increment files &lt;- list.files(&quot;./data/Soil_grid/Values/0_5/&quot;, pattern = &quot;*tif&quot;, full.names = TRUE) SoilGrid.zero &lt;- stack(files) SoilGrid.zero &lt;- projectRaster(SoilGrid.zero, crs = &quot;EPSG:32638&quot;) # For 5-15 cm increment files &lt;- list.files(&quot;./data/Soil_grid/Values/5_15/&quot;, pattern = &quot;*tif&quot;, full.names = TRUE) SoilGrid.five &lt;- stack(files) SoilGrid.five &lt;- projectRaster(SoilGrid.five, crs = &quot;EPSG:32638&quot;) # For 15-30 cm increment files &lt;- list.files(&quot;./data/Soil_grid/Values/15_30/&quot;, pattern = &quot;*tif&quot;, full.names = TRUE) SoilGrid.fifteen &lt;- stack(files) SoilGrid.fifteen &lt;- projectRaster(SoilGrid.fifteen, crs = &quot;EPSG:32638&quot;) # For 30-60 cm increment files &lt;- list.files(&quot;./data/Soil_grid/Values/30_60/&quot;, pattern = &quot;*tif&quot;, full.names = TRUE) SoilGrid.thirty &lt;- stack(files) SoilGrid.thirty &lt;- projectRaster(SoilGrid.thirty, crs = &quot;EPSG:32638&quot;) # For 60-100 cm increment files &lt;- list.files(&quot;./data/Soil_grid/Values/60_100/&quot;, pattern = &quot;*tif&quot;, full.names = TRUE) SoilGrid.sixty &lt;- stack(files) SoilGrid.sixty &lt;- projectRaster(SoilGrid.sixty, crs = &quot;EPSG:32638&quot;) # 09.2 Top soil preparation ==================================================== Prediction.zero &lt;- stack(&quot;./export/final_maps/0_10_prediction_map.tif&quot;) Prediction.zero&lt;- projectRaster(Prediction.zero, crs = &quot;EPSG:32638&quot;) Prediction.ten &lt;- stack(&quot;./export/final_maps/10_30_prediction_map.tif&quot;) Prediction.ten &lt;- projectRaster(Prediction.ten, crs = &quot;EPSG:32638&quot;) # Resize and resample SoilGrid.zero_crop &lt;- crop(SoilGrid.zero, Prediction.zero$pH) SoilGrid.five_crop &lt;- crop(SoilGrid.five, Prediction.zero$pH) SoilGrid.fifteen_crop &lt;- crop(SoilGrid.fifteen, Prediction.zero$pH) Prediction.zero_resample &lt;- resample(Prediction.zero, SoilGrid.zero_crop, method = &quot;bilinear&quot;) Prediction.ten_resample &lt;- resample(Prediction.ten, SoilGrid.zero_crop, method = &quot;bilinear&quot;) # Convert into DF Prediction.zero_df &lt;- raster::as.data.frame(Prediction.zero_resample, xy = TRUE) Prediction.zero_df &lt;- Prediction.zero_df[complete.cases(Prediction.zero_df),] Prediction.ten_df &lt;- raster::as.data.frame(Prediction.ten_resample, xy = TRUE) Prediction.ten_df &lt;- Prediction.ten_df[complete.cases(Prediction.ten_df),] SoilGrid.zero_df &lt;- raster::as.data.frame(SoilGrid.zero_crop, xy = TRUE) SoilGrid.zero_df &lt;- SoilGrid.zero_df[complete.cases(SoilGrid.zero_df),] SoilGrid.five_df &lt;- raster::as.data.frame(SoilGrid.five_crop, xy = TRUE) SoilGrid.five_df &lt;- SoilGrid.five_df[complete.cases(SoilGrid.five_df),] SoilGrid.fifteen_df &lt;- raster::as.data.frame(SoilGrid.fifteen_crop, xy = TRUE) SoilGrid.fifteen_df&lt;- SoilGrid.fifteen_df[complete.cases(SoilGrid.fifteen_df),] SoilGrid_top_soil &lt;- SoilGrid.zero_df for (i in 3: length(SoilGrid_top_soil)) { SoilGrid_top_soil[i] &lt;- ((SoilGrid.zero_df[i]*5) + (SoilGrid.five_df[i]*10) + (SoilGrid.fifteen_df[i]*15))/30 } # Convert to % values and reduce the pH by 10 to fit our values colnames(SoilGrid_top_soil) &lt;- c(&quot;x&quot;, &quot;y&quot;, &quot;SoilGrid.Clay&quot;, &quot;SoilGrid.Corg&quot;, &quot;SoilGrid.Nt&quot;, &quot;SoilGrid.pH&quot;, &quot;SoilGrid.Sand&quot;, &quot;SoilGrid.Silt&quot;) SoilGrid_top_soil[,c(3,6:8)] &lt;- SoilGrid_top_soil[,c(3,6:8)]/10 # pH standardisation Aitken and Moody (1991) R2 = 0.78 SoilGrid_top_soil[6] &lt;- (1.28 * SoilGrid_top_soil[6]) - 0.613 SoilGrid_top_soil[4] &lt;- SoilGrid_top_soil[4]/1000 SoilGrid_top_soil[5] &lt;- SoilGrid_top_soil[5]/10000 # Replace zero value and values under or over the SD from the SoilGrid for (i in 3:8) { SoilGrid_top_soil[[i]][SoilGrid_top_soil[[i]] == 0] &lt;- median(SoilGrid_top_soil[[i]]) } sum(SoilGrid_top_soil == 0) summary(SoilGrid_top_soil[3:8]) replace_sd &lt;- function(x) { mean_val &lt;- mean(x, na.rm = TRUE) sd_val &lt;- sd(x, na.rm = TRUE) x &lt;- ifelse(x &gt; (mean_val + sd_val), (mean_val + sd_val), ifelse(x &lt; (mean_val - sd_val), (mean_val - sd_val), x)) return(x) } SoilGrid_top_soil[, 3:8] &lt;- as.data.frame(lapply(SoilGrid_top_soil[, 3:8], replace_sd)) Prediction_top_soil &lt;- Prediction.zero_df for (i in 3:length(Prediction.zero_df)) { Prediction_top_soil[i] &lt;- ((Prediction.zero_df[i]*10) + (Prediction.ten_df[i]*20))/30 } # pH standardisation Aitken and Moody (1991) R2 = 0.8 Prediction_top_soil[3] &lt;- (1.175*Prediction_top_soil[3]) - 0.262 # Texture standardisation Minasny and McBratney (2001) 0.063 to 0.05 Texture &lt;- Prediction_top_soil[,c(9:11)] colnames(Texture) &lt;- c(&quot;SAND&quot;,&quot;SILT&quot;, &quot;CLAY&quot;) Texture &lt;- TT.normalise.sum(Texture, css.names = c(&quot;SAND&quot;,&quot;SILT&quot;, &quot;CLAY&quot;)) Texture &lt;- TT.text.transf( tri.data = Texture, dat.css.ps.lim = c(0, 0.002, 0.063, 2), # German system base.css.ps.lim = c(0, 0.002, 0.05, 2) # USDA system ) Prediction_top_soil[,c(9:11)] &lt;- Texture # 09.3 Sub soil preparation ==================================================== # We decide to match 30 - 70 cm depth increment of our prediction with 30 - 60 cm SoilGrid model Prediction.thirty &lt;- stack(&quot;./export/final_maps/30_50_prediction_map.tif&quot;) Prediction.thirty &lt;- projectRaster(Prediction.thirty, crs = &quot;EPSG:32638&quot;) Prediction.fifty &lt;- stack(&quot;./export/final_maps/50_70_prediction_map.tif&quot;) Prediction.fifty &lt;- projectRaster(Prediction.fifty, crs = &quot;EPSG:32638&quot;) # Resize and resample SoilGrid.thirty_crop &lt;- crop(SoilGrid.thirty, Prediction.thirty$pH) Prediction.thirty_resample &lt;- resample(Prediction.thirty, SoilGrid.thirty_crop, method = &quot;bilinear&quot;) Prediction.fifty_resample &lt;- resample(Prediction.fifty, SoilGrid.thirty_crop, method = &quot;bilinear&quot;) # Convert into DF Prediction.thirty_df &lt;- raster::as.data.frame(Prediction.thirty_resample, xy = TRUE) Prediction.thirty_df &lt;- Prediction.thirty_df[complete.cases(Prediction.thirty_df),] Prediction.fifty_df &lt;- raster::as.data.frame(Prediction.fifty_resample, xy = TRUE) Prediction.fifty_df &lt;- Prediction.fifty_df[complete.cases(Prediction.fifty_df),] SoilGrid.thirty_df &lt;- raster::as.data.frame(SoilGrid.thirty_crop, xy = TRUE) SoilGrid.thirty_df &lt;- SoilGrid.thirty_df[complete.cases(SoilGrid.thirty_df),] SoilGrid_sub_soil &lt;- SoilGrid.thirty_df # Convert to % values and reduce the pH by 10 to fit our values colnames(SoilGrid_sub_soil) &lt;- c(&quot;x&quot;, &quot;y&quot;, &quot;SoilGrid.Clay&quot;, &quot;SoilGrid.Corg&quot;, &quot;SoilGrid.Nt&quot;, &quot;SoilGrid.pH&quot;, &quot;SoilGrid.Sand&quot;, &quot;SoilGrid.Silt&quot;) SoilGrid_sub_soil[,c(3,6:8)] &lt;- SoilGrid_sub_soil[,c(3,6:8)]/10 # pH standardisation Aitken and Moody (1991) R2 = 0.78 SoilGrid_sub_soil[6] &lt;- (1.28 * SoilGrid_sub_soil[6]) - 0.613 SoilGrid_sub_soil[4] &lt;- SoilGrid_sub_soil[4]/1000 SoilGrid_sub_soil[5] &lt;- SoilGrid_sub_soil[5]/10000 for (i in 3:8) { SoilGrid_sub_soil[[i]][SoilGrid_sub_soil[[i]] == 0] &lt;- median(SoilGrid_sub_soil[[i]]) } sum(SoilGrid_sub_soil == 0) summary(SoilGrid_sub_soil[3:8]) replace_sd &lt;- function(x) { mean_val &lt;- mean(x, na.rm = TRUE) sd_val &lt;- sd(x, na.rm = TRUE) x &lt;- ifelse(x &gt; (mean_val + sd_val), (mean_val + sd_val), ifelse(x &lt; (mean_val - sd_val), (mean_val - sd_val), x)) return(x) } SoilGrid_sub_soil[, 3:8] &lt;- as.data.frame(lapply(SoilGrid_sub_soil[, 3:8], replace_sd)) summary(SoilGrid_sub_soil[3:8]) Prediction_sub_soil &lt;- Prediction.thirty_df for (i in 3:length(Prediction.thirty_df)) { Prediction_sub_soil[i] &lt;- ((Prediction.thirty_df[i]*20) + (Prediction.fifty_df[i]*20))/40 } # pH standardisation Aitken and Moody (1991) R2 = 0.8 Prediction_sub_soil[3] &lt;- (1.175*Prediction_sub_soil[3]) - 0.262 # Texture standardisation Minasny and McBratney (2001) 0.063 to 0.05 Texture &lt;- Prediction_sub_soil[,c(9:11)] colnames(Texture) &lt;- c(&quot;SAND&quot;,&quot;SILT&quot;, &quot;CLAY&quot;) Texture &lt;- TT.normalise.sum(Texture, css.names = c(&quot;SAND&quot;,&quot;SILT&quot;, &quot;CLAY&quot;)) Texture &lt;- TT.text.transf( tri.data = Texture, dat.css.ps.lim = c(0, 0.002, 0.063, 2), # German system base.css.ps.lim = c(0, 0.002, 0.05, 2) # USDA system ) Prediction_sub_soil[,c(9:11)] &lt;- Texture # 09.4 Lower soil preparation ================================================== Prediction.seventy &lt;- stack(&quot;./export/final_maps/70_100_prediction_map.tif&quot;) Prediction.seventy &lt;- projectRaster(Prediction.seventy, crs = &quot;EPSG:32638&quot;) # Resize and resample SoilGrid.sixty_crop &lt;- crop(SoilGrid.sixty, Prediction.seventy$pH) Prediction.seventy_resample &lt;- resample(Prediction.seventy, SoilGrid.sixty_crop, method = &quot;bilinear&quot;) # Convert into DF Prediction.seventy_df &lt;- raster::as.data.frame(Prediction.seventy_resample, xy = TRUE) Prediction.seventy_df &lt;- Prediction.seventy_df[complete.cases(Prediction.seventy_df),] SoilGrid.sixty_df &lt;- raster::as.data.frame(SoilGrid.sixty_crop, xy = TRUE) SoilGrid.sixty_df &lt;- SoilGrid.sixty_df[complete.cases(SoilGrid.sixty_df),] SoilGrid_lower_soil &lt;- SoilGrid.sixty_df # Convert to % values and reduce the pH by 10 to fit our values colnames(SoilGrid_lower_soil) &lt;- c(&quot;x&quot;, &quot;y&quot;, &quot;SoilGrid.Clay&quot;, &quot;SoilGrid.Corg&quot;, &quot;SoilGrid.Nt&quot;, &quot;SoilGrid.pH&quot;, &quot;SoilGrid.Sand&quot;, &quot;SoilGrid.Silt&quot;) SoilGrid_lower_soil[,c(3,6:8)] &lt;- SoilGrid_lower_soil[,c(3,6:8)]/10 # pH standardisation Aitken and Moody (1991) R2 = 0.78 SoilGrid_lower_soil[6] &lt;- (1.28 * SoilGrid_lower_soil[6]) - 0.613 SoilGrid_lower_soil[4] &lt;- SoilGrid_lower_soil[4]/1000 SoilGrid_lower_soil[5] &lt;- SoilGrid_lower_soil[5]/10000 for (i in 3:8) { SoilGrid_lower_soil[[i]][SoilGrid_lower_soil[[i]] == 0] &lt;- median(SoilGrid_lower_soil[[i]]) } sum(SoilGrid_lower_soil == 0) summary(SoilGrid_lower_soil[3:8]) replace_sd &lt;- function(x) { mean_val &lt;- mean(x, na.rm = TRUE) sd_val &lt;- sd(x, na.rm = TRUE) x &lt;- ifelse(x &gt; (mean_val + sd_val), (mean_val + sd_val), ifelse(x &lt; (mean_val - sd_val), (mean_val - sd_val), x)) return(x) } SoilGrid_lower_soil[, 3:8] &lt;- as.data.frame(lapply(SoilGrid_lower_soil[, 3:8], replace_sd)) summary(SoilGrid_lower_soil[3:8]) Prediction_lower_soil &lt;- Prediction.seventy_df # pH standardisation Aitken and Moody (1991) R2 = 0.8 Prediction_lower_soil[3] &lt;- (1.175*Prediction_lower_soil[3]) - 0.262 # Texture standardisation Minasny and McBratney (2001) 0.063 to 0.05 Texture &lt;- Prediction_lower_soil[,c(9:11)] colnames(Texture) &lt;- c(&quot;SAND&quot;,&quot;SILT&quot;, &quot;CLAY&quot;) Texture &lt;- TT.normalise.sum(Texture, css.names = c(&quot;SAND&quot;,&quot;SILT&quot;, &quot;CLAY&quot;)) Texture &lt;- TT.text.transf( tri.data = Texture, dat.css.ps.lim = c(0, 0.002, 0.063, 2), # German system base.css.ps.lim = c(0, 0.002, 0.05, 2) # USDA system ) Prediction_lower_soil[,c(9:11)] &lt;- Texture 7.6.4 Bivariate and density plot of the two predictions # 09.5 Explore the relations ================================================== # Here we decided to split every run by soil depth to have a better vision # on the running process. #=============================================================================== depth &lt;- &quot;lower_soil&quot; increment &lt;- &quot;lower soil&quot; compared_map &lt;- merge(SoilGrid_lower_soil, Prediction_lower_soil[,c(1:3,5,7,9:11)], by =c(&quot;x&quot;, &quot;y&quot;)) colnames(compared_map) compared_map &lt;- compared_map[, c(&quot;x&quot;, &quot;y&quot;, &quot;SoilGrid.pH&quot;, &quot;SoilGrid.Nt&quot;, &quot;SoilGrid.Corg&quot;,&quot;SoilGrid.Sand&quot;, &quot;SoilGrid.Silt&quot;, &quot;SoilGrid.Clay&quot;, &quot;pH&quot; ,&quot;Nt&quot;, &quot;Corg&quot;, &quot;Sand&quot;, &quot;Silt&quot; ,&quot;Clay&quot;)] colnames(compared_map) summary(compared_map[3:8]) summary(compared_map[9:14]) legend &lt;- c(&quot;pH&quot;, &quot;Nt [%]&quot;, &quot;Corg [%]&quot;, &quot;Sand [%]&quot;, &quot;Silt [%]&quot;, &quot;Clay [%]&quot;) two_raster_plot &lt;- function(df, value1, value2, variable, increment) { gg1 &lt;- ggplot() + geom_raster(data = df, aes(x = x, y = y, fill = value1), show.legend = TRUE) + scale_fill_gradientn(colors = hcl.colors(100, &quot;Blues&quot;), name = variable) + ggtitle(paste0(&quot;SoilGrid 250m map of &quot;, variable, &quot; at the &quot;, increment )) + theme_void() + theme( legend.position = &quot;right&quot;, plot.title = element_text(size = 8), legend.title = element_text(size = 6), legend.text = element_text(size = 6), legend.key.size = unit(0.3, &quot;cm&quot;), plot.margin = margin(0, 0, 0, 0) ) + coord_equal(ratio = 1) gg2 &lt;- ggplot() + geom_raster(data = df, aes( x = x, y = y, fill = value2), show.legend = TRUE) + scale_fill_gradientn(colors = hcl.colors(100, &quot;Blues&quot;), name = variable) + ggtitle(paste0(&quot;Predicted map of &quot;, variable, &quot; at the &quot;, increment)) + theme_void() + theme( legend.position = &quot;right&quot;, plot.title = element_text(size = 8), legend.title = element_text(size = 6), legend.text = element_text(size = 6), legend.key.size = unit(0.3, &quot;cm&quot;), plot.margin = margin(0, 0, 0, 0) ) + coord_equal(ratio = 1) two_map &lt;- plot_grid(gg1, gg2, ncol = 2) return(two_map) } comparaison_maps &lt;- list() for (i in 3:8) { z &lt;- i - 2 t &lt;- i + 6 variable &lt;- legend[z] value1 &lt;- compared_map[[i]] value2 &lt;- compared_map[[t]] comparaison_maps[[paste0(variable)]] &lt;- two_raster_plot(compared_map, value1, value2, variable, increment) ggsave(paste0(&quot;./export/visualisations/&quot;,depth,&quot;/Two_maps_&quot;,names(compared_map[t]),&quot;_&quot;,depth,&quot;.pdf&quot;),comparaison_maps[[paste0(variable)]], width = 20, height = 10) ggsave(paste0(&quot;./export/visualisations/&quot;,depth,&quot;/Two_maps_&quot;,names(compared_map[t]),&quot;_&quot;,depth,&quot;.png&quot;),comparaison_maps[[paste0(variable)]], width = 20, height = 10) } # It is not possible to automatise selection of column with &#39;bi_class&#39; data.pH &lt;- bi_class(compared_map, x = pH, y = SoilGrid.pH , style = &quot;quantile&quot;, dim = 3) data.Nt &lt;- bi_class(compared_map, x = Nt, y = SoilGrid.Nt , style = &quot;quantile&quot;, dim = 3) data.Corg &lt;- bi_class(compared_map, x = Corg, y = SoilGrid.Corg , style = &quot;quantile&quot;, dim = 3) data.Sand &lt;- bi_class(compared_map, x = Sand, y = SoilGrid.Sand , style = &quot;quantile&quot;, dim = 3) data.Silt &lt;- bi_class(compared_map, x = Silt, y = SoilGrid.Silt , style = &quot;quantile&quot;, dim = 3) data.Clay &lt;- bi_class(compared_map, x = Clay, y = SoilGrid.Clay , style = &quot;quantile&quot;, dim = 3) data &lt;- cbind(compared_map,data.pH[15],data.Nt[15],data.Corg[15],data.Sand[15],data.Silt[15],data.Clay[15]) names(data)[15:ncol(data)] &lt;- c(&quot;bi_class_pH&quot;, &quot;bi_class_Nt&quot;, &quot;bi_class_Corg&quot;, &quot;bi_class_Sand&quot;, &quot;bi_class_Silt&quot;, &quot;bi_class_Clay&quot;) comparaison_stats &lt;- list() for (i in 3:8) { t &lt;- i + 6 z &lt;- i + 12 value &lt;- names(compared_map[t]) map &lt;- ggplot() + geom_raster(data = data, mapping = aes( x = x, y = y, fill = data[[z]]), show.legend = FALSE) + bi_scale_fill(pal = &quot;GrPink&quot;, dim = 3) + labs(title = paste0(&quot;Prediction maps vs. SoilGrid model for &quot;, increment), subtitle = paste0(&quot;Bivariate map comparison for &quot;, value)) + bi_theme() + coord_equal(ratio = 1) + theme( plot.title = element_text(size = 12), plot.subtitle = element_text(size = 10), axis.title = element_blank(), axis.text = element_blank() ) legend &lt;- bi_legend(pal = &quot;GrPink&quot;, dim = 3, xlab = &quot;Higher values from SoilGrid&quot;, ylab = &quot;Higher values from pred. map&quot;, size = 7) finalPlot &lt;- ggdraw() + draw_plot(map, 0, 0, 1, 1) + draw_plot(legend, 0.01, 0.78, 0.2, 0.2) # Calculate density of predicted values dens_predicted &lt;- density(data[[t]]) dens_predicted$y &lt;- dens_predicted$y / sum(dens_predicted$y) dens_SoilGrid &lt;- density(data[[i]]) dens_SoilGrid$y &lt;- dens_SoilGrid$y / sum(dens_SoilGrid$y) # Check density plot(dens_predicted, main = &quot;Density Plot with predicted Normalization&quot;, xlab = paste0(value), ylab = &quot;Density&quot;) plot(dens_SoilGrid, main = &quot;Density Plot with SoilGrid Normalization&quot;, xlab = paste0(value), ylab = &quot;Density&quot;) gg &lt;- ggplot(data) + geom_density(aes(x = data[[t]], fill = &quot;Predicted values&quot;), alpha = 0.5) + geom_density(aes(x = data[[i]], fill = &quot;SoilGrid values&quot;), alpha = 0.5) + scale_fill_manual(values = c(&quot;Predicted values&quot; = &quot;#404080&quot;, &quot;SoilGrid values&quot; = &quot;#69b3a2&quot;)) + labs(title = paste0(&quot;Density Plot of &quot;, value, &quot; variable between predicted map and SoilGrid model&quot;), x = paste0(value), y = &quot;Density&quot;) + theme_minimal() + theme(legend.title = element_blank(), legend.position = &quot;top&quot;) combined_plot &lt;- plot_grid(gg, finalPlot, ncol = 2) comparaison_stats[[paste0(value)]] &lt;- combined_plot comparaison_stats[[paste0(value)]] ggsave(paste0(&quot;./export/visualisations/&quot;,depth,&quot;/SoilGrid_vs_prediction_for_&quot;,value,&quot;_&quot;,depth,&quot;.pdf&quot;),comparaison_stats[[paste0(value)]], width = 20, height = 10) ggsave(paste0(&quot;./export/visualisations/&quot;,depth,&quot;/SoilGrid_vs_prediction_for_&quot;,value,&quot;_&quot;,depth,&quot;.png&quot;),comparaison_stats[[paste0(value)]], width = 20, height = 10) } save(compared_map, file= paste0(&quot;./export/save/&quot;,depth,&quot;_SoilGrid.RData&quot;)) # 09.6 Plot variation of values for both maps ================================= load(&quot;./export/save/top_soil_SoilGrid.RData&quot;) top.soil &lt;- compared_map load(&quot;./export/save/sub_soil_SoilGrid.RData&quot;) sub.soil &lt;- compared_map load(&quot;./export/save/lower_soil_SoilGrid.RData&quot;) lower.soil &lt;- compared_map final.map &lt;- top.soil[,c(1:2)] compared.list &lt;- list(top.soil = top.soil, sub.soil = sub.soil, lower.soil = lower.soil) # Repeat for each variables names. The command &#39;bi_class&#39; does not work with columns number. for (i in 1:length(compared.list)) { compared.list[[i]] &lt;- bi_class(compared.list[[i]], x = Silt, y = SoilGrid.Silt , style = &quot;quantile&quot;, dim = 3) split_result &lt;- stringr::str_split_fixed(compared.list[[i]][[length(compared.list[[i]])]], &quot;-&quot;, n = 2) compared.list[[i]][[paste0(length(compared.list[[i]]), &quot;_1&quot;)]] &lt;- as.numeric(split_result[,1]) compared.list[[i]][[paste0(length(compared.list[[i]]) + 1, &quot;_2&quot;)]] &lt;- as.numeric(split_result[,2]) } df &lt;- top.soil[,c(1:2)] df$predicted.Silt &lt;- (compared.list[[1]][[length(compared.list[[1]])- 1]] + compared.list[[2]][[length(compared.list[[2]])- 1]] + compared.list[[3]][[length(compared.list[[3]])- 1]])/3 df$SoilGrid.Silt &lt;- (compared.list[[1]][[length(compared.list[[1]])]] + compared.list[[2]][[length(compared.list[[2]])]] + compared.list[[3]][[length(compared.list[[3]])]])/3 df[,c(3:4)] &lt;- round(df[, c(3:4)], digit =0) # Combine columns final.map$Silt &lt;- paste(df[[3]], df[[4]], sep = &quot;-&quot;) light_graph &lt;- list() for (i in 3:length(final.map)) { value &lt;- names(final.map[i]) map &lt;- ggplot() + geom_raster(data = final.map, mapping = aes(x = x, y = y, fill = final.map[[i]]), show.legend = FALSE) + bi_scale_fill(pal = &quot;GrPink&quot;, dim = 3) + labs(title = paste0(&quot;Bivarite map comparison for &quot;, value)) + bi_theme() + coord_equal(ratio = 1) + theme( plot.title = element_text(size = 12), axis.title = element_blank(), axis.text = element_blank() ) if (i == 3) { legend &lt;- bi_legend(pal = &quot;GrPink&quot;, dim = 3, xlab = &quot;Higher values from SoilGrid&quot;, ylab = &quot;Higher values from pred. map&quot;, size = 7) finalPlot &lt;- ggdraw() + draw_plot(map, 0, 0, 1, 1) + draw_plot(legend, 0.01, 0.78, 0.2, 0.2) t &lt;- i - 2 light_graph[[t]] &lt;- finalPlot } if (i &gt; 3){ t &lt;- i - 2 light_graph[[t]] &lt;- map } } png(&quot;./export/visualisations/combinned/SoilGrid_vs_prediction_values.png&quot;, # File name width = 1800, height = 1200) grid.arrange(grobs = light_graph, ncol = 3, top = textGrob(&quot;Bivaviate maps of predicted vs. SoilGrid values&quot;, gp = gpar(fontsize = 10, fontface = &quot;bold&quot;))) dev.off() pdf(&quot;./export/visualisations/combinned/SoilGrid_vs_prediction_values.pdf&quot;, width = 18, height = 12, bg = &quot;white&quot;, colormodel = &quot;cmyk&quot;) grid.arrange(grobs = light_graph, ncol = 3, top = textGrob(&quot;Bivariate maps of predicted vs. SoilGrid values&quot;, gp = gpar(fontsize = 10, fontface = &quot;bold&quot;))) dev.off() rm(list = ls(all.names = TRUE)) 7.6.5 Selection between our predictions and SoilGrid model We adapted our approach of the ensemble model to compare both products from the code in VimiVaron (2022). # 10 Uncertainty ############################################################## # 10.1 Prepare data ============================================================ # To repeat for every soil depth depth &lt;- &quot;0_10&quot; layers &lt;- list.files(paste0(&quot;./export/uncertainty_DSM/&quot;, depth,&quot;/&quot;), pattern = &quot;*tif&quot; , full.names = TRUE) raster_stack &lt;- stack(layers) raster_stack &lt;- raster_stack[[c(8,1,7,4,3,5,9,10,2,6)]] survey &lt;- st_read(&quot;./data/Survey_Area.gpkg&quot;, layer = &quot;Survey_Area&quot;) names(raster_stack) &lt;- c(&quot;pH&quot;, &quot;CaCO3&quot;, &quot;Nt&quot;, &quot;Ct&quot;, &quot;Corg&quot;, &quot;EC&quot;, &quot;Sand&quot;, &quot;Silt&quot;, &quot;Clay&quot;, &quot;MWD&quot;) crs(raster_stack) &lt;- &quot;EPSG:32638&quot; x &lt;- rast(raster_stack) x_croped &lt;- crop(x, survey) x_masked &lt;- mask(x_croped, survey) terra::writeRaster(x_masked, paste0(&quot;./export/uncertainty_DSM/&quot;, depth, &quot;_uncertainty_map.tif&quot;), overwrite=TRUE) rm(list = ls(all.names = TRUE)) # 10.2 Prepare SG ============================================================== # For 0-5 cm increment files &lt;- list.files(&quot;./data/Soil_grid/Uncertainty/0_5/&quot;, pattern = &quot;*tif&quot;, full.names = TRUE) SoilGrid.zero &lt;- stack(files) SoilGrid.zero &lt;- projectRaster(SoilGrid.zero, crs = &quot;EPSG:32638&quot;) # For 5-15 cm increment files &lt;- list.files(&quot;./data/Soil_grid/Uncertainty/5_15/&quot;, pattern = &quot;*tif&quot;, full.names = TRUE) SoilGrid.five &lt;- stack(files) SoilGrid.five &lt;- projectRaster(SoilGrid.five, crs = &quot;EPSG:32638&quot;) # For 15-30 cm increment files &lt;- list.files(&quot;./data/Soil_grid/Uncertainty/15_30/&quot;, pattern = &quot;*tif&quot;, full.names = TRUE) SoilGrid.fifteen &lt;- stack(files) SoilGrid.fifteen &lt;- projectRaster(SoilGrid.fifteen, crs = &quot;EPSG:32638&quot;) # For 30-60 cm increment files &lt;- list.files(&quot;./data/Soil_grid/Uncertainty/30_60/&quot;, pattern = &quot;*tif&quot;, full.names = TRUE) SoilGrid.thirty &lt;- stack(files) SoilGrid.thirty &lt;- projectRaster(SoilGrid.thirty, crs = &quot;EPSG:32638&quot;) # For 60-100 cm increment files &lt;- list.files(&quot;./data/Soil_grid/Uncertainty/60_100/&quot;, pattern = &quot;*tif&quot;, full.names = TRUE) SoilGrid.sixty &lt;- stack(files) SoilGrid.sixty &lt;- projectRaster(SoilGrid.sixty, crs = &quot;EPSG:32638&quot;) # 10.3 Top soil preparation ==================================================== Prediction.zero &lt;- stack(&quot;./export/uncertainty_DSM/0_10_uncertainty_map.tif&quot;) crs(Prediction.zero) &lt;- &quot;EPSG:32638&quot; Prediction.ten &lt;- stack(&quot;./export/uncertainty_DSM/10_30_uncertainty_map.tif&quot;) crs(Prediction.ten) &lt;- &quot;EPSG:32638&quot; # Resize and resample SoilGrid.zero_crop &lt;- crop(SoilGrid.zero, Prediction.zero$pH) SoilGrid.five_crop &lt;- crop(SoilGrid.five, Prediction.zero$pH) SoilGrid.fifteen_crop &lt;- crop(SoilGrid.fifteen, Prediction.zero$pH) Prediction.zero_resample &lt;- resample(Prediction.zero, SoilGrid.zero_crop, method = &quot;bilinear&quot;) Prediction.ten_resample &lt;- resample(Prediction.ten, SoilGrid.zero_crop, method = &quot;bilinear&quot;) # Convert into DF Prediction.zero_df &lt;- raster::as.data.frame(Prediction.zero_resample, xy = TRUE) Prediction.zero_df &lt;- Prediction.zero_df[complete.cases(Prediction.zero_df),] Prediction.ten_df &lt;- raster::as.data.frame(Prediction.ten_resample, xy = TRUE) Prediction.ten_df &lt;- Prediction.ten_df[complete.cases(Prediction.ten_df),] SoilGrid.zero_df &lt;- raster::as.data.frame(SoilGrid.zero_crop, xy = TRUE) SoilGrid.zero_df &lt;- SoilGrid.zero_df[complete.cases(SoilGrid.zero_df),] SoilGrid.five_df &lt;- raster::as.data.frame(SoilGrid.five_crop, xy = TRUE) SoilGrid.five_df &lt;- SoilGrid.five_df[complete.cases(SoilGrid.five_df),] SoilGrid.fifteen_df &lt;- raster::as.data.frame(SoilGrid.fifteen_crop, xy = TRUE) SoilGrid.fifteen_df&lt;- SoilGrid.fifteen_df[complete.cases(SoilGrid.fifteen_df),] SoilGrid_top_soil &lt;- SoilGrid.zero_df for (i in 3: length(SoilGrid_top_soil)) { SoilGrid_top_soil[i] &lt;- ((SoilGrid.zero_df[i]*5) + (SoilGrid.five_df[i]*10) + (SoilGrid.fifteen_df[i]*15))/30 } # Convert to % values and reduce the pH by 10 to fit our values colnames(SoilGrid_top_soil) &lt;- c(&quot;x&quot;, &quot;y&quot;, &quot;SoilGrid.Clay&quot;, &quot;SoilGrid.Corg&quot;, &quot;SoilGrid.Nt&quot;, &quot;SoilGrid.pH&quot;, &quot;SoilGrid.Sand&quot;, &quot;SoilGrid.Silt&quot;) SoilGrid_top_soil[,c(3,6:8)] &lt;- SoilGrid_top_soil[,c(3,6:8)]/10 SoilGrid_top_soil[4] &lt;- SoilGrid_top_soil[4]/1000 SoilGrid_top_soil[5] &lt;- SoilGrid_top_soil[5]/10000 # Replace zero value and values under or over the SD from the SoilGrid for (i in 3:8) { SoilGrid_top_soil[[i]][SoilGrid_top_soil[[i]] == 0] &lt;- median(SoilGrid_top_soil[[i]]) } sum(SoilGrid_top_soil == 0) summary(SoilGrid_top_soil[3:8]) replace_sd &lt;- function(x) { mean_val &lt;- mean(x, na.rm = TRUE) sd_val &lt;- sd(x, na.rm = TRUE) x &lt;- ifelse(x &gt; (mean_val + sd_val), (mean_val + sd_val), ifelse(x &lt; (mean_val - sd_val), (mean_val - sd_val), x)) return(x) } SoilGrid_top_soil[, 3:8] &lt;- as.data.frame(lapply(SoilGrid_top_soil[, 3:8], replace_sd)) summary(SoilGrid_top_soil[3:8]) Prediction_top_soil &lt;- Prediction.zero_df for (i in 3:length(Prediction.zero_df)) { Prediction_top_soil[i] &lt;- ((Prediction.zero_df[i]*10) + (Prediction.ten_df[i]*20))/30 } # 10.4 Sub soil preparation ==================================================== # We decide to match 30 - 70 cm depth increment of our prediction with 30 - 60 cm SoilGrid model Prediction.thirty &lt;- stack(&quot;./export/uncertainty_DSM/30_50_uncertainty_map.tif&quot;) crs(Prediction.thirty) &lt;- &quot;EPSG:32638&quot; Prediction.fifty &lt;- stack(&quot;./export/uncertainty_DSM/50_70_uncertainty_map.tif&quot;) crs(Prediction.fifty) &lt;- &quot;EPSG:32638&quot; # Resize and resample SoilGrid.thirty_crop &lt;- crop(SoilGrid.thirty, Prediction.thirty$pH) Prediction.thirty_resample &lt;- resample(Prediction.thirty, SoilGrid.thirty_crop, method = &quot;bilinear&quot;) Prediction.fifty_resample &lt;- resample(Prediction.fifty, SoilGrid.thirty_crop, method = &quot;bilinear&quot;) # Convert into DF Prediction.thirty_df &lt;- raster::as.data.frame(Prediction.thirty_resample, xy = TRUE) Prediction.thirty_df &lt;- Prediction.thirty_df[complete.cases(Prediction.thirty_df),] Prediction.fifty_df &lt;- raster::as.data.frame(Prediction.fifty_resample, xy = TRUE) Prediction.fifty_df &lt;- Prediction.fifty_df[complete.cases(Prediction.fifty_df),] SoilGrid.thirty_df &lt;- raster::as.data.frame(SoilGrid.thirty_crop, xy = TRUE) SoilGrid.thirty_df &lt;- SoilGrid.thirty_df[complete.cases(SoilGrid.thirty_df),] SoilGrid_sub_soil &lt;- SoilGrid.thirty_df # Convert to % values and reduce the pH by 10 to fit our values colnames(SoilGrid_sub_soil) &lt;- c(&quot;x&quot;, &quot;y&quot;, &quot;SoilGrid.Clay&quot;, &quot;SoilGrid.Corg&quot;, &quot;SoilGrid.Nt&quot;, &quot;SoilGrid.pH&quot;, &quot;SoilGrid.Sand&quot;, &quot;SoilGrid.Silt&quot;) SoilGrid_sub_soil[,c(3,6:8)] &lt;- SoilGrid_sub_soil[,c(3,6:8)]/10 SoilGrid_sub_soil[4] &lt;- SoilGrid_sub_soil[4]/1000 SoilGrid_sub_soil[5] &lt;- SoilGrid_sub_soil[5]/10000 for (i in 3:8) { SoilGrid_sub_soil[[i]][SoilGrid_sub_soil[[i]] == 0] &lt;- median(SoilGrid_sub_soil[[i]]) } sum(SoilGrid_sub_soil == 0) summary(SoilGrid_sub_soil[3:8]) replace_sd &lt;- function(x) { mean_val &lt;- mean(x, na.rm = TRUE) sd_val &lt;- sd(x, na.rm = TRUE) x &lt;- ifelse(x &gt; (mean_val + sd_val), (mean_val + sd_val), ifelse(x &lt; (mean_val - sd_val), (mean_val - sd_val), x)) return(x) } SoilGrid_sub_soil[, 3:8] &lt;- as.data.frame(lapply(SoilGrid_sub_soil[, 3:8], replace_sd)) summary(SoilGrid_sub_soil[3:8]) Prediction_sub_soil &lt;- Prediction.thirty_df for (i in 3:length(Prediction.thirty_df)) { Prediction_sub_soil[i] &lt;- ((Prediction.thirty_df[i]*20) + (Prediction.fifty_df[i]*20))/40 } # 10.5 Lower soil preparation ================================================== Prediction.seventy &lt;- stack(&quot;./export/uncertainty_DSM/70_100_uncertainty_map.tif&quot;) crs(Prediction.seventy) &lt;- &quot;EPSG:32638&quot; # Resize and resample SoilGrid.sixty_crop &lt;- crop(SoilGrid.sixty, Prediction.seventy$pH) Prediction.seventy_resample &lt;- resample(Prediction.seventy, SoilGrid.sixty_crop, method = &quot;bilinear&quot;) # Convert into DF Prediction.seventy_df &lt;- raster::as.data.frame(Prediction.seventy_resample, xy = TRUE) Prediction.seventy_df &lt;- Prediction.seventy_df[complete.cases(Prediction.seventy_df),] SoilGrid.sixty_df &lt;- raster::as.data.frame(SoilGrid.sixty_crop, xy = TRUE) SoilGrid.sixty_df &lt;- SoilGrid.sixty_df[complete.cases(SoilGrid.sixty_df),] SoilGrid_lower_soil &lt;- SoilGrid.sixty_df # Convert to % values and reduce the pH by 10 to fit our values colnames(SoilGrid_lower_soil) &lt;- c(&quot;x&quot;, &quot;y&quot;, &quot;SoilGrid.Clay&quot;, &quot;SoilGrid.Corg&quot;, &quot;SoilGrid.Nt&quot;, &quot;SoilGrid.pH&quot;, &quot;SoilGrid.Sand&quot;, &quot;SoilGrid.Silt&quot;) SoilGrid_lower_soil[,c(3,6:8)] &lt;- SoilGrid_lower_soil[,c(3,6:8)]/10 SoilGrid_lower_soil[4] &lt;- SoilGrid_lower_soil[4]/1000 SoilGrid_lower_soil[5] &lt;- SoilGrid_lower_soil[5]/10000 for (i in 3:8) { SoilGrid_lower_soil[[i]][SoilGrid_lower_soil[[i]] == 0] &lt;- median(SoilGrid_lower_soil[[i]]) } sum(SoilGrid_lower_soil == 0) summary(SoilGrid_lower_soil[3:8]) replace_sd &lt;- function(x) { mean_val &lt;- mean(x, na.rm = TRUE) sd_val &lt;- sd(x, na.rm = TRUE) x &lt;- ifelse(x &gt; (mean_val + sd_val), (mean_val + sd_val), ifelse(x &lt; (mean_val - sd_val), (mean_val - sd_val), x)) return(x) } SoilGrid_lower_soil[, 3:8] &lt;- as.data.frame(lapply(SoilGrid_lower_soil[, 3:8], replace_sd)) summary(SoilGrid_lower_soil[3:8]) Prediction_lower_soil &lt;- Prediction.seventy_df # 10.6 Ensemble model ========================================================== depth &lt;- &quot;lower_soil&quot; load(paste0(&quot;./export/save/&quot;,depth,&quot;_SoilGrid.RData&quot;)) prediction_map &lt;- rasterFromXYZ(compared_map) crs(prediction_map) &lt;- &quot;EPSG:32638&quot; uncertainty_df &lt;- merge(SoilGrid_lower_soil, Prediction_lower_soil[,c(1:3,5,7,9:11)], by =c(&quot;x&quot;, &quot;y&quot;)) colnames(uncertainty_map) uncertainty_df &lt;- uncertainty_df[, c(&quot;x&quot;, &quot;y&quot;, &quot;SoilGrid.pH&quot;, &quot;SoilGrid.Nt&quot;, &quot;SoilGrid.Corg&quot;,&quot;SoilGrid.Sand&quot;, &quot;SoilGrid.Silt&quot;, &quot;SoilGrid.Clay&quot;, &quot;pH&quot; ,&quot;Nt&quot;, &quot;Corg&quot;, &quot;Sand&quot;, &quot;Silt&quot; ,&quot;Clay&quot;)] uncertainty_map &lt;- rasterFromXYZ(uncertainty_df) crs(uncertainty_map) &lt;- &quot;EPSG:32638&quot; uncertainty_map &lt;- resample(uncertainty_map, prediction_map, method= &quot;bilinear&quot;) # Calculate MAE from all residuals SG_ERROR_ABS&lt;-(abs(uncertainty_map$SoilGrid.pH)+ abs(uncertainty_map$SoilGrid.Nt)+ abs(uncertainty_map$SoilGrid.Corg)+ abs(uncertainty_map$SoilGrid.Sand)+ abs(uncertainty_map$SoilGrid.Silt)+ abs(uncertainty_map$SoilGrid.Clay))/6 Prediction_ERROR_ABS&lt;-(abs(uncertainty_map$pH)+ abs(uncertainty_map$Nt)+ abs(uncertainty_map$Corg)+ abs(uncertainty_map$Sand)+ abs(uncertainty_map$Silt)+ abs(uncertainty_map$Clay))/6 residuals &lt;-stack(Prediction_ERROR_ABS, SG_ERROR_ABS) names(residuals)&lt;-c(&quot;ERROR_Prediction&quot;,&quot;ERROR_SG&quot;) ensemble &lt;- function(predvalues, serrors, basemap = 1){ serrors &lt;- round(serrors, 2) result &lt;- predvalues[[basemap]] names(result) &lt;- &#39;result&#39; model &lt;- raster(predvalues[[1]]) values(model) &lt;- basemap model[is.na(result)] &lt;- NA minerror &lt;- min(stack(serrors)) names(model) &lt;- &quot;model&quot; names(minerror) &lt;- &quot;error&quot; result[serrors[[2]] == minerror] &lt;- predvalues[[2]][serrors[[2]] == minerror] model[serrors[[2]] == minerror] &lt;- 2 minerror &lt;- mask(minerror, result) model &lt;- mask(model, result) return(stack(result, minerror, model)) } predictions &lt;- list() for (i in 1:(nlayers(prediction_map)/2)) { x &lt;- stack(prediction_map[[i+6]],prediction_map[[i]]) # Run the comparison model start &lt;- Sys.time() predictions[[i]] &lt;- ensemble(predvalues= x , serrors=abs(residuals)) print(Sys.time() - start) } # 10.7 Export the evaluation of SG ============================================= r_stack &lt;- stack(predictions[[1]][[3]], predictions[[2]][[3]],predictions[[3]][[3]],predictions[[4]][[3]], predictions[[5]][[3]],predictions[[6]][[3]]) r_stack[is.na(r_stack)] &lt;- 1 model_raster &lt;- calc(r_stack, fun = function(x) { if (any(x == 2)) { return(2) } else { return(1) } }) save(model_raster, file= paste0(&quot;./export/save/&quot;,depth,&quot;_model_selection.RData&quot;)) rm(list = ls(all.names = TRUE)) # 10.8 Plot the models maps =================================================== load(&quot;./export/save/top_soil_model_selection.RData&quot;) top.soil &lt;- model_raster load(&quot;./export/save/sub_soil_model_selection.RData&quot;) sub.soil &lt;- model_raster load(&quot;./export/save/lower_soil_model_selection.RData&quot;) lower.soil &lt;- model_raster survey &lt;- st_read(&quot;./data/Survey_Area.gpkg&quot;, layer = &quot;Survey_Area&quot;) model.map &lt;- stack(top.soil, sub.soil, lower.soil) names(model.map) &lt;- c(&quot;top_soil&quot;, &quot;sub_soil&quot;, &quot;lower_soil&quot;) crs(model.map) &lt;- &quot;EPSG:32638&quot; x &lt;- rast(model.map) x_croped &lt;- crop(x, survey) x_masked &lt;- mask(x_croped, survey) x_repro &lt;- project(x_masked, &quot;EPSG:4326&quot;) terra::writeRaster(x_repro, paste0(&quot;./export/visualisations/Model_accuracy.tif&quot;), overwrite=TRUE) model_df &lt;- raster::as.data.frame(x_repro, xy = TRUE) model_df &lt;- model_df[complete.cases(model_df),] raster_plot &lt;- function(rasterdf, depth) { p &lt;- ggplot() + geom_raster(data = rasterdf, aes(x = x, y = y, fill = Model)) + ggtitle(paste0(&quot;Models for the &quot;, depth)) + scale_fill_manual( values = c(&quot;Prediction model&quot; = &quot;grey&quot;, &quot;SoilGrid model&quot; = &quot;red&quot;), name = &quot;Best model based on residuals&quot; ) + theme_void() + theme( plot.title = element_text(size = 12), axis.title = element_blank(), axis.text = element_blank() ) + coord_equal(ratio = 1) return(p) } graph &lt;- list() depth &lt;- c(&quot;Top soil&quot;, &quot;Sub soil&quot;, &quot;Lower soil&quot;) for (i in 3:length(model_df)) { t &lt;- i -2 model_df$Model &lt;- ifelse(model_df[[i]] &lt;= 1, &quot;Prediction model&quot;, &quot;SoilGrid model&quot;) graph[[t]] &lt;-raster_plot(model_df, depth[t]) } png(&quot;./export/visualisations/combinned/Model_accuracy.png&quot;, width = 1800, height = 1200) grid.arrange(grobs = graph, ncol = 3, top = textGrob(&quot;Comparison of models residuals&quot;, gp = gpar(fontsize = 10, fontface = &quot;bold&quot;))) dev.off() pdf(&quot;./export/visualisations/combinned/Model_accuracy.pdf&quot;, width = 18, height = 12, bg = &quot;white&quot;, colormodel = &quot;cmyk&quot;) grid.arrange(grobs = graph, ncol = 3, top = textGrob(&quot;Comparison of models residuals&quot;, gp = gpar(fontsize = 10, fontface = &quot;bold&quot;))) dev.off() rm(list = ls(all.names = TRUE)) References "],["soil-depth-mapping.html", "8 Soil depth mapping 8.1 Introduction 8.2 Soil depth mapping prediction 8.3 Visualisations and exports", " 8 Soil depth mapping 8.1 Introduction 8.1.1 Purpose This document is meant to present the methodology we used for soil depth mapping. Our data are based on the sampling campaigns 2022 - 2023 and additional remote sensing observation for bare rock formations. We based our approach on Liu et al. (2022) and added small modifications. 8.1.2 Covariates All the data listed below are available freely online excepted for the digitized maps (Geology). (#tab:input_5)Data used in the production of the conditioned soil depth mapping. Name/ID Original resolution (m) Type/Unit Source Aspect/TE.1 25 Radians ESA and Airbus (2022) DEM/TE.5 25 Meters ESA and Airbus (2022) General curvature/TE.7 25 - ESA and Airbus (2022) MrRTF/TE.8 25 - ESA and Airbus (2022) MrVBF/TE.9 25 - ESA and Airbus (2022) Plan curvature/TE.12 25 - ESA and Airbus (2022) Profile curvature/TE.14 25 - ESA and Airbus (2022) Slope/TE.16 25 Radians ESA and Airbus (2022) TPI/TE.21 25 - ESA and Airbus (2022) Landsat 5 B2/LA5.1 30 0.52 - 0.60 Î¼m EROS (2013) Landsat 5 B3/LA5.2 30 0.63 - 0.69 Î¼m EROS (2013) Landsat 5 B4/LA5.3 30 0.76 - 0.90 Î¼m EROS (2013) Landsat 5 B7/LA5.4 30 2.08 - 2.35 Î¼m EROS (2013) Landsat 5 NDVI/LA5.5 30 - Mathias Bellat / EROS (2013) Landsat 5 NDWI/LA5.6 30 - EROS (2013) LST Apr-May/LST.1 1000 Kelvin Hulley and Hook (2018) LST Feb-Mar/LST.2 1000 Kelvin Hulley and Hook (2018) LST Jun-Jul/LST.3 1000 Kelvin Hulley and Hook (2018) LST Oct-Nov/LST.4 1000 Kelvin Hulley and Hook (2018) Geology/OT.2 1 : 250 000 Lithology Sissakian, Hagopian, and Hasan (1995), al-mousawi_geological_2007 Landuse/OT.4 10 Landuse class Zanaga et al. (2021) PET sum/OT.5 750 mm Zomer and Trabucco (2022) Prec. sum/OT.6 1000 mm Fick and Hijmans (2017) SRAD sum/OT.7 1000 MJ m\\(^{-2}\\) Fick and Hijmans (2017) Wind sum/OT.9 1000 m s\\(^{-1}\\) Fick and Hijmans (2017) Temp avg/OT.10 1000 °C Fick and Hijmans (2017) 8.1.2.1 Terrain All terrain covariates were computed the same way as in the previous chapter. 8.1.2.2 Remote sensing images and indexes The Landsat 5 images were collected via a Google Earth Engine script on a period covering 1990 - 2010, the median of the composite image from Collection 2 Tier 1 TOA was used. The land surface temperature (LST) were computed also on Google Earth Engine with a time covering from 2012 to 2016 from the Suomi National Polar-Orbiting satellite. We took the average of each period: February - Mars, April - May, June - July and October November from the VIIRS instrument. The javascript codes for scraping these images are available in the supplementary file inside the “8 - Soil depth/code” folder. We computed the following indexes: Normalized Difference Vegetation Index (NDVI) (McFeeters 1996) and Normalized Difference Water Index (NDWI). \\[ NDVI = (NIR - Red) / (NIR + Red) \\] \\[ NDWI = (Green - NIR) / (Green + NIR) \\] 8.1.3 Soil depths The soil depths of the 122 soil profiles were measured during the field campaign and can be found found online at https://doi.org/10.1594/PANGAEA.973714. Additional 25 site with no soil (or zero value to soil depth), were added after looking at Bing and Google earth satellite image. We only selected bare rock formation for these sites. 8.1.4 Preparation of the data All raster were sampled to 25 x 25 m tiles to match the DEM. We used bilinear method excepted for the discrete maps (geology) where we used ngb resampling. library(raster) library(terra) # 1.1 Import data ============================================================== DEM &lt;- raster(&quot;./data/DEM_GLO_25.tif&quot;) rlist&lt;-list.files(&quot;./data/remote&quot;, full.names=T) rlist2&lt;-list.files(&quot;./data/remote/&quot;, full.names=F) outpath&lt;-&quot;./data/remote/resample/&quot; outfiles &lt;- paste0(outpath, rlist2) # 1.1 Resample loop ============================================================ for (i in (1:length(rlist))){ x&lt;-raster(rlist[i]) x &lt;- projectRaster(x, crs=CRS(&quot;+init=epsg:32638&quot;)) x&lt;-resample(x,DEM,method = &quot;bilinear&quot;) #Careful to implement &quot;ngb&quot; option for discrete values x&lt;-crop(x,DEM) writeRaster(x,filename=outfiles[i], format=&quot;GTiff&quot;,overwrite=T) } # 1.2 Export as a stack_raster ================================================= rlist &lt;- list.files(&quot;./data/remote/resample/&quot;, full.names=T) x &lt;- stack(rlist, DEM) x &lt;- rast(x) terra::writeRaster(x, &quot;./export/Stack_layers_depth.tif&quot;) 8.2 Soil depth mapping prediction 8.2.1 Preparation of the environment # 0.1 Prepare environment ====================================================== # Folder check getwd() # Set folder direction setwd(dir=&quot;./depth&quot;) # Clean up workspace rm(list = ls(all.names = TRUE)) # 0.2 Install packages ========================================================= install.packages(&quot;pacman&quot;) #Install and load the &quot;pacman&quot; package (allow easier download of packages) library(pacman) pacman::p_load(ggplot2, terra, mapview, sf, raster, dplyr, corrplot, viridis, rsample, caret, parsnip, tmap, DescTools, patchwork, quantregForest, wesanderson, cblindplot, grid, gridExtra, ggspatial, cowplot, ncdf4) # 0.3 Show session infos ======================================================= sessionInfo() ## R version 4.4.0 (2024-04-24 ucrt) ## Platform: x86_64-w64-mingw32/x64 ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=French_France.utf8 LC_CTYPE=French_France.utf8 ## [3] LC_MONETARY=French_France.utf8 LC_NUMERIC=C ## [5] LC_TIME=French_France.utf8 ## ## time zone: Europe/Berlin ## tzcode source: internal ## ## attached base packages: ## [1] grid stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] ncdf4_1.23 cowplot_1.1.3 ggspatial_1.1.9 ## [4] gridExtra_2.3 cblindplot_0.0.4 wesanderson_0.3.7 ## [7] quantregForest_1.3-7.1 RColorBrewer_1.1-3 patchwork_1.3.0 ## [10] DescTools_0.99.57 tmap_3.3-4 parsnip_1.2.1 ## [13] caret_6.0-94 rsample_1.2.1 viridis_0.6.5 ## [16] corrplot_0.95 dplyr_1.1.4 raster_3.6-30 ## [19] sp_2.1-4 sf_1.0-18 mapview_2.11.2 ## [22] terra_1.7-83 DT_0.33 readr_2.1.5 ## [25] SpaDES.tools_2.0.7 SpaDES.core_2.1.0 quickPlot_1.0.2 ## [28] reproducible_2.1.0 remotes_2.5.0 randomForest_4.7-1.2 ## [31] lattice_0.22-6 viridisLite_0.4.2 pacman_0.5.1 ## [34] stringr_1.5.1 ggplot2_3.5.1 bookdown_0.41 ## [37] tufte_0.13 rmarkdown_2.29 knitr_1.49 ## ## loaded via a namespace (and not attached): ## [1] fpCompare_0.2.4 fs_1.6.4 lubridate_1.9.3 ## [4] httr_1.4.7 tools_4.4.0 backports_1.5.0 ## [7] utf8_1.2.4 R6_2.5.1 withr_3.0.2 ## [10] leaflet_2.2.2 leafem_0.2.3 cli_3.6.3 ## [13] labeling_0.4.3 sass_0.4.9 mvtnorm_1.3-1 ## [16] proxy_0.4-27 systemfonts_1.1.0 svglite_2.1.3 ## [19] dichromat_2.0-0.1 parallelly_1.38.0 readxl_1.4.3 ## [22] rstudioapi_0.17.1 generics_0.1.3 RApiSerialize_0.1.4 ## [25] crosstalk_1.2.1 vroom_1.6.5 Matrix_1.7-0 ## [28] fansi_1.0.6 abind_1.4-8 lifecycle_1.0.4 ## [31] whisker_0.4.1 yaml_2.3.10 mathjaxr_1.6-0 ## [34] tmaptools_3.1-1 recipes_1.1.0 crayon_1.5.3 ## [37] Require_1.0.1 sys_3.4.3 pillar_1.9.0 ## [40] tcltk_4.4.0 boot_1.3-30 gld_2.6.6 ## [43] future.apply_1.11.3 codetools_0.2-20 glue_1.7.0 ## [46] fontLiberation_0.1.0 data.table_1.16.2 vctrs_0.6.5 ## [49] png_0.1-8 cellranger_1.1.0 gtable_0.3.6 ## [52] kernlab_0.9-33 cachem_1.1.0 gower_1.0.1 ## [55] xfun_0.48 prodlim_2024.06.25 survival_3.5-8 ## [58] timeDate_4041.110 iterators_1.0.14 hardhat_1.4.0 ## [61] units_0.8-5 lava_1.8.0 brew_1.0-10 ## [64] ipred_0.9-15 nlme_3.1-164 satellite_1.0.5 ## [67] bit64_4.5.2 fontquiver_0.2.1 bslib_0.8.0 ## [70] KernSmooth_2.23-22 rpart_4.1.23 colorspace_2.1-1 ## [73] DBI_1.2.3 nnet_7.3-19 Exact_3.3 ## [76] tidyselect_1.2.1 bit_4.5.0 compiler_4.4.0 ## [79] extrafontdb_1.0 curl_5.2.3 expm_1.0-0 ## [82] fontBitstreamVera_0.1.1 stringfish_0.16.0 checkmate_2.3.2 ## [85] scales_1.3.0 classInt_0.4-10 digest_0.6.37 ## [88] htmltools_0.5.8.1 pkgconfig_2.0.3 base64enc_0.1-3 ## [91] extrafont_0.19 fastmap_1.2.0 rlang_1.1.4 ## [94] htmlwidgets_1.6.4 farver_2.1.2 jquerylib_0.1.4 ## [97] jsonlite_1.8.9 ModelMetrics_1.2.2.2 magrittr_2.0.3 ## [100] munsell_0.5.1 Rcpp_1.0.13 gdtools_0.4.0 ## [103] furrr_0.3.1 lobstr_1.1.2 stringi_1.8.4 ## [106] leafsync_0.1.0 pROC_1.18.5 rootSolve_1.8.2.4 ## [109] MASS_7.3-60.2 plyr_1.8.9 parallel_4.4.0 ## [112] listenv_0.9.1 lmom_3.2 stars_0.6-6 ## [115] splines_4.4.0 hms_1.1.3 leafpop_0.1.0 ## [118] igraph_2.1.1 uuid_1.2-1 reshape2_1.4.4 ## [121] stats4_4.4.0 XML_3.99-0.17 evaluate_1.0.1 ## [124] leaflet.providers_2.0.0 RcppParallel_5.1.9 tzdb_0.4.0 ## [127] foreach_1.5.2 Rttf2pt1_1.3.12 tidyr_1.3.1 ## [130] purrr_1.0.2 qs_0.27.2 future_1.34.0 ## [133] lwgeom_0.2-14 e1071_1.7-16 class_7.3-22 ## [136] tibble_3.2.1 cluster_2.1.6 timechange_0.3.0 ## [139] globals_0.16.3 8.2.2 Prepare the data # 01 Import data sets ########################################################## # 01.1 Import soils depths ===================================================== depth &lt;- read.csv(&quot;./data/Soil_depth.csv&quot;, sep=&quot;;&quot;) depth$Depth &lt;- as.numeric(depth$Depth) # 01.2 Plot the depth information ============================================== # Short overview of the values head(depth) ## X Y Site.Name Depth ## 1 287940.2 4095202 B07_2022_001 100 ## 2 286917.0 4104485 B07_2022_002 65 ## 3 280594.1 4090376 B07_2022_003 40 ## 4 275039.6 4097134 B07_2022_004 80 ## 5 289512.9 4086706 B07_2022_006 80 ## 6 287922.6 4092158 B07_2022_007 0 # Histogram of the values ggplot(depth, aes(x = Depth)) + geom_histogram(breaks = seq(0, 100, by = 10), alpha = 0.5, fill = &#39;steelblue&#39;, color = &#39;black&#39;) + labs(title =&quot;Number of smaple for each soil depth&quot;, x = &quot;Depth (cm)&quot;, y = &quot;Number of samples&quot;) + xlim(0, 100) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) # 01.3 Transform and plot the data ============================================= depth$sqrt &lt;- sqrt(depth$Depth) # Histogram of the sqrt values ggplot(depth, aes(x = sqrt)) + geom_histogram(breaks = seq(0, 10, by = 0.5), alpha = 0.5, fill = &#39;steelblue&#39;, color = &#39;black&#39;) + labs(title =&quot;Number of sample for each soil depth in cm&quot;, x = &quot;Sqrt-transformed soil depth in cm&quot;, y = &quot;Number of samples&quot;) + xlim(0, 11) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) # 01.4 Plot the depth spatial information ====================================== # Create a spatial dataframe sp_df &lt;- st_as_sf(depth, coords = c(&quot;X&quot;, &quot;Y&quot;), crs = 32638) # Plot the values of depth regarding the elevation mapview(sp_df, zcol = &quot;Depth&quot;, legend =TRUE, map.types =&quot;Esri.WorldShadedRelief&quot;) # 01.5 Import covariates raster ================================================ Landsat &lt;- raster::stack(list.files(&quot;./data/Landsat/&quot;, full.names = TRUE)) names(Landsat) Terrain &lt;- raster::stack(list.files(&quot;./data/Terrain/&quot;, full.names = TRUE)) names(Terrain) Others &lt;- raster::stack(list.files(&quot;./data/Others/&quot;, full.names = TRUE)) names(Others) LST &lt;- raster::stack(list.files(&quot;./data/LST/&quot;, full.names = TRUE)) names(LST) DSM_cov_name &lt;- read.table(&quot;./data/Covariates_names_DSM.txt&quot;) df_names &lt;- data.frame() for (i in 1:length(names(Terrain))) { col_name &lt;- names(Terrain)[i] if (col_name %in% DSM_cov_name$V2) { transformed_name &lt;- DSM_cov_name$V1[DSM_cov_name$V2 == col_name] } else { transformed_name &lt;- paste0(&quot;TE.26&quot;) } df_names[i,1] &lt;- transformed_name df_names[i,2] &lt;- names(Terrain)[i] } t &lt;- nrow(df_names) for (i in 1:length(names(Landsat))) { c &lt;- paste0(&quot;LA5.&quot;,i) df_names[i+t,1] &lt;- c df_names[i+t,2] &lt;- names(Landsat)[i] } t &lt;- nrow(df_names) for (i in 1:length(names(LST))) { c &lt;- paste0(&quot;LST.&quot;,i) df_names[i+t,1] &lt;- c df_names[i+t,2] &lt;- names(LST)[i] } t &lt;- nrow(df_names) for (i in 1:length(names(Others))) { col_name &lt;- names(Others)[i] if (col_name %in% DSM_cov_name$V2) { transformed_name &lt;- DSM_cov_name$V1[DSM_cov_name$V2 == col_name] } else { transformed_name &lt;- &quot;OT.10&quot; #Only one new variable } df_names[i+t,1] &lt;- transformed_name df_names[i+t,2] &lt;- names(Others)[i] } write.table(df_names,&quot;./data/Covariates_names_soil_depth.txt&quot;) all_cov_name &lt;- rbind(DSM_cov_name, df_names) all_cov_name &lt;- distinct(all_cov_name) x &lt;- raster::stack(Terrain, Landsat, LST, Others) names(x) &lt;- df_names[,1] x &lt;- rast(x) terra::writeRaster(x, &quot;./data/Stack_layers_soil_depth.tif&quot;, overwrite = TRUE) covariates &lt;- stack(&quot;./data/Stack_layers_soil_depth.tif&quot;) # 01.6 Plot the covariates maps ================================================ reduce &lt;- aggregate(covariates, fact=10, fun=mean) plot(reduce) \\(\\\\[0.5cm]\\) # 01.7 Extract the values ====================================================== # Extract the values of each band for the sampling location df_cov = raster::extract(covariates, sp_df, method=&#39;simple&#39;) # 01.8 Save the data =========================================================== # Create a csv out of it write.csv(data.frame(df_cov), &quot;./data/df_cov_soil_depth.csv&quot;) # 02 Check the data ############################################################ # 02.1 Import the data and merge =============================================== Covdfgis &lt;- read.csv(&quot;./data/df_cov_soil_depth.csv&quot;) ID &lt;- 1:nrow(Covdfgis) SoilCov &lt;- cbind(ID, depth, Covdfgis[,-c(1)]) #Check the data names(SoilCov) str(SoilCov) # Prepare data for ML modelling SoilCovML &lt;- SoilCov[,-c(1:7)] ## [1] &quot;ID&quot; &quot;X&quot; &quot;Y&quot; &quot;Site.Name&quot; &quot;Depth&quot; &quot;sqrt&quot; ## [7] &quot;TE.1&quot; &quot;TE.5&quot; &quot;TE.7&quot; &quot;TE.8&quot; &quot;TE.9&quot; &quot;TE.12&quot; ## [13] &quot;TE.14&quot; &quot;TE.16&quot; &quot;TE.21&quot; &quot;LA5.1&quot; &quot;LA5.2&quot; &quot;LA5.3&quot; ## [19] &quot;LA5.4&quot; &quot;LA5.5&quot; &quot;LA5.6&quot; &quot;LST.1&quot; &quot;LST.2&quot; &quot;LST.3&quot; ## [25] &quot;LST.4&quot; &quot;OT.2&quot; &quot;OT.4&quot; &quot;OT.5&quot; &quot;OT.6&quot; &quot;OT.7&quot; ## [31] &quot;OT.10&quot; &quot;OT.9&quot; ## &#39;data.frame&#39;: 147 obs. of 32 variables: ## $ ID : int 1 2 3 4 5 6 7 8 9 10 ... ## $ X : num 287940 286917 280594 275040 289513 ... ## $ Y : num 4095202 4104485 4090376 4097134 4086706 ... ## $ Site.Name: chr &quot;B07_2022_001&quot; &quot;B07_2022_002&quot; &quot;B07_2022_003&quot; &quot;B07_2022_004&quot; ... ## $ Depth : num 100 65 40 80 80 0 25 100 75 75 ... ## $ sqrt : num 10 8.06 6.32 8.94 8.94 ... ## $ TE.1 : num 5.13 4.27 3.53 4.41 2.6 ... ## $ TE.5 : num 483 613 352 400 380 ... ## $ TE.7 : num -0.003644 0.000265 0 -0.003209 -0.006941 ... ## $ TE.8 : num 0.0733 0.4736 0.7583 0.0832 0.0127 ... ## $ TE.9 : num 1.637 0.445 2.258 0.118 0.394 ... ## $ TE.12 : num -2.09e-04 1.64e-05 2.24e-04 1.59e-03 3.37e-04 ... ## $ TE.14 : num -0.000943 0.000229 -0.000917 0.001199 -0.000281 ... ## $ TE.16 : num 0.0928 0.06845 0.00189 0.17723 0.15261 ... ## $ TE.21 : num -2.078 1.333 -0.852 1.378 -3.201 ... ## $ LA5.1 : num 0.16 0.18 0.2 0.167 0.196 ... ## $ LA5.2 : num 0.192 0.225 0.243 0.204 0.23 ... ## $ LA5.3 : num 0.272 0.288 0.314 0.252 0.28 ... ## $ LA5.4 : num 0.193 0.248 0.282 0.226 0.264 ... ## $ LA5.5 : num 0.16 0.123 0.135 0.103 0.117 ... ## $ LA5.6 : num -0.244 -0.233 -0.23 -0.206 -0.197 ... ## $ LST.1 : num 311 310 312 312 314 ... ## $ LST.2 : num 296 296 296 296 296 ... ## $ LST.3 : num 327 327 328 329 329 ... ## $ LST.4 : num 306 305 306 306 307 ... ## $ OT.2 : int 13 13 5 8 6 8 13 7 8 7 ... ## $ OT.4 : int 5 11 11 11 11 11 11 5 5 11 ... ## $ OT.5 : num 2113 2069 2122 2127 2144 ... ## $ OT.6 : num 734 738 664 647 739 ... ## $ OT.7 : num 211009 210498 211285 210657 211260 ... ## $ OT.10 : num 19.5 19 19.9 19.9 20 ... ## $ OT.9 : num 25.3 24.6 25.5 25.6 25.6 ... # 02.3 Plot and export the correlation matrix ================================== # Correlation of the data corrplot(cor(SoilCovML), method = &quot;color&quot;, col = viridis(200), type = &quot;upper&quot;, addCoef.col = &quot;black&quot;, # Add coefficient of correlation tl.col = &quot;black&quot;, tl.srt = 45, # Text label color and rotation number.cex = 0.7, # Size of the text labels cl.cex = 0.7) # Size of the color legend text #cl.lim = c(-1, 1)) # Color legend limits Regarding the correlation plot, the Landsat 5 bands and land surface temperature layers have correlation index over 0.75, as some transformed layers based on DEM (curvature, slope ,aspect …) and the solar radiation with wind. # 02.4 Export and save data ==================================================== save(covariates, SoilCov, file = &quot;./export/Save/Pre_process_soil_depth.RData&quot;) rm(list = ls()) 8.2.3 Tune the model # 03 Tune the model ############################################################ load(file = &quot;./export/Save/Pre_process_soil_depth.RData&quot;) # 03.1 Create the train and test set =========================================== #R Remove the site name and x, y columns df_soilCov &lt;- SoilCov[,c(1,5:length(SoilCov))] # Set a random seed set.seed(1070) # preprocess the layers preproc &lt;- preProcess(df_soilCov[,4:length(df_soilCov)], method=c(&quot;range&quot;)) df_soilCovTrans &lt;- predict(preproc, df_soilCov[,4:length(df_soilCov)]) df_soilCovTrans &lt;- cbind(df_soilCovTrans, df_soilCov[,1:3]) # Create a 80% split df_soil &lt;- initial_split(df_soilCovTrans, prop = 0.8, strata = ID) trainData &lt;- training(df_soil) testData &lt;- testing(df_soil) # Implement the index row.names(testData) &lt;- testData$ID row.names(trainData) &lt;- trainData$ID # Create the splits X_train &lt;- trainData[,-c(27:29)] y_train &lt;- trainData[,c(28)] y_train_sqrt &lt;- trainData[,c(29)] X_test &lt;- testData[,-c(27:29)] y_test &lt;- testData[,c(28)] y_test_sqrt &lt;- testData[,c(29)] # 03.2 Create the model train controls with cross-validation ================== TrainControl &lt;- trainControl(method=&quot;repeatedcv&quot;, 10, 3, allowParallel = TRUE, savePredictions=TRUE, verboseIter = TRUE) # 03.3 Set the grid for mtry and nodesize ====================================== tuneGrid &lt;- expand.grid(mtry = c(1, 3, 5, 7, 10, 12, 15)) nodesize_values &lt;- c(3, 4, 5, 6, 7, 8, 9, 10, 11, 12) all_results &lt;- list() set.seed(1070) # 03.4 Run the model tunning =================================================== # Loop over different nodesize values and different mtry for (nodesize in nodesize_values) { start_time &lt;- Sys.time() qrf_model &lt;- train(x = X_train, y = y_train_sqrt, method = &quot;qrf&quot;, trControl = TrainControl, tuneGrid = tuneGrid, ntree = 500, # Number of trees metric = &quot;RMSE&quot;, # Specify the metric you want to optimize nodesize = nodesize # Minimum node size ) end_time &lt;- Sys.time() print(end_time - start_time) # Store the results with the corresponding nodesize results &lt;- qrf_model$results results$nodesize &lt;- nodesize # Add the nodesize information all_results[[as.character(nodesize)]] &lt;- results } final_results &lt;- do.call(rbind, all_results) # View the combined results for all mtry and nodesize combinations print(final_results) ## mtry RMSE Rsquared MAE RMSESD RsquaredSD MAESD nodesize ## 3.1 1 2.729334 0.4962853 1.842530 0.8103775 0.2462337 0.5842215 3 ## 3.2 3 2.833004 0.4773500 1.904092 0.7890278 0.2513403 0.5469537 3 ## 3.3 5 2.841813 0.4714421 1.922363 0.7898268 0.2548695 0.5590358 3 ## 3.4 7 2.882372 0.4651227 1.941985 0.7952213 0.2559438 0.5670334 3 ## 3.5 10 2.910301 0.4604997 1.974741 0.7570323 0.2461620 0.5293020 3 ## 3.6 12 2.916578 0.4596933 1.975224 0.7161846 0.2408349 0.5149407 3 ## 3.7 15 2.951068 0.4526866 2.001091 0.7184835 0.2388678 0.5245988 3 ## 4.1 1 2.773342 0.4929364 1.867376 0.7325954 0.2481142 0.5790523 4 ## 4.2 3 2.838034 0.4895359 1.915626 0.7145291 0.2440721 0.5656305 4 ## 4.3 5 2.830851 0.4901692 1.905814 0.7078239 0.2359281 0.5483191 4 ## 4.4 7 2.878877 0.4750751 1.973288 0.7049437 0.2440490 0.5876352 4 ## 4.5 10 2.908365 0.4670291 1.969554 0.6328296 0.2241551 0.5133742 4 ## 4.6 12 2.947753 0.4580659 1.994925 0.6831769 0.2171447 0.5480810 4 ## 4.7 15 2.982162 0.4583561 2.026137 0.6708416 0.2158921 0.5384385 4 ## 5.1 1 2.663055 0.5221769 1.800572 0.7448825 0.2207681 0.5635888 5 ## 5.2 3 2.779503 0.5010455 1.867847 0.7509376 0.2170405 0.5612229 5 ## 5.3 5 2.789159 0.4988129 1.876542 0.7544052 0.2195226 0.5458377 5 ## 5.4 7 2.845951 0.4780145 1.923041 0.7654205 0.2351576 0.5985707 5 ## 5.5 10 2.916293 0.4551181 1.983060 0.7682618 0.2436306 0.5844046 5 ## 5.6 12 2.901288 0.4604422 1.969209 0.7500825 0.2318361 0.5945813 5 ## 5.7 15 2.928156 0.4543225 1.980911 0.7637384 0.2398471 0.5924727 5 ## 6.1 1 2.737298 0.5416168 1.848400 0.7511112 0.2307820 0.5255341 6 ## 6.2 3 2.769118 0.5411176 1.875529 0.7338547 0.2318195 0.5177122 6 ## 6.3 5 2.832541 0.5293194 1.907256 0.6903118 0.2239275 0.4742872 6 ## 6.4 7 2.857926 0.5173423 1.931808 0.7112448 0.2242474 0.4928804 6 ## 6.5 10 2.926328 0.4900923 1.970553 0.7080537 0.2288874 0.4873877 6 ## 6.6 12 2.942479 0.4853409 1.977365 0.6608536 0.2189466 0.4556416 6 ## 6.7 15 2.957402 0.4782926 1.985686 0.7045822 0.2218300 0.4805035 6 ## 7.1 1 2.730479 0.5095423 1.827464 0.7556501 0.2258124 0.5525021 7 ## 7.2 3 2.794204 0.4925000 1.903247 0.8293877 0.2384477 0.5883647 7 ## 7.3 5 2.851583 0.4780661 1.933760 0.7945918 0.2344559 0.5876801 7 ## 7.4 7 2.893746 0.4675265 1.944191 0.7999453 0.2305590 0.5874803 7 ## 7.5 10 2.871070 0.4677090 1.948722 0.7555909 0.2257198 0.5477448 7 ## 7.6 12 2.991799 0.4378629 2.030453 0.7139264 0.2149300 0.5409700 7 ## 7.7 15 3.007470 0.4320991 2.046692 0.8110356 0.2321078 0.5965111 7 ## 8.1 1 2.685270 0.5249273 1.793010 0.6919063 0.2178054 0.4843446 8 ## 8.2 3 2.763586 0.5095831 1.848866 0.7294080 0.2303865 0.5100895 8 ## 8.3 5 2.779153 0.5088139 1.871082 0.7549698 0.2398836 0.5340549 8 ## 8.4 7 2.773158 0.5061415 1.878156 0.7643847 0.2390085 0.5535613 8 ## 8.5 10 2.842823 0.4844367 1.922981 0.7414883 0.2325255 0.5451719 8 ## 8.6 12 2.779985 0.4986677 1.890600 0.7697851 0.2382519 0.5572981 8 ## 8.7 15 2.832611 0.4851692 1.920453 0.7173450 0.2241389 0.5217087 8 ## 9.1 1 2.708775 0.5081740 1.822192 0.6939379 0.2420910 0.5060151 9 ## 9.2 3 2.843956 0.4809980 1.902569 0.6654775 0.2326433 0.4704214 9 ## 9.3 5 2.865376 0.4708315 1.891287 0.6255177 0.2188883 0.4407110 9 ## 9.4 7 2.882954 0.4646611 1.915415 0.6112141 0.2152987 0.4561261 9 ## 9.5 10 2.895203 0.4690362 1.920161 0.6001704 0.2199620 0.4374223 9 ## 9.6 12 2.876605 0.4713040 1.922756 0.5518848 0.1996927 0.4064532 9 ## 9.7 15 2.910187 0.4565587 1.927773 0.5937036 0.2140814 0.4306462 9 ## 10.1 1 2.669146 0.5193111 1.814073 0.7783866 0.2272123 0.5854923 10 ## 10.2 3 2.693680 0.5166215 1.828976 0.7915503 0.2280681 0.5746169 10 ## 10.3 5 2.735058 0.5080277 1.850283 0.7329562 0.2181400 0.5332899 10 ## 10.4 7 2.799899 0.4926177 1.898245 0.7327455 0.2183946 0.5429160 10 ## 10.5 10 2.822178 0.4870636 1.934005 0.7389095 0.2152389 0.5410894 10 ## 10.6 12 2.840115 0.4845868 1.934662 0.7306276 0.2082656 0.5552059 10 ## 10.7 15 2.828377 0.4889028 1.921956 0.7200878 0.2129413 0.5440597 10 ## 11.1 1 2.735141 0.5307591 1.850768 0.7655969 0.2501171 0.5644426 11 ## 11.2 3 2.738556 0.5260517 1.863128 0.7605979 0.2525759 0.5524572 11 ## 11.3 5 2.752541 0.5154618 1.874229 0.7562428 0.2553953 0.5714878 11 ## 11.4 7 2.820722 0.4978160 1.914186 0.7809397 0.2586396 0.5791621 11 ## 11.5 10 2.869406 0.4885411 1.950299 0.7591539 0.2539700 0.5947951 11 ## 11.6 12 2.876452 0.4770955 1.967754 0.7989715 0.2685377 0.6284647 11 ## 11.7 15 2.906921 0.4680887 1.989789 0.7629615 0.2646727 0.6125713 11 ## 12.1 1 2.702005 0.5072654 1.818173 0.8076490 0.2471731 0.6133061 12 ## 12.2 3 2.754790 0.4953014 1.880478 0.8198944 0.2518294 0.6431279 12 ## 12.3 5 2.790744 0.4896488 1.921956 0.8345103 0.2542266 0.6653885 12 ## 12.4 7 2.857767 0.4728454 1.958128 0.8065503 0.2489765 0.6457750 12 ## 12.5 10 2.855369 0.4734595 1.966860 0.8357491 0.2489488 0.6733135 12 ## 12.6 12 2.906046 0.4599975 1.987815 0.7731286 0.2355587 0.6506323 12 ## 12.7 15 2.904322 0.4593194 1.995720 0.8032117 0.2424297 0.6707301 12 The two parameters that we are modifying are the minimum node size/ nodesize (min_n) and the number of randomly selected covariates (mtry). We set 500 trees as default. # 03.5 Plot the different optimization ========================================= gg1 &lt;- ggplot(final_results, aes(x = mtry, y = Rsquared, color = as.factor(nodesize))) + geom_line() + geom_point() + labs(x = &quot;mtry&quot;, y = &quot;R²&quot;, color = &quot;nodesize&quot;) + theme_minimal() gg2 &lt;- ggplot(final_results, aes(x = mtry, y = RMSE, color = as.factor(nodesize))) + geom_line() + geom_point() + labs(x = &quot;mtry&quot;, y = expression(&quot;RMSE (&quot; *cm^0.5* &quot;)&quot;), color = &quot;nodesize&quot;) + theme_minimal() # Combine R2 and RMSE in one plot combined_plot &lt;- wrap_plots(gg1, gg2, ncol = 2) plot(combined_plot) # Save and export the plot ggsave(&quot;./export/Optimisation of the model.pdf&quot;, combined_plot, width = 20, height = 10) ggsave(&quot;./export/Optimisation of the model.png&quot;, combined_plot, width = 20, height = 10) 8.2.4 Final model Here we selected optimised nodesize of 6 and mtry of 1 with ntree always set at 500. # 04.1 Produce the final model ################################################# nodesize &lt;- 6 mtry &lt;- expand.grid(mtry = 1) set.seed(1070) qrf_final &lt;- train(x = X_train, y = y_train_sqrt, method = &quot;qrf&quot;, trControl = TrainControl, tuneGrid = mtry, ntree = 500, # Number of trees metric = &quot;RMSE&quot;, # Specify the metric you want to optimize nodesize = nodesize ) results_qrf &lt;- qrf_final$results # View R² and RMSE values for final model print(results_qrf[, c(&quot;mtry&quot;, &quot;RMSE&quot;, &quot;Rsquared&quot;)]) ## mtry RMSE Rsquared ## 1 1 2.619363 0.5257963 # Select the final model final_model &lt;- qrf_final$finalModel # 04.2 Show covariates importance ============================================== importance_df &lt;- as.data.frame(varImp(final_model)) # Convert row names (variables) to a column for ggplot importance_df$scale &lt;- (importance_df$Overall/sum(importance_df$Overall)*100) importance_df$Variable &lt;- rownames(importance_df) # Plot using ggplot2 gg1 &lt;- ggplot(importance_df, aes(x = reorder(Variable, scale), y = scale)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;lightblue&quot;) + coord_flip() + xlab(&quot;Covariates&quot;) + ylab(&quot;Importance scaled&quot;) + ggtitle(&quot;Variable Importance from QRF Model (%)&quot;) gg1 8.2.5 Model evaluation We used five metrics to evaluate the model: RMSE= Root mean square error R\\(^2\\)= Coefficient of determination, also called rsquared. MAE = Mean absolute error CCC = Concordance correlation coefficient PICP = Prediction interval coverage probability set at 90% interval \\[ \\\\[0.5cm] RMSE = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{N}} \\] \\[ \\\\[0.5cm] R^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y}_i)^2} \\] \\[ \\\\[0.5cm] MAE = \\frac{1}{n} \\sum_{i=1}^{n}|Y_i - \\hat{Y}_i| \\] \\[ \\\\[0.5cm] CCC = \\frac{2S_{XY}}{S_{X}^2+S_{Y}^2+(\\bar{X}-\\bar{Y})^2} \\] \\[ \\\\[0.5cm] PICP = \\frac{1}{v} ~ count ~ j \\\\ j = PL^{L}_{j} \\leq t_j \\leq PL^{U}_{j} \\] # Save and export the plot ggsave(&quot;./export/Variables importance.png&quot;, gg1, width = 10, height = 8) ggsave(&quot;./export/Variables importance.pdf&quot;, gg1, width = 10, height = 8) # 04.3 Predict the test set ==================================================== predictions &lt;- predict(final_model, X_test) # 04.4 Get the metrics ======================================================== metrics &lt;- postResample(predictions[,2], y_test_sqrt) ccc &lt;- CCC(y_test_sqrt, predictions[,2]) PICP &lt;- (y_test_sqrt &gt;= predictions[,1]) &amp; (y_test_sqrt &lt;= predictions[,3]) PICP &lt;- mean(PICP)*100 Final_stats &lt;- as.data.frame(t(c(metrics, ccc$rho.c$est, PICP))) colnames(Final_stats) &lt;- c(&quot;RMSE&quot;, &quot;R²&quot;, &quot;MAE&quot;, &quot;CCC&quot;, &quot;PICP&quot;) # Print the metrics print(Final_stats) ## RMSE (cm-0.5) R² MAE CCC PICP ## 1 2.422883 0.5680413 1.538656 0.7375648 96.875 # Write it write.table(Final_stats, file=&quot;./export/Final_stats.txt&quot;, row.names = FALSE) # 04.5 Save the model and train and test sets ================================== save(trainData, testData, final_model, qrf_model, all_results, qrf_final, file = &quot;./export/Save/Model.RData&quot;) rm(list = ls()) The final model explains around 56% of the variability (R\\(^2\\)) of the soil depth with a precision (RMSE) of soil depth around 2.42 cm\\(^{-0.5}\\). 8.2.6 Mapping the soil depth of the area Due to the highly detailed data we had to split the prediction into different blocks (16). This process allows the computer to run separately each prediction. # 05 Predict the map ########################################################### # 05.1 Import the previous data ================================================ load(file = &quot;./export/save/Model_soil_depth.RData&quot;) stack_raster &lt;- stack(&quot;./data/Stack_layers_soil_depth.tif&quot;) cov &lt;- read.csv(&quot;./data/df_cov_soil_depth.csv&quot;) cov &lt;- cov[,-1] cov[] &lt;- lapply(cov , as.numeric) # 05.2 Normalise the values from the rasters =================================== process_layer &lt;- function(layer) { # Convert in numeric layer &lt;- as.numeric(layer) # Replace the NAs by median median_value &lt;- median(layer, na.rm = TRUE) layer[is.na(layer)] &lt;- median_value return(layer) } scaling_params &lt;- lapply(1:ncol(cov), function(i) { list(min = min(cov[[i]], na.rm = TRUE), max = max(cov[[i]], na.rm = TRUE)) }) scale_layer &lt;- function(layer, min_val, max_val) { (layer - min_val) / (max_val - min_val) } stack_scaled &lt;- stack() # Apply transformation for (i in 1:nlayers(raster_stack)) { min_val &lt;- scaling_params[[i]]$min max_val &lt;- scaling_params[[i]]$max # Normalised the layer layer_processed &lt;- calc(raster_stack[[i]], process_layer) layer_scaled &lt;- calc(layer_processed, function(x) scale_layer(x, min_val, max_val)) stack_scaled &lt;- addLayer(stack_scaled, layer_scaled) cat(round((i/nlayers(raster_stack))*100, 1),&quot;% \\n&quot;) } stack_scaled &lt;- rast(stack_scaled) sum(is.na(values(stack_scaled))) names(stack_scaled) &lt;- names(raster_stack) terra::writeRaster(stack_scaled, &quot;./export/Soil_depth/Stack_raster_normalised_soil_depth.tif&quot;, overwrite = TRUE) # 05.3 Prediction map for soil depth =========================================== rm(list = ls(all.names = TRUE)) load(&quot;./export/save/Model_soil_depth.RData&quot;) raster_stack_normalised &lt;- stack(&quot;./export/Soil_depth/Stack_raster_normalised_soil_depth.tif&quot;) # Divide in block for allowing the computer to run it block_info &lt;- blockSize(raster_stack_normalised ) # Create an empty raster for storing predicted values predicted_raster &lt;- raster_stack_normalised [[1]] # Use the first layer as a template predicted_raster &lt;- writeStart(predicted_raster, &quot;./export/Prediction_depth_sqrt.tif&quot;, overwrite = TRUE) # Loop through each block of the raster stack for (i in 1:block_info$n) { # Read block of raster data start_time &lt;- proc.time() block &lt;- getValuesBlock(raster_stack_normalised , row = block_info$row[i], nrows = block_info$nrows[i]) block &lt;- as.data.frame(block) # Process the block predicted_block &lt;- predict(final_model, block, what = c(0.05, 0.5, 0.95)) # Write the predicted block to the output raster predicted_raster &lt;- writeValues(predicted_raster, predicted_block[,2], block_info$row[i]) end_time &lt;- proc.time() print(end_time - start_time) print(i) } # Close the creation of the raster predicted_raster &lt;- writeStop(predicted_raster) # Rescale the raster and export it predicted_raster_sqrt &lt;- (predicted_raster)^2 writeRaster(predicted_raster_sqrt,&quot;./export/Prediction_depth.tif&quot;, format = &quot;GTiff&quot;,overwrite=T) 8.2.7 Prediction of uncertainty map The uncertainty of the model is based on the following equation from Yan et al. (2018). \\[ \\\\[0.5cm] Uncertainty = \\frac{Qp ~ 0.95- Qp ~ 0.05}{Qp ~ 0.5} \\] # 05.4 Uncertainty map of soil depth =========================================== # Create an empty raster for storing predicted values uncertainty_raster &lt;- raster_stack_normalised [[1]] # Use the first layer as a template uncertainty_raster &lt;- writeStart(uncertainty_raster, &quot;./export/Uncertainty_depth_sqrt.tif&quot;, overwrite = TRUE) # Loop through each block of the raster stack for (i in 1:block_info$n) { # Read block of raster data start_time &lt;- proc.time() block &lt;- getValuesBlock(raster_stack_normalised, row = block_info$row[i], nrows = block_info$nrows[i]) block &lt;- as.data.frame(block) # Process the block predicted_block &lt;- predict(final_model, block, what = c(0.05, 0.5, 0.95)) # Write the predicted block to the output raster values &lt;- (predicted_block[,3] - predicted_block[,1]) /predicted_block[,2] uncertainty_raster &lt;- writeValues(uncertainty_raster, values, block_info$row[i]) end_time &lt;- proc.time() print(end_time - start_time) print(i) } # Close the creation of the raster uncertainty_raster &lt;- writeStop(uncertainty_raster) uncertainty_raster_sqrt &lt;- (uncertainty_raster)^2 writeRaster(uncertainty_raster_sqrt ,&quot;./export/Uncertainty_depth.tif&quot;, format = &quot;GTiff&quot;,overwrite=T) 8.3 Visualisations and exports 8.3.1 Basic visualisations # 06 Visualisation of the prediction ########################################### # 06.1 Plot the depth maps ===================================================== rm(list = ls()) load(file = &quot;./export/save/Pre_process_soil_depth.RData&quot;) predicted_raster &lt;- raster(&quot;./export/Prediction_depth.tif&quot;) uncertainty_raster &lt;- raster(&quot;./export/Uncertainty_depth.tif&quot;) survey &lt;- st_read(&quot;./data/Survey_Area.gpkg&quot;, layer = &quot;Survey_Area&quot;) sp_df &lt;- st_as_sf(SoilCov, coords = c(&quot;X&quot;, &quot;Y&quot;), crs = 32638) predicted_raster_resize &lt;- aggregate(predicted_raster, fact=5, fun=mean) uncertainty_resize &lt;- aggregate(uncertainty_raster, fact=5, fun=mean) # Plot continuous values of the map mapview(predicted_raster_resize, at = seq(0,100,20), legend = TRUE, layer.name = &quot;Soil depth prediction (cm)&quot;,col.regions = wes_palettes$Zissou1Continuous) + mapview(sp_df, zcol = &quot;Depth&quot;, at = seq(0,100,20), legend = TRUE, layer.name = &quot;Soil samples depth (cm)&quot;, col.regions = wes_palettes$Zissou1Continuous) \\(\\\\[0.5cm]\\) mapview(uncertainty_resize, legend = TRUE, layer.name = &quot;Soil depth prediction uncertainty&quot;, col.regions = brewer.pal(5, &quot;YlGnBu&quot;)) \\(\\\\[0.5cm]\\) 8.3.2 Colorblind visualisations # 06.3 Visualise for colorblind ================================================ wes_palette_hcl &lt;- function(palette_name, n = 7) { wes_colors &lt;- wes_palette(palette_name, n = n, type = &quot;continuous&quot;) hex_colors &lt;- as.character(wes_colors) return(hex_colors) } palette_wes &lt;- rev(wes_palette_hcl(&quot;Zissou1&quot;, n = 7)) palette_blue &lt;- colorRampPalette(c(&quot;lightblue&quot;, &quot;blue&quot;, &quot;darkblue&quot;))(7) # Replace the cvd palette with palette_wes for depht and palette blue for uncertainty gg1 &lt;- cblind.plot(uncertainty_resize, cvd = palette_blue) gg2 &lt;- cblind.plot(uncertainty_resize, cvd = &quot;deuteranopia&quot;) gg3 &lt;- cblind.plot(uncertainty_resize, cvd = &quot;tritanopia&quot;) gg4 &lt;- cblind.plot(uncertainty_resize, cvd = &quot;protanopia&quot;) grid &lt;- grid.arrange( arrangeGrob(gg1, bottom = textGrob(&quot;Original&quot;, gp = gpar(fontsize = 14))), arrangeGrob(gg2, bottom = textGrob(&quot;Deuteranopia&quot;, gp = gpar(fontsize = 14))), arrangeGrob(gg3, bottom = textGrob(&quot;Tritanopia&quot;, gp = gpar(fontsize = 14))), arrangeGrob(gg4, bottom = textGrob(&quot;Protanopia&quot;, gp = gpar(fontsize = 14))), nrow = 2, ncol = 2, top = textGrob(&quot;Soil depth uncertainty for different visions&quot;, gp = gpar(fontsize = 16, fontface = &quot;bold&quot;))) ggsave(&quot;./export/Visualisation_soil_depth_uncertainty.png&quot;, grid, width = 20, height = 10) ggsave(&quot;./export/Visualisation_soil_depth_uncertainty.pdf&quot;, grid, width = 20, height = 10) dev.off() (#fig:Colorblind plot hide)Soil depth prediction map adapted for colorblindness \\(\\\\[0.5cm]\\) (#fig:Colorblind plot hide second)Soil depth prediction uncertainty map adapted for colorblind 8.3.3 Export final maps # 06.4 Export final maps ======================================================= # Replace missing values by zero as it occurs only in mountainous area uncertainty_raster[is.na(uncertainty_raster)] &lt;- 0 predicted_raster[is.na(predicted_raster)] &lt;- 0 # For GeoTiff format soil_depth_map &lt;- stack(predicted_raster, uncertainty_raster) soil_depth_crop &lt;- crop(soil_depth_map, survey) soil_depth_mask &lt;- mask(soil_depth_crop, survey, inverse=FALSE, updatevalue=NA, updateNA=TRUE) crs(soil_depth_mask) &lt;- &quot;EPSG:32638&quot; x &lt;- rast(soil_depth_mask) x_repro &lt;- project(x, &quot;EPSG:4326&quot;) names(x_repro) &lt;- c(&quot;Prediction&quot;, &quot;Uncertainty&quot;) terra::writeRaster(x_repro,&quot;./export/Soil_depth_prediction_map.tif&quot;, overwrite=TRUE) # For netCDF format CDF_df &lt;- lapply(1:nlayers(x_repro), function(i) { rast(x_repro[[i]]) }) names(CDF_df) &lt;- c(&quot;Prediction&quot;, &quot;Uncertainty&quot;) soil_to_netcdf &lt;- function(soil_list, output_file, overwrite = FALSE) { # Check if file exists and handle overwrite if (file.exists(output_file) &amp;&amp; !overwrite) { stop(&quot;File already exists and overwrite = FALSE&quot;) } # If file exists and overwrite is TRUE, remove the existing file if (file.exists(output_file) &amp;&amp; overwrite) { file.remove(output_file) } # Get dimensions and CRS from first raster r &lt;- soil_list[[1]] nx &lt;- ncol(r) ny &lt;- nrow(r) crs_string &lt;- crs(r) # Create longitude and latitude vectors ext &lt;- ext(r) lon &lt;- seq(from = ext[1], to = ext[2], length.out = nx) lat &lt;- seq(from = ext[3], to = ext[4], length.out = ny) # Changed back to ascending order # Define dimensions londim &lt;- ncdim_def(&quot;longitude&quot;, &quot;degrees_east&quot;, lon) latdim &lt;- ncdim_def(&quot;latitude&quot;, &quot;degrees_north&quot;, lat) # Define units for each soil property units_list &lt;- list( Prediction = &quot;cm&quot;, Uncertainty = &quot;cm&quot; ) # Create list of variables with appropriate units var_list &lt;- list() for (var_name in names(soil_list)) { var_list[[var_name]] &lt;- ncvar_def( name = var_name, units = units_list[[var_name]], dim = list(londim, latdim), missval = NA ) } # Create netCDF file ncout &lt;- nc_create(output_file, var_list) # Write data for (var_name in names(soil_list)) { # Convert SpatRaster to matrix and handle orientation values_matrix &lt;- t(as.matrix(soil_list[[var_name]], wide=TRUE)) values_matrix &lt;- values_matrix[,ncol(values_matrix):1] ncvar_put(ncout, var_list[[var_name]], vals = values_matrix) } # Add global attributes ncatt_put(ncout, 0, &quot;title&quot;, &quot;Soil depth until R horizon &quot;) ncatt_put(ncout,0,&quot;institution&quot;,&quot;Tuebingen University, CRC1070 ResourceCultures&quot;) ncatt_put(ncout, 0, &quot;description&quot;, &quot;Soil depth until R horizon for the top 100 cm in the Northern Kurdsitan region of Irak&quot;) ncatt_put(ncout,0,&quot;author&quot;, &quot;Mathias Bellat PhD. candidate at Tuebingen University (mathias.bellat@uni-tuebingen.de)&quot;) ncatt_put(ncout, 0, &quot;creation_date&quot;, format(Sys.time(), &quot;%Y-%m-%d %H:%M:%S&quot;)) # Add CRS information if (!is.na(crs_string)) { ncatt_put(ncout, 0, &quot;crs&quot;, crs_string) ncatt_put(ncout, 0, &quot;spatial_ref&quot;, crs_string) # Add standard CF grid mapping attributes ncatt_put(ncout, 0, &quot;Conventions&quot;, &quot;CF-1.6&quot;) ncatt_put(ncout, &quot;longitude&quot;, &quot;standard_name&quot;, &quot;longitude&quot;) ncatt_put(ncout, &quot;longitude&quot;, &quot;axis&quot;, &quot;X&quot;) ncatt_put(ncout, &quot;latitude&quot;, &quot;standard_name&quot;, &quot;latitude&quot;) ncatt_put(ncout, &quot;latitude&quot;, &quot;axis&quot;, &quot;Y&quot;) } # Add variable descriptions var_descriptions &lt;- list( Prediction = &quot;Soil depth until R horizon up to 100 cm&quot;, Uncertainty = &quot;Uncertainty of the Quantile Random Forest of soil depth prediction based on 0.05 - 0.95 interval&quot; ) # Add variable-specific attributes for (var_name in names(soil_list)) { ncatt_put(ncout, var_list[[var_name]], &quot;long_name&quot;, var_descriptions[[var_name]]) } # Close the file nc_close(ncout) } soil_to_netcdf(CDF_df, &quot;./export/Soil_depth_prediction_map.nc&quot;, overwrite = TRUE) # 06.5 Final visualisations ==================================================== #Change parameters for uncertainy and predictions raster_resize &lt;- aggregate(soil_depth_map, fact=5, fun=mean) rasterdf &lt;- raster::as.data.frame(raster_resize, xy = TRUE) rasterdf &lt;- rasterdf[complete.cases(rasterdf),] bounds &lt;- st_bbox(survey) xlim_new &lt;- c(bounds[&quot;xmin&quot;] - 3000, bounds[&quot;xmax&quot;] + 3000) ylim_new &lt;- c(bounds[&quot;ymin&quot;] - 3000, bounds[&quot;ymax&quot;] + 3000) palette_wes &lt;- rev(wes_palette(&quot;Zissou1&quot;, type = &quot;continuous&quot;)) palette_blue &lt;- colorRampPalette(c(&quot;lightblue&quot;, &quot;blue&quot;, &quot;darkblue&quot;))(7) gg1 &lt;- ggplot() + geom_raster(data = rasterdf, aes(x = x, y = y,fill = Uncertainty )) + ggtitle(&quot;Soil depth prediction uncertainty map&quot;) + scale_fill_gradientn(colors = palette_blue, name = &quot;Uncertainty&quot;) + annotation_scale( location = &quot;br&quot;, width_hint = 0.3, height = unit(0.3, &quot;cm&quot;), line_col = &quot;black&quot;, text_col = &quot;black&quot;, bar_cols = c(&quot;white&quot;, &quot;red&quot;) ) + annotate(&quot;text&quot;, label = paste(&quot;Projection: WGS84 UTM 38N&quot;), x = Inf, y = -Inf, hjust = 1.5, vjust = -3, size = 3, color = &quot;black&quot;) + annotation_north_arrow(location = &quot;tr&quot;, which_north = &quot;true&quot;, height = unit(1, &quot;cm&quot;), width = unit(0.75, &quot;cm&quot;)) + theme_bw() + theme( panel.grid = element_blank(), axis.text.y = element_text(angle = 90, vjust = 0.5, hjust = 0.5), axis.title = element_blank(), legend.position = &quot;right&quot;)+ coord_equal(ratio = 1) ggsave(&quot;./export/Visualisation_soil_depth_uncertainty_prediction_maps.pdf&quot;, gg1, width = 20, height = 10) ggsave(&quot;./export/Visualisation_soil_depth_uncertainty_prediction_maps.png&quot;, gg1, width = 20, height = 10) (#fig:final plot hide)Final soil depth prediction map (#fig:final plot second hide)Final soil depth prediction uncertainty map adapted # 06.6 Simplified version for publication ======================================= gg1 &lt;- ggplot() + geom_raster(data = rasterdf, aes(x = x, y = y, fill = Prediction)) + ggtitle(&quot;Soil depth prediction map&quot;) + scale_fill_gradientn(colors = palette_wes, name = &quot;Soil depth in cm&quot;) + theme_void() + theme( legend.position = &quot;right&quot;, plot.title = element_text(size = 8), legend.title = element_text(size = 10), legend.text = element_text(size = 8), legend.key.size = unit(0.7, &quot;cm&quot;)) + coord_equal(ratio = 1) gg2 &lt;- ggplot() + geom_raster(data = rasterdf, aes(x = x, y = y, fill = Uncertainty)) + ggtitle(&quot;Soil depth uncertainty prediction map&quot;) + scale_fill_gradientn(colors = palette_blue, name = &quot;Uncertainty&quot;) + theme_void() + theme( legend.position = &quot;right&quot;, plot.title = element_text(size = 8), legend.title = element_text(size = 10), legend.text = element_text(size = 8), legend.key.size = unit(0.7, &quot;cm&quot;)) + coord_equal(ratio = 1) combined_plot &lt;- plot_grid(gg1, gg2, ncol = 2) ggsave(&quot;./export/Publication_soil_depth_map.pdf&quot;, combined_plot, width = 20, height = 10) ggsave(&quot;./export/Publication_soil_depth_map.png&quot;, combined_plot, width = 20, height = 10) dev.off() (#fig:publication plot hide)Simplified plot of the prediction maps "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
